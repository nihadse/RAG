
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏦 Bank Information System RAG Chatbot (Weaviate + Azure OpenAI, LangChain)\n",
    "\n",
    "This notebook demonstrates a full Retrieval Augmented Generation (RAG) workflow tailored for banking information system documentation, using **Weaviate** as the vector database and **Azure OpenAI** for LLM/embeddings, with French-friendly prompts."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# -- 1. Install required packages (run only once)\n",
    "!pip install python-dotenv langchain langchain-openai langchain-community weaviate-client[embedded] unstructured[all-docs] pypdf sentence-transformers rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment & Config\n",
    "Set up your secrets!\n",
    "- Create a `.env` file in your project with your Azure OpenAI and Weaviate credentials."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "REQUIRED_VARS = [\n",
    "    'AZURE_OPENAI_API_KEY',\n",
    "    'AZURE_OPENAI_ENDPOINT',\n",
    "    'AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME',\n",
    "    'AZURE_OPENAI_DEPLOYMENT_NAME'\n",
    "]\n",
    "\n",
    "for var in REQUIRED_VARS:\n",
    "    if not os.getenv(var):\n",
    "        raise EnvironmentError(f\"Missing env variable: {var}\")\n",
    "# Optionally set Weaviate config if connecting to a remote instance. For embedded test, skip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {}, 
   "source": [ "## 3. Document Ingestion (PDF/CSV/XLSX)\n", "Load your files from structured folders." ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader, CSVLoader, UnstructuredExcelLoader\n",
    "import glob\n",
    "\n",
    "DOCS_DIR = \"documents\"\n",
    "\n",
    "def load_documents():\n",
    "    documents = []\n",
    "    source_metadata = {}\n",
    "    \n",
    "    # PDFs from ./documents/pdfs\n",
    "    if os.path.exists(os.path.join(DOCS_DIR, \"pdfs\")):\n",
    "        for file in glob.glob(os.path.join(DOCS_DIR, \"pdfs\", \"*.pdf\")):\n",
    "            try:\n",
    "                loader = PyPDFDirectoryLoader(os.path.dirname(file))\n",
    "                docs = loader.load()\n",
    "                file_name = os.path.basename(file)\n",
    "                \n",
    "                # Add source metadata to each document\n",
    "                for doc in docs:\n",
    "                    doc.metadata[\"source_file\"] = file_name\n",
    "                    doc.metadata[\"source_type\"] = \"pdf\"\n",
    "                \n",
    "                documents.extend(docs)\n",
    "                print(f\"Loaded {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "    # CSVs from ./documents/csv\n",
    "    csv_dir = os.path.join(DOCS_DIR, \"csv\")\n",
    "    if os.path.exists(csv_dir):\n",
    "        for file in glob.glob(os.path.join(csv_dir, '*.csv')):\n",
    "            try:\n",
    "                docs = CSVLoader(file_path=file).load()\n",
    "                file_name = os.path.basename(file)\n",
    "                \n",
    "                for doc in docs:\n",
    "                    doc.metadata[\"source_file\"] = file_name\n",
    "                    doc.metadata[\"source_type\"] = \"csv\"\n",
    "                    \n",
    "                documents.extend(docs)\n",
    "                print(f\"Loaded {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "    # Excels from ./documents/excel\n",
    "    xls_dir = os.path.join(DOCS_DIR, \"excel\")\n",
    "    if os.path.exists(xls_dir):\n",
    "        for file in glob.glob(os.path.join(xls_dir, '*.xls*')):\n",
    "            try:\n",
    "                docs = UnstructuredExcelLoader(file_path=file).load()\n",
    "                file_name = os.path.basename(file)\n",
    "                \n",
    "                for doc in docs:\n",
    "                    doc.metadata[\"source_file\"] = file_name\n",
    "                    doc.metadata[\"source_type\"] = \"excel\"\n",
    "                    \n",
    "                documents.extend(docs)\n",
    "                print(f\"Loaded {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents in total\")\n",
    "    return documents\n",
    "\n",
    "docs = load_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [ "## 4. Semantic Text Splitting" ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Function for semantic text splitting\n",
    "def split_documents_semantic(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split documents into semantically coherent chunks\"\"\"\n",
    "    # First get basic chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"]\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Re-group semantically similar chunks\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        print(f\"Loaded semantic model: {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading semantic model: {e}\")\n",
    "        # Fall back to regular chunking\n",
    "        return chunks\n",
    "    \n",
    "    # Group similar chunks using embeddings similarity\n",
    "    grouped_chunks = []\n",
    "    current_group = []\n",
    "    last_embedding = None\n",
    "    similarity_threshold = 0.75\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Skip empty chunks\n",
    "        if not chunk.page_content.strip():\n",
    "            continue\n",
    "            \n",
    "        # Get embedding for this chunk\n",
    "        current_embedding = model.encode(chunk.page_content)\n",
    "        \n",
    "        # Start a new group if we don't have one yet\n",
    "        if not current_group:\n",
    "            current_group.append(chunk)\n",
    "            last_embedding = current_embedding\n",
    "            continue\n",
    "            \n",
    "        # Calculate similarity with the previous chunk\n",
    "        similarity = np.dot(current_embedding, last_embedding) / (np.linalg.norm(current_embedding) * np.linalg.norm(last_embedding))\n",
    "        \n",
    "        # If similar enough and combined length not too large, add to current group\n",
    "        combined_length = sum(len(c.page_content) for c in current_group) + len(chunk.page_content)\n",
    "        if similarity > similarity_threshold and combined_length < chunk_size * 1.5:\n",
    "            current_group.append(chunk)\n",
    "            # Update last embedding as average of group\n",
    "            group_embeddings = model.encode([c.page_content for c in current_group])\n",
    "            last_embedding = np.mean(group_embeddings, axis=0)\n",
    "        else:\n",
    "            # Combine current group into a single document and add to results\n",
    "            if current_group:\n",
    "                combined_text = \" \".join([c.page_content for c in current_group])\n",
    "                combined_metadata = current_group[0].metadata.copy()\n",
    "                combined_metadata[\"chunk_sources\"] = [c.metadata.get(\"source_file\", \"unknown\") for c in current_group]\n",
    "                grouped_chunks.append(Document(page_content=combined_text, metadata=combined_metadata))\n",
    "                \n",
    "            # Start a new group with this chunk\n",
    "            current_group = [chunk]\n",
    "            last_embedding = current_embedding\n",
    "    \n",
    "    # Add the last group if it exists\n",
    "    if current_group:\n",
    "        combined_text = \" \".join([c.page_content for c in current_group])\n",
    "        combined_metadata = current_group[0].metadata.copy()\n",
    "        combined_metadata[\"chunk_sources\"] = [c.metadata.get(\"source_file\", \"unknown\") for c in current_group]\n",
    "        grouped_chunks.append(Document(page_content=combined_text, metadata=combined_metadata))\n",
    "    \n",
    "    print(f\"Initial chunking: {len(chunks)} chunks\")\n",
    "    print(f\"After semantic grouping: {len(grouped_chunks)} chunks\")\n",
    "    return grouped_chunks\n",
    "\n",
    "# Apply the semantic splitter\n",
    "chunks = split_documents_semantic(docs)\n",
    "print(f\"Split into {len(chunks)} final chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [ "## 5. Weaviate Vector Store Setup + Embeddings (Azure OpenAI)" ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We use embedded Weaviate for testing. If you want remote, set the proper config env.\n",
    "import weaviate\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Weaviate as LCWeaviate\n",
    "\n",
    "WEAVIATE_CLASS = \"BankDocChunk\"\n",
    "BANK_DB_STATE_PATH = \"bank_rag_state.json\"\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    openai_api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "# Create embedded Weaviate client\n",
    "client = weaviate.Client(\"embedded\")\n",
    "\n",
    "# Clean up any previous test schema/classes\n",
    "if client.schema.exists(WEAVIATE_CLASS):\n",
    "    client.schema.delete_class(WEAVIATE_CLASS)\n",
    "\n",
    "# Add a class (collection) for chunks with metadata fields\n",
    "client.schema.create_class({\n",
    "    \"class\": WEAVIATE_CLASS,\n",
    "    \"vectorizer\": \"none\",\n",
    "    \"properties\": [\n",
    "        {\"name\": \"text\", \"dataType\": [\"text\"]},\n",
    "        {\"name\": \"source_file\", \"dataType\": [\"string\"]},\n",
    "        {\"name\": \"source_type\", \"dataType\": [\"string\"]},\n",
    "        {\"name\": \"page\", \"dataType\": [\"int\"]},\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(f\"Weaviate schema: {[x['class'] for x in client.schema.get()['classes']]}\")\n",
    "\n",
    "# Use LangChain's Weaviate wrapper for RAG\n",
    "vectorstore = LCWeaviate.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    client=client,\n",
    "    by_text=True,\n",
    "    text_key=\"text\",\n",
    "    class_name=WEAVIATE_CLASS,\n",
    ")\n",
    "\n",
    "print(f\"Inserted {vectorstore._client.query.aggregate(WEAVIATE_CLASS).with_meta_count().do()['data']['Aggregate'][WEAVIATE_CLASS][0]['meta']['count']} vector chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [ "## 6. Free Custom Re-ranker Using BM25 Instead of Cohere" ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import pickle\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "\n",
    "class BM25Reranker:\n",
    "    \"\"\"A document reranker using BM25 algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, k: int = 4):\n",
    "        \"\"\"Initialize with the number of documents to return\"\"\"\n",
    "        self.k = k\n",
    "        self.tokenized_corpus = None\n",
    "        self.bm25 = None\n",
    "        self.document_map = {}\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Fit the BM25 model on the corpus\"\"\"\n",
    "        tokenized_corpus = [doc.page_content.lower().split() for doc in documents]\n",
    "        self.document_map = {i: doc for i, doc in enumerate(documents)}\n",
    "        self.tokenized_corpus = tokenized_corpus\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        return self\n",
    "    \n",
    "    def rerank(self, query: str, documents):\n",
    "        \"\"\"Rerank documents based on BM25 scores\"\"\"\n",
    "        if not self.bm25:\n",
    "            self.fit(documents)\n",
    "        \n",
    "        # Tokenize query\n",
    "        tokenized_query = query.lower().split()\n",
    "        \n",
    "        # Create a map from content to original document to handle duplicates\n",
    "        doc_lookup = {}\n",
    "        for i, doc in enumerate(documents):\n",
    "            doc_lookup[doc.page_content] = i\n",
    "        \n",
    "        # Get BM25 scores\n",
    "        tokenized_docs = [doc.page_content.lower().split() for doc in documents]\n",
    "        temp_bm25 = BM25Okapi(tokenized_docs)\n",
    "        scores = temp_bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Sort documents by score\n",
    "        scored_docs = [(score, documents[i]) for i, score in enumerate(scores)]\n",
    "        sorted_docs = sorted(scored_docs, key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Return top k documents\n",
    "        return [doc for _, doc in sorted_docs[:self.k]]\n",
    "    \n",
    "class CustomRetrieverWithHistory:\n",
    "    \"\"\"A custom retriever that uses BM25 reranking and leverages query history\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, chat_history=None, k=10, rerank_k=4):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.chat_history = chat_history or []\n",
    "        self.reranker = BM25Reranker(k=rerank_k) \n",
    "        self.k = k\n",
    "    \n",
    "    def get_relevant_documents(self, query):\n",
    "        # First, get documents from vector store\n",
    "        docs = self.vectorstore.similarity_search(query, k=self.k)\n",
    "        \n",
    "        # If we have chat history, use it to enhance our retrieval\n",
    "        if self.chat_history and len(self.chat_history) > 0:\n",
    "            # Get the last few exchanges to provide context\n",
    "            recent_history = self.chat_history[-3:] if len(self.chat_history) > 3 else self.chat_history\n",
    "            history_context = \"\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in recent_history])\n",
    "            \n",
    "            # Combine with current query for broader retrieval\n",
    "            enhanced_query = f\"{query} {history_context}\"\n",
    "            additional_docs = self.vectorstore.similarity_search(enhanced_query, k=5)\n",
    "            \n",
    "            # Combine both sets of documents (removing duplicates)\n",
    "            unique_docs = {}\n",
    "            for doc in docs + additional_docs:\n",
    "                if doc.page_content not in unique_docs:\n",
    "                    unique_docs[doc.page_content] = doc\n",
    "            \n",
    "            docs = list(unique_docs.values())\n",
    "        \n",
    "        # Apply reranking to final set\n",
    "        return self.reranker.rerank(query, docs)\n",
    "\n",
    "    def update_history(self, query, answer):\n",
    "        \"\"\"Update chat history with new exchange\"\"\"\n",
    "        self.chat_history.append((query, answer))\n",
    "    \n",
    "    def save_history(self, file_path=BANK_DB_STATE_PATH):\n",
    "        \"\"\"Save chat history to a file\"\"\"\n",
    "        state = {\n",
    "            \"chat_history\": self.chat_history,\n",
    "            \"timestamp\": str(import datetime; datetime.datetime.now())\n",
    "        }\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(state, f)\n",
    "            \n",
    "    def load_history(self, file_path=BANK_DB_STATE_PATH):\n",
    "        \"\"\"Load chat history from a file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                state = json.load(f)\n",
    "                self.chat_history = state.get(\"chat_history\", [])\n",
    "                print(f\"Loaded {len(self.chat_history)} previous conversations\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"No previous history found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading history: {e}\")\n",
    "\n",
    "# Initialize our custom retriever\n",
    "retriever = CustomRetrieverWithHistory(vectorstore=vectorstore, k=10, rerank_k=4)\n",
    "\n",
    "# Load previous chat history if available\n",
    "retriever.load_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [ "## 7. Setup LLM, Prompts and Chain (French, RAG)" ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Setup LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"Vous êtes un assistant expert en documentation de base de données bancaire.\n",
    "Votre objectif est de fournir des informations précises et spécifiques sur l'emplacement des données dans le système de base de données de la banque.\n",
    "\n",
    "Informations contextuelles issues de la documentation de la base de données ci-dessous:\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "\n",
    "Compte tenu des informations contextuelles et de la question, fournissez une réponse détaillée qui indique clairement:\n",
    "1. La ou les table(s) exacte(s) où se trouvent les données demandées\n",
    "2. Les noms spécifiques des colonnes\n",
    "3. Les relations avec d'autres tables\n",
    "4. Les types de données et les contraintes si pertinent\n",
    "\n",
    "Important: À la fin de votre réponse, indiquez clairement les sources d'où proviennent ces informations en listant les noms des fichiers PDF sources.\n",
    "\n",
    "Soyez précis et technique. Si les informations ne peuvent pas être trouvées dans le contexte, reconnaissez-le clairement.\n",
    "N'inventez pas d'informations sur les noms de tables ou les structures.\n",
    "\n",
    "Historique de la conversation: {chat_history}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=RAG_TEMPLATE,\n",
    "    input_variables=[\"context\", \"chat_history\", \"question\"]\n",
    ")\n",
    "\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [ "## 8. Query Reformulation, Explanation, Commentary Functions (French)" ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_alternative_queries(query: str) -> list:\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Génère 3 façons alternatives de poser la question suivante à propos d'une base de données bancaire.\\nRends les alternatives spécifiques et concentrées sur la structure de la base de données et l'emplacement des données.\\nQuestion originale: {question}\\n\\nRetourne uniquement les questions, une par ligne.\"\"\",\n",
    "        input_variables=[\"question\"]\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(question=query)\n",
    "    alts = [q.strip() for q in result.split('\\n') if q.strip()]\n",
    "    return alts + [query]\n",
    "\n",
    "def generate_answer_explanation(query, answer, sources):\n",
    "    # Use only up to first 2 source texts for summary\n",
    "    sources_summary = \"\\n\".join([doc.page_content[:300] for doc in sources[:2]])\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"En tant qu'expert en base de données bancaire, explique comment tu as formulé la réponse suivante à la question posée.\\nFais référence aux données spécifiques et à la structure de la base de données bancaire.\\n\\nQuestion: {query}\\nRéponse: {answer}\\n\\nSources principales:\\n{sources}\\n\\nExplique ton raisonnement en français en te concentrant sur:\\n1. Comment les sources ont influencé ta réponse\\n2. Les éléments clés de la structure de la base de données mentionnés\\n3. Les relations entre les tables identifiées\\n4. Toute implication ou considération technique importante\\n\\nExplication (en 3-5 phrases):\"\"\",\n",
    "        input_variables=[\"query\", \"answer\", \"sources\"]\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    explanation = chain.run(query=query, answer=answer, sources=sources_summary)\n",
    "    return explanation.strip()\n",
    "\n",
    "def generate_technical_commentary(query, answer):\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"En tant qu'expert en bases de données bancaires, fournis un commentaire technique bref\\nsur les aspects de la structure de la base de données mentionnés dans cette question et réponse.\\n\\nQuestion: {query}\\nRéponse: {answer}\\n\\nFournis un commentaire technique (2-3 phrases) qui pourrait aider un développeur ou un analyste de données\\nà mieux comprendre les implications techniques de cette information:\"\"\",\n",
    "        input_variables=[\"query\", \"answer\"]\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    return chain.run(query=query, answer=answer).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation Functions & User Feedback"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_rag_response(query, answer, retrieved_docs, feedback=None):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of a RAG response based on various metrics\n",
    "    \"\"\"\n",
    "    evaluation = {}\n",
    "    \n",
    "    # 1. Document relevance score - Compare query with retrieved docs\n",
    "    query_tokens = set(query.lower().split())\n",
    "    relevant_docs = 0\n",
    "    \n",
    "    for doc in retrieved_docs:\n",
    "        doc_tokens = set(doc.page_content.lower().split())\n",
    "        overlap = len(query_tokens.intersection(doc_tokens)) / len(query_tokens) if query_tokens else 0\n",
    "        if overlap > 0.2:  # Simple relevance threshold\n",
    "            relevant_docs += 1\n",
    "    \n",
    "    doc_relevance = relevant_docs / len(retrieved_docs) if retrieved_docs else 0\n",
    "    evaluation[\"document_relevance\"] = round(doc_relevance * 10, 2)  # Score out of 10\n",
    "    \n",
    "    # 2. Source diversity - Are we pulling from different sources?\n",
    "    sources = set()\n",
    "    for doc in retrieved_docs:\n",
    "        if \"source_file\" in doc.metadata:\n",
    "            sources.add(doc.metadata[\"source_file\"])\n",
    "    \n",
    "    source_diversity = min(len(sources) / 3, 1.0)  # Normalize to max of 1\n",
    "    evaluation[\"source_diversity\"] = round(source_diversity * 10, 2)  # Score out of 10\n",
    "    \n",
    "    # 3. Answer relevance - Analyze how well the answer matches the query\n",
    "    # This requires LLM evaluation, creating a simplified heuristic\n",
    "    answer_tokens = set(answer.lower().split())\n",
    "    query_answer_overlap = len(query_tokens.intersection(answer_tokens)) / len(query_tokens) if query_tokens else 0\n",
    "    evaluation[\"answer_query_alignment\"] = round(query_answer_overlap * 10, 2)  # Score out of 10\n",
    "    \n",
    "    # 4. User feedback score if available\n",
    "    if feedback is not None:\n",
    "        evaluation[\"user_feedback\"] = feedback  # User score (1-5)\n",
    "    \n",
    "    # 5. Hallucination risk - Check if answer cites sources properly\n",
    "    contains_source_citation = any([\n",
    "        \"source\" in answer.lower(),\n",
    "        \"pdf\" in answer.lower(),\n",
    "        \"provien\" in answer.lower(),\n",
    "        \"extract\" in answer.lower()\n",
    "    ])\n",
    "    evaluation[\"hallucination_risk\"] = \"Low\" if contains_source_citation else \"Medium\"\n",
    "    \n",
    "    # Overall score (weighted average)\n",
    "    weights = {\"document_relevance\": 0.4, \"source_diversity\": 0.3, \"answer_query_alignment\": 0.3}\n",
    "    overall_score = sum(evaluation[k] * weights[k] for k in weights.keys())\n",
    "    evaluation[\"overall_score\"] = round(overall_score, 2)  # Score out of 10\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "class UserFeedbackTracker:\n",
    "    \"\"\"Track user feedback for responses\"\"\"\n",
    "    \n",
    "    def __init__(self, feedback_file=\"user_feedback.json\"):\n",
    "        self.feedback_file = feedback_file\n",
    "        self.feedback_history = self._load_feedback()\n",
    "    \n",
    "    def _load_feedback(self):\n",
    "        \"\"\"Load feedback history from file\"\"\"\n",
    "        try:\n",
    "            with open(self.feedback_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading feedback: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def save_feedback(self, query, answer, rating, comment=None):\n",
    "        \"\"\"Save user feedback\"\"\"\n",
    "        feedback_entry = {\n",
    "            \"timestamp\": str(import datetime; datetime.datetime.now()),\n",
    "            \"query\": query,\n",
    "            \"answer\": answer[:100] + \"...\",  # Store shortened answer\n",
    "            \"rating\": rating,\n",
    "            \"comment\": comment\n",
    "        }\n",
    "        \n",
    "        self.feedback_history.append(feedback_entry)\n",
    "        \n",
    "        with open(self.feedback_file, 'w') as f:\n",
    "            json.dump(self.feedback_history, f)\n",
    "        \n",
    "        print(f\"Feedback saved. Current rating: {rating}/5\")\n",
    "        return rating\n",
    "    \n",
    "    def get_average_rating(self):\n",
    "        \"\"\"Get average feedback rating\"\"\"\n",
    "        if not self.feedback_history:\n",
    "            return None\n",
    "        \n",
    "        ratings = [entry[\"rating\"] for entry in self.feedback_history if \"rating\" in entry]\n",
    "        return sum(ratings) / len(ratings) if ratings else None\n",
    "\n",
    "# Initialize feedback tracker\n",
    "feedback_tracker = UserFeedbackTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Process a Query Through the Full Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def extract_source_files(documents):\n",
    "    \"\"\"Extract source files from document metadata\"\"\"\n",
    "    sources = []\n",
    "    for doc in documents:\n",
    "        if \"source_file\" in doc.metadata:\n",
    "            sources.append(doc.metadata[\"source_file\"])\n",
    "        elif \"source\" in doc.metadata:\n",
    "            sources.append(doc.metadata[\"source\"])\n",
    "    return list(set(sources))\n",
    "\n",
    "def process_query(query: str):\n",
    "    # Generate alternative phrasings of the query\n",
    "    alt_queries = generate_alternative_queries(query)\n",
    "    all_results = []\n",
    "    \n",
    "    # Process each alternative query\n",
    "    for q in alt_queries:\n",
    "        res = rag_chain({\"question\": q, \"chat_history\": retriever.chat_history})\n",
    "        all_results.append(res)\n",
    "    \n",
    "    # Pick result with most source docs\n",
    "    best_result = max(all_results, key=lambda x: len(x['source_documents']))\n",
    "    answer, sources = best_result['answer'], best_result['source_documents']\n",
    "    \n",
    "    # Generate additional context\n",
    "    explanation = generate_answer_explanation(query, answer, sources)\n",
    "    commentary = generate_technical_commentary(query, answer)\n",
    "    evaluation = evaluate_rag_response(query, answer, sources)\n",
    "    \n",
    "    # Add source files information if not already in the answer\n",
    "    source_files = extract_source_files(sources)\n",
    "    if source_files and \"Sources:\" not in answer:\n",
    "        answer += f\"\\n\\nSources: {', '.join(source_files)}\"\n",
    "    \n",
    "    # Update chat history\n",
    "    retriever.update_history(query, answer)\n",
    "    retriever.save_history()\n",
    "    \n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'sources': sources,\n",
    "        'source_files': source_files,\n",
    "        'explanation': explanation,\n",
    "        'commentary': commentary,\n",
    "        'evaluation': evaluation\n",
    "    }\n",
    "\n",
    "def submit_feedback(query, answer, rating, comment=None):\n",
    "    \"\"\"Submit user feedback for a response\"\"\"\n",
    "    return feedback_tracker.save_feedback(query, answer, rating, comment)\n",
    "\n",
    "# Usage example:\n",
    "user_query = \"Dans quelle table puis-je trouver les informations sur les transactions clients ?\"\n",
    "result = process_query(user_query)\n",
    "\n",
    "print('Réponse:', result['answer'])\n",
    "print('\\nExplication:', result['explanation'])\n",
    "print('\\nCommentaire technique:', result['commentary'])\n",
    "print('\\nÉvaluation:', result['evaluation'])\n",
    "print('\\nSources des documents:', result['source_files'])\n",
    "\n",
    "# Example of submitting feedback\n",
    "# submit_feedback(user_query, result['answer'], 5, \"Excellente réponse!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
