import chromadb
from typing import List
from FlagEmbedding import FlagModel

# Initialize Chroma client with proper configuration
chroma_client = chromadb.PersistentClient(
    path="chroma_db",
    settings=chromadb.config.Settings(
        chroma_db_impl="duckdb+parquet",  # More reliable for most use cases
        persist_directory="chroma_db",
        allow_reset=True,
        is_persistent=True,
        migrations="apply"  # Ensure schema migrations are handled
    )
)

# Initialize BGE-M3 model
model_path = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"
model = FlagModel(
    model_path,
    query_instruction_for_retrieval="Generate representation for this sentence to retrieve relevant articles:",
    use_fp16=False  # Disable FP16 if not using compatible GPU
)

# Create compliant embedding function with dimension validation
class BGE_M3_EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        self.dimension = 1024  # BGE-M3 embedding dimension
        
    def __call__(self, input: List[str]) -> List[List[float]]:
        if isinstance(input, str):
            input = [input]
        embeddings = self.model.encode(input).tolist()
        # Validate dimension consistency
        if len(embeddings[0]) != self.dimension:
            raise ValueError(f"Embedding dimension mismatch. Expected {self.dimension}, got {len(embeddings[0])}")
        return embeddings

# Initialize embedding function
embedding_function = BGE_M3_EmbeddingFunction(model)

# Create collection with proper configuration
try:
    # Try to delete existing collection if it has invalid configuration
    chroma_client.delete_collection("pdf_documents3")
except Exception as e:
    print(f"Cleanup warning: {str(e)}")

collection = chroma_client.get_or_create_collection(
    name="pdf_documents3",
    embedding_function=embedding_function,
    metadata={
        "hnsw:space": "cosine",
        "dimension": embedding_function.dimension,  # Direct reference
        "hnsw:construction_ef": 200,
        "hnsw:M": 16,
        "hnsw:max_elements": 1000000  # Adjust based on your needs
    }
)

# Verify collection configuration
print("Collection metadata:", collection.metadata)
print("Collection count:", collection.count())

# Query test with validation
try:
    results = collection.query(
        query_texts=["example query"],
        n_results=20,
        include=["documents", "embeddings"]
    )
    print("Query results:", results)
except Exception as e:
    print("Query failed:", str(e))
    if "embeddings" in locals():
        print("First embedding dimension:", len(results["embeddings"][0][0]))





import chromadb
from typing import List
from FlagEmbedding import FlagModel

# Initialize Chroma client
chroma_client = chromadb.PersistentClient(path="chroma_db")

# Initialize BGE-M3 model
model_path = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"
model = FlagModel(
    model_path,
    query_instruction_for_retrieval="Generate representation for this sentence to retrieve relevant articles:",
    use_fp16=True  # Enable if using GPU
)

# Create compliant embedding function
class BGE_M3_EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        
    def __call__(self, input: List[str]) -> List[List[float]]:
        # Convert single string to list
        if isinstance(input, str):
            input = [input]
            
        # Get embeddings (automatically batches inputs)
        embeddings = self.model.encode(input).tolist()
        return embeddings

# Create embedding function instance
bge_ef = BGE_M3_EmbeddingFunction(model)

# Create collection with dimension metadata
collection = chroma_client.get_or_create_collection(
    name="pdf_documents3",
    embedding_function=bge_ef,
    metadata={"hnsw:space": "cosine", "dimension": 1024}
)

# Query the collection
results = collection.query(
    query_texts=["example query"],
    n_results=20,
    include=["documents"]
)

print(results)








import chromadb
from typing import List
from FlagEmbedding import FlagModel

# Initialize Chroma client
chroma_client = chromadb.PersistentClient(path="chroma_db")

# Initialize BGE-M3 model
model_path = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"
model = FlagModel(
    model_path,
    query_instruction_for_retrieval="Generate representation for this sentence to retrieve relevant articles:",
    use_fp16=True  # Enable if using GPU
)

# Create compliant embedding function
class BGE_M3_EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        
    def __call__(self, input: List[str]) -> List[List[float]]:
        # Convert single string to list
        if isinstance(input, str):
            input = [input]
            
        # Get embeddings (automatically batches inputs)
        embeddings = self.model.encode(input).tolist()
        return embeddings

# Create embedding function instance
bge_ef = BGE_M3_EmbeddingFunction(model)

# Create collection with dimension metadata
collection = chroma_client.get_or_create_collection(
    name="pdf_documents3",
    embedding_function=bge_ef,
    metadata={"hnsw:space": "cosine", "dimension": 1024}
)

# Query the collection
results = collection.query(
    query_texts=["example query"],
    n_results=20,
    include=["documents"]
)

print(results)



import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from FlagEmbedding import FlagModel

# Initialize Chroma client
chroma_client = chromadb.PersistentClient(path="chroma_db")

# Initialize BGE-M3 model
model_path = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"
model = FlagModel(
    model_path,
    query_instruction_for_retrieval="Generate representation for this sentence to retrieve relevant articles:",
    use_fp16=True  # Enable if using GPU
)

# Create custom embedding function
def bge_m3_embedding_function(texts):
    return model.encode(texts).tolist()

# Create collection with custom embedding function
collection = chroma_client.get_or_create_collection(
    name="pdf_documents3",
    embedding_function=bge_m3_embedding_function
)

# Query the collection
results = collection.query(
    query_texts=["example query"],
    n_results=20,
    include=["documents"]
)

print(results)





import streamlit as st
from your_rag_functions import (  # Replace with your actual imports
    generate_alternative_queries,
    generate_response,
    generate_answer_explanation,
    evaluate_rag_response,
    extract_source_files
)

# Initialize session state for chat history
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "current_chat" not in st.session_state:
    st.session_state.current_chat = []

# Initialize chat sessions
if "chat_sessions" not in st.session_state:
    st.session_state.chat_sessions = {"New Chat": []}

def rag_chatbot(query):
    # Your existing RAG processing logic
    all_queries = [query] + generate_alternative_queries(query)
    all_results = []

    for q in all_queries:
        context = retriever.get_relevant_documents(q)
        answer = generate_response(q, context)
        all_results.append({
            "query": q,
            "answer": answer,
            "context": context,
            "num_sources": len(context)
        })

    best_result = max(all_results, key=lambda x: x["num_sources"])
    explanation = generate_answer_explanation(best_result["query"], best_result["answer"], best_result["context"])
    evaluation = evaluate_rag_response(best_result["query"], best_result["answer"], best_result["context"])
    sources = extract_source_files(best_result["context"])

    # Update history
    retriever.update_history(best_result["query"], best_result["answer"])
    retriever.save_history()

    return {
        "original_query": query,
        "alternative_queries": all_queries[1:],
        "final_query": best_result["query"],
        "answer": best_result["answer"],
        "explanation": explanation,
        "evaluation": evaluation,
        "sources": sources
    }

# Sidebar for chat sessions
with st.sidebar:
    st.header("Chat Sessions")
    
    # Create new chat
    if st.button("+ New Chat"):
        new_chat_name = f"Chat {len(st.session_state.chat_sessions) + 1}"
        st.session_state.chat_sessions[new_chat_name] = []
        st.session_state.current_chat = new_chat_name
    
    # Display existing chats
    selected_chat = st.radio(
        "Select Chat",
        options=list(st.session_state.chat_sessions.keys()),
        index=0
    )
    st.session_state.current_chat = selected_chat

# Main chat interface
st.title("RAG Chatbot")

# Display chat history
chat_container = st.container()
with chat_container:
    for message in st.session_state.chat_sessions[st.session_state.current_chat]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if message["role"] == "assistant":
                with st.expander("Details"):
                    st.write("**Sources:**", message["sources"])
                    st.write("**Evaluation:**", message["evaluation"])

# Input area
user_input = st.chat_input("Type your message here...")
if user_input:
    # Add user message to chat history
    st.session_state.chat_sessions[st.session_state.current_chat].append({
        "role": "user",
        "content": user_input
    })
    
    # Process query
    with st.spinner("Thinking..."):
        response = rag_chatbot(user_input)
    
    # Add assistant response to chat history
    st.session_state.chat_sessions[st.session_state.current_chat].append({
        "role": "assistant",
        "content": response["answer"],
        "sources": response["sources"],
        "evaluation": response["evaluation"]
    })
    
    # Rerun to update the chat display
    st.rerun()
