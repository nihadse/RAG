import os
import pandas as pd

# Folder containing your "JSON" files
json_folder = "structured_json_folder"
output_file = "structured_data.xlsx"

# Columns in desired order
columns_order = [
    "Monnaie",
    "Montant en chiffres",
    "Montant en lettres",
    "Nom/Pr√©nom",
    "Adresse compl√®te",
    "Num√©ro de compte",
    "Adresse SWIFT",
    "source_file"  # NEW column for filename
]

all_data = []

for filename in os.listdir(json_folder):
    if filename.startswith("structure_") and filename.endswith(".json"):
        path = os.path.join(json_folder, filename)
        with open(path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        row = {col: "" for col in columns_order}  # initialize empty row
        row["source_file"] = filename  # add filename

        for line in lines:
            line = line.strip()
            if not line:
                continue

            for col in columns_order[:-1]:  # skip 'source_file'
                if line.startswith(col):
                    value = line[len(col):].strip()
                    value = value.strip('"')
                    if col == "Montant en chiffres":
                        value = value.replace(",", ".")
                    row[col] = value

        all_data.append(row)

# Convert to DataFrame
df = pd.DataFrame(all_data, columns=columns_order)

# Save to Excel
df.to_excel(output_file, index=False, engine="openpyxl")

print("‚úÖ Excel file created successfully with filenames!")




import os
import pandas as pd

# Folder containing your "JSON" files
json_folder = "structured_json_folder"
output_file = "structured_data.xlsx"

# Columns in desired order
columns_order = [
    "Monnaie",
    "Montant en chiffres",
    "Montant en lettres",
    "Nom/Pr√©nom",
    "Adresse compl√®te",
    "Num√©ro de compte",
    "Adresse SWIFT"
]

all_data = []

for filename in os.listdir(json_folder):
    if filename.startswith("structure_") and filename.endswith(".json"):
        path = os.path.join(json_folder, filename)
        with open(path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        row = {col: "" for col in columns_order}  # initialize empty row

        for line in lines:
            line = line.strip()
            # Skip empty lines
            if not line:
                continue

            for col in columns_order:
                # If line starts with the column name
                if line.startswith(col):
                    # Remove the column name from the line
                    value = line[len(col):].strip()
                    # Remove surrounding quotes if any
                    value = value.strip('"')
                    # Fix comma decimal to dot
                    if col == "Montant en chiffres":
                        value = value.replace(",", ".")
                    row[col] = value

        all_data.append(row)

# Convert to DataFrame
df = pd.DataFrame(all_data, columns=columns_order)

# Save to Excel
df.to_excel(output_file, index=False, engine="openpyxl")

print("‚úÖ Excel file created successfully!")




import os
import json
import re
import pandas as pd

json_folder = "structured_json_folder"
output_file = "structured_data.xlsx"

# Desired columns and order
columns_order = [
    "Monnaie",
    "Montant en chiffres",
    "Montant en lettres",
    "Nom/Pr√©nom",
    "Adresse compl√®te",
    "Num√©ro de compte",
    "Adresse SWIFT"
]

all_data = []

def extract_json(text):
    """
    Extract valid JSON object from messy text.
    """
    match = re.search(r'\{.*\}', text, re.DOTALL)
    if match:
        json_text = match.group()
        # Fix broken quotes
        json_text = json_text.replace('\\"', '"').replace('""', '"')
        try:
            return json.loads(json_text)
        except:
            return None
    return None

for filename in os.listdir(json_folder):
    if filename.startswith("structure_") and filename.endswith(".json"):
        path = os.path.join(json_folder, filename)
        with open(path, "r", encoding="utf-8") as f:
            raw = f.read()

        data = extract_json(raw)
        if data:
            # Keep only desired columns
            row = {col: data.get(col, "") for col in columns_order}
            all_data.append(row)

# Convert to DataFrame
df = pd.DataFrame(all_data, columns=columns_order)

# Save to Excel
df.to_excel(output_file, index=False, engine="openpyxl")

print("‚úÖ Excel file created successfully!")










import os
import json
import pandas as pd
from openpyxl import Workbook

# Path to your JSON folder
json_folder = "json_folder"

# List to store all JSON data
all_data = []

# Loop through files
for filename in os.listdir(json_folder):
    if filename.startswith("structure_") and filename.endswith(".json"):
        file_path = os.path.join(json_folder, filename)
        
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            all_data.append(data)

# Convert to DataFrame
df = pd.DataFrame(all_data)

# Save to Excel using openpyxl
output_file = "output.xlsx"
df.to_excel(output_file, index=False, engine="openpyxl")

print("Excel file created successfully!")





import glob
import os
import json
import pandas as pd
from datetime import datetime

STRUCTURED_FOLDER = "Dossier_agence/structured_json"
OUTPUT_EXCEL = "Dossier_agence/Final_output.xlsx"

print("Start:", datetime.now())

json_files = glob.glob(os.path.join(STRUCTURED_FOLDER, "*.json"))
print(f"Found {len(json_files)} structured JSON files.")

all_rows = []

for json_path in json_files:

    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    file_name = os.path.basename(json_path).replace("structured_", "").replace(".json", "")

    # If single dictionary
    if isinstance(data, dict):
        data["File Name"] = file_name
        all_rows.append(data)

    # If list of dictionaries
    elif isinstance(data, list):
        for item in data:
            if isinstance(item, dict):
                item["File Name"] = file_name
                all_rows.append(item)

df = pd.DataFrame(all_rows)

df.to_excel(OUTPUT_EXCEL, index=False)

print(f"Excel file created at: {OUTPUT_EXCEL}")
print("Finished:", datetime.now())





import glob
import os
import json
import pandas as pd
from datetime import datetime

print("Start:", datetime.now())

json_files = glob.glob(os.path.join(STRUCTURED_FOLDER, "*.json"))
print(f"Found {len(json_files)} structured JSON files.")

all_rows = []

for json_path in json_files:

    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    file_name = os.path.basename(json_path).replace("structured_", "").replace(".json", "")

    # ‚úÖ If JSON is a dictionary
    if isinstance(data, dict):
        data["File Name"] = file_name
        all_rows.append(data)

    # ‚úÖ If JSON is a list of dictionaries
    elif isinstance(data, list):
        for item in data:
            if isinstance(item, dict):
                item["File Name"] = file_name
                all_rows.append(item)

df = pd.DataFrame(all_rows)

df.to_excel(OUTPUT_EXCEL, index=False)

print(f"Excel file created at: {OUTPUT_EXCEL}")
print("Finished:", datetime.now())




.def parse_json_from_output(text: str) -> dict:
    data = {}

    lines = text.splitlines()

    for line in lines:
        line = line.strip()

        if ":" in line:
            try:
                key, value = line.split(":", 1)

                key = key.strip().strip('"')
                value = value.strip().strip('"').strip(",")

                if key:
                    data[key] = value

            except:
                continue

    return data




import json
import re

def parse_json_from_output(text: str) -> dict:

    text = text.strip()

    # Wrap with braces if missing
    if not text.startswith("{"):
        text = "{" + text

    if not text.endswith("}"):
        text = text.rstrip(",") + "}"

    # Remove trailing commas before }
    text = re.sub(r",\s*}", "}", text)

    try:
        return json.loads(text)
    except json.JSONDecodeError as e:
        print("==== BROKEN JSON ====")
        print(text)
        print("=====================")
        raise e







import json
import re

def parse_json_from_output(text: str) -> dict:

    # Remove assistant prefix if exists
    if "<|im_start|>assistant" in text:
        text = text.split("<|im_start|>assistant")[-1]

    text = text.strip()

    # Try to extract JSON block
    json_match = re.search(r"\{.*\}", text, re.DOTALL)
    if json_match:
        return json.loads(json_match.group(0))

    # üî• If no braces but looks like JSON fields ‚Üí wrap it
    if ":" in text and "{" not in text:
        cleaned = "{" + text.strip().rstrip(",") + "}"
        return json.loads(cleaned)

    raise ValueError(f"No valid JSON found in model output:\n{text[:500]}")









import os
import json

os.makedirs(OUTPUT_STRUCTURED_FOLDER, exist_ok=True)

all_rows = []

for filename in os.listdir(INPUT_FOLDER):

    if filename.endswith(".json"):

        file_path = os.path.join(INPUT_FOLDER, filename)

        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        extracted_values = extract_structured_data(ocr_pages)

        print(json.dumps(extracted_values, indent=2, ensure_ascii=False))

        # ‚úÖ Create output file path (important fix)
        output_file_path = os.path.join(
            OUTPUT_STRUCTURED_FOLDER,
            f"structured_{filename}"
        )

        # ‚úÖ Save JSON inside folder (not the folder itself)
        with open(output_file_path, "w", encoding="utf-8") as f:
            json.dump(extracted_values, f, indent=4, ensure_ascii=False)

        print(f"Done! Structured data saved to {output_file_path}")






structured_list = []

for item in extracted_values:
    parts = item.split("\n")
    data_dict = {}
    i = 0
    while i < len(parts):
        key = parts[i].strip()
        if key in fields:
            # The next line is the value
            if i + 1 < len(parts):
                value = parts[i + 1].strip()
                data_dict[key] = value
                i += 2
            else:
                i += 1
        else:
            i += 1
    structured_list.append(data_dict)

# Save as JSON file
with open("structured_output.json", "w", encoding="utf-8") as f:
    json.dump(structured_list, f, indent=2, ensure_ascii=False)

print("JSON file saved successfully!")





def map_values_to_fields(raw_text: str):
    values = [v.strip() if v.strip() != "" else None for v in raw_text.split("I")]
    structured_dict = {field: values[i] if i < len(values) else None for i, field in enumerate(BANK_FIELDS)}
    return structured_dict









import os
import json
import re

INPUT_FOLDER = "/mnt/Dossier_agence/json_first_page"
OUTPUT_STRUCTURED_FOLDER = "/mnt/Dossier_agence/structured_json"
os.makedirs(OUTPUT_STRUCTURED_FOLDER, exist_ok=True)

def parse_extracted_text(text):
    """
    Convert messy extracted text into a structured dictionary.
    """
    structured_dict = {}

    # Fix common JSON-like parts
    text = text.replace('\n', ' ').replace('"', '')
    
    # Extract key-value pairs like 'Key: Value'
    pattern = r'(\b[A-Za-z√©√®√ß√†√â√à√á√Ä ]+)\s*[:]\s*([^:]+?)(?=\s+[A-Za-z√©√®√ß√†√â√à√á√Ä ]+[:]|$)'
    matches = re.findall(pattern, text)
    
    for key, value in matches:
        structured_dict[key.strip()] = value.strip()

    return structured_dict

for filename in os.listdir(INPUT_FOLDER):
    if filename.endswith(".json"):
        file_path = os.path.join(INPUT_FOLDER, filename)
        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        # Extracted text from your tokenizer function
        extracted_text = extract_structured_data(ocr_pages)

        if not extracted_text:
            print(f"No data extracted from {filename}")
            continue

        # Parse messy text to structured dictionary
        structured_dict = parse_extracted_text(extracted_text)

        # Save as JSON
        structured_file_path = os.path.join(
            OUTPUT_STRUCTURED_FOLDER, f"{os.path.splitext(filename)[0]}_structured.json"
        )
        with open(structured_file_path, "w", encoding="utf-8") as sf:
            json.dump(structured_dict, sf, ensure_ascii=False, indent=4)

print("All structured JSON files saved!")












structured_dict = {}
        # This regex splits lines like 'Key: Value'
        for line in extracted_text.split("\n"):
            if ":" in line:
                key, value = line.split(":", 1)
                structured_dict[key.strip()] = value.strip()

        # Save as JSON
        structured_file_path = os.path.join(
            OUTPUT_STRUCTURED_FOLDER, f"{os.path.splitext(filename)[0]}_structured.json"
        )
        with open(structured_file_path, "w", encoding="utf-8") as sf:
            json.dump(structured_dict, sf, ensure_ascii=False, indent=4)




file
        structured_file_path = os.path.join(
            OUTPUT_STRUCTURED_FOLDER, f"{os.path.splitext(filename)[0]}_structured.json"
        )
        with open(structured_file_path, "w", encoding="utf-8") as sf:
            json.dump(extracted_values, sf, ensure_ascii=False, indent=4)




{
  "Monnaie": "DZD",
  "Montant en chiffres": "318,060.13",
  "Montant en lettres": "trois cent dix-huit mille soixante virgule treize Dinars",
  "Agence": "MOSTAGANEM",
  "Nom/Pr√©nom": "ALI SAYED HASMAT",
  "Adresse compl√®te": "ARATI, PATSAWRA, PASCHIM BARDHAMAN, DURGAPUR, WEST BENGAL PIN - 713363",
  "Num√©ro de compte": "50100692108329",
  "Date de valeur": "05/02/2026",
  "bank code": "073",
  "Si√®ge": "027",
  "Racine": "007",
  "Ordinal": "310",
  "Devise": "DZD",
  "Cl√©": "110526",
  "Adresse SWIFT": "HDFC INBB",
  "Autre (Transfert en EURO)": null
}





import json
import os
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONFIG
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

INPUT_FOLDER = "/mnt/Dossier_agence/json_first_page"
OUTPUT_STRUCTURED_FOLDER = "/mnt/Dossier_agence/structured_json"

BANK_FIELDS = [
    "Monnaie", "Montant en chiffres", "Montant en lettres", "Agence",
    "Nom/Pr√©nom", "Adresse compl√®te", "Num√©ro de compte", "Date de valeur",
    "bank code", "Si√®ge", "Racine", "Ordinal", "Devise", "Cl√©",
    "Adresse SWIFT", "Autre (Transfert en EURO)",
]

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# LOAD MODEL
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("‚úì Model loaded\n")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FIXED PROMPT (DON'T SHOW FIELD NAMES)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
def build_prompt(text: str) -> str:
    """Build prompt WITHOUT showing field names to avoid confusion"""
    
    prompt = f"""Extract bank transfer information from this document.

Return values in this EXACT order, separated by "|":
1. Currency (Monnaie)
2. Amount in numbers
3. Amount in words
4. Agency name
5. Beneficiary name
6. Complete address
7. Account number
8. Value date
9. Bank code (3 digits)
10. Si√®ge (5 digits)
11. Racine (6 digits)
12. Ordinal (3 digits)
13. Devise (letters)
14. Cl√© (2 digits)
15. SWIFT address
16. Other (Transfer in EURO)

Rules:
- Return ONLY the values separated by "|"
- Use "null" if a field is missing
- No field names, no explanations

Example format:
DZD | 318060.13 | trois cent dix-huit mille... | MOSTAGANEM | ALI SAYED HASMAT | ARATI... | 50100692 | 05/02/2026 | 073 | 10110 | 526001 | 261 | DZD | 52 | HDFC INBB | null

Document text:
{text[:700]}
"""
    return prompt.strip()

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# EXTRACTION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
def extract_structured_data(ocr_pages: list) -> dict:
    """Extract from OCR pages"""
    # Get text
    if isinstance(ocr_pages, dict):
        full_text = ocr_pages.get("text", "")
    else:
        full_text = "\n".join(p.get("text", "") for p in ocr_pages if isinstance(p, dict) and p.get("text"))
    
    if not full_text.strip():
        raise ValueError("No text found")
    
    # Build prompt (NO field names shown)
    prompt = build_prompt(full_text)
    
    # Generate
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1500).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    # Decode
    result = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    # Parse
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    structured = {}
    for i, field in enumerate(BANK_FIELDS):
        if i < len(values):
            val = values[i]
            structured[field] = None if val.lower() == "null" or val == "" else val
        else:
            structured[field] = None
    
    return structured

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# BATCH PROCESS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
os.makedirs(OUTPUT_STRUCTURED_FOLDER, exist_ok=True)

print(f"Processing files from: {INPUT_FOLDER}")
print(f"Start: {datetime.now()}\n")

success = 0
errors = 0

for filename in os.listdir(INPUT_FOLDER):
    if filename.endswith(".json"):
        file_path = os.path.join(INPUT_FOLDER, filename)
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                ocr_pages = json.load(f)
            
            print(f"Processing: {filename}...", end=" ")
            
            # Extract
            extracted = extract_structured_data(ocr_pages)
            
            # Save
            output_file = filename.replace(".json", "_structured.json")
            output_path = os.path.join(OUTPUT_STRUCTURED_FOLDER, output_file)
            
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(extracted, f, indent=4, ensure_ascii=False)
            
            print("‚úì")
            success += 1
            
        except Exception as e:
            print(f"‚úó Error: {e}")
            errors += 1

print(f"\nFinished: {datetime.now()}")
print(f"Success: {success} | Errors: {errors}")





extracted_values = extract_structured_data(ocr_pages)

# Clean split and strip
if isinstance(extracted_values, str):
    values = [v.strip() for v in extracted_values.split("|")]
else:
    values = [v.strip() for v in extracted_values]

# Safety: fix length mismatch
if len(values) != len(BANK_FIELDS):
    values += ["null"] * (len(BANK_FIELDS) - len(values))





def build_prompt(text: str) -> str:
    order = " | ".join(BANK_FIELDS)

    prompt = f"""
You are an expert document information extraction system.

Extract the following fields in EXACT order separated by "|":

{order}

Rules for bank documents:
- bank code = 3 numbers
- Si√®ge = 5 numbers
- Racine = 6 numbers
- Ordinal = 3 numbers
- Devise = letters
- Cl√© = 2 numbers

Return ONLY values separated by "|".
If a value is missing, return null.
No explanations, no headers.

OCR:
{text}
"""
    return prompt.strip()








INPUT_FOLDER = "/mnt/json_folder"
OUTPUT_STRUCTURED_FOLDER = "/mnt/structured_output"
OUTPUT_EXCEL = "/mnt/final_output.xlsx"

os.makedirs(OUTPUT_STRUCTURED_FOLDER, exist_ok=True)

all_rows = []

for filename in os.listdir(INPUT_FOLDER):

    if filename.endswith(".json"):

        file_path = os.path.join(INPUT_FOLDER, filename)

        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        extracted_values = extract_structured_data(ocr_pages)

        row_dict = dict(zip(BANK_FIELDS, extracted_values))
        row_dict["source_file"] = filename

        # ‚úÖ Save individual structured JSON
        output_json_path = os.path.join(
            OUTPUT_STRUCTURED_FOLDER,
            filename.replace(".json", "_structured.json")
        )

        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(row_dict, f, indent=2, ensure_ascii=False)

        all_rows.append(row_dict)

# ‚úÖ Save global Excel
df = pd.DataFrame(all_rows)
df.to_excel(OUTPUT_EXCEL, index=False)

print("‚úÖ All files processed.")















INPUT_FOLDER = "/mnt/json_folder"
OUTPUT_STRUCTURED_FOLDER = "/mnt/structured_output"
OUTPUT_EXCEL = "/mnt/final_output.xlsx"

os.makedirs(OUTPUT_STRUCTURED_FOLDER, exist_ok=True)

all_rows = []

for filename in os.listdir(INPUT_FOLDER):

    if filename.endswith(".json"):

        file_path = os.path.join(INPUT_FOLDER, filename)

        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        extracted_values = extract_structured_data(ocr_pages)

        row_dict = dict(zip(BANK_FIELDS, extracted_values))
        row_dict["source_file"] = filename

        # ‚úÖ Save individual structured JSON
        output_json_path = os.path.join(
            OUTPUT_STRUCTURED_FOLDER,
            filename.replace(".json", "_structured.json")
        )

        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(row_dict, f, indent=2, ensure_ascii=False)

        all_rows.append(row_dict)

# ‚úÖ Save global Excel
df = pd.DataFrame(all_rows)
df.to_excel(OUTPUT_EXCEL, index=False)

print("‚úÖ All files processed.")














def extract_structured_data(ocr_pages: list) -> list:
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))

    prompt = build_prompt(full_text)

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=800)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            use_cache=True,
            num_beams=1,
        )

    structured = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()

    # Convert to list
    values = structured.split("|")

    # Ensure correct length
    if len(values) != len(BANK_FIELDS):
        values = values + ["null"] * (len(BANK_FIELDS) - len(values))

    return values





INPUT_FOLDER = "/mnt/json_folder"
OUTPUT_EXCEL = "/mnt/final_output.xlsx"

all_rows = []

for filename in os.listdir(INPUT_FOLDER):

    if filename.endswith(".json"):

        file_path = os.path.join(INPUT_FOLDER, filename)

        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        extracted_values = extract_structured_data(ocr_pages)

        # Create row dictionary
        row_dict = dict(zip(BANK_FIELDS, extracted_values))

        # Optional: keep file name
        row_dict["source_file"] = filename

        all_rows.append(row_dict)

# Convert to DataFrame
df = pd.DataFrame(all_rows)

# Save Excel
df.to_excel(OUTPUT_EXCEL, index=False)

print("‚úÖ All files processed successfully.")
print(f"Excel saved at: {OUTPUT_EXCEL}")












INPUT_FOLDER = "/mnt/json_folder"
OUTPUT_EXCEL = "/mnt/final_output.xlsx"

all_rows = []

for filename in os.listdir(INPUT_FOLDER):

    if filename.endswith(".json"):

        file_path = os.path.join(INPUT_FOLDER, filename)

        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        extracted_values = extract_structured_data(ocr_pages)

        # Create row dictionary
        row_dict = dict(zip(BANK_FIELDS, extracted_values))

        # Optional: keep file name
        row_dict["source_file"] = filename

        all_rows.append(row_dict)

# Convert to DataFrame
df = pd.DataFrame(all_rows)

# Save Excel
df.to_excel(OUTPUT_EXCEL, index=False)

print("‚úÖ All files processed successfully.")
print(f"Excel saved at: {OUTPUT_EXCEL}")





INPUT_FOLDER = "/mnt/json_folder"
OUTPUT_EXCEL = "/mnt/final_output.xlsx"

all_rows = []

for filename in os.listdir(INPUT_FOLDER):

    if filename.endswith(".json"):

        file_path = os.path.join(INPUT_FOLDER, filename)

        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        extracted_values = extract_structured_data(ocr_pages)

        # Create row dictionary
        row_dict = dict(zip(BANK_FIELDS, extracted_values))

        # Optional: keep file name
        row_dict["source_file"] = filename

        all_rows.append(row_dict)

# Convert to DataFrame
df = pd.DataFrame(all_rows)

# Save Excel
df.to_excel(OUTPUT_EXCEL, index=False)

print("‚úÖ All files processed successfully.")
print(f"Excel saved at: {OUTPUT_EXCEL}")




import zipfile
import os

# Path to your zip file
zip_path = "your_folder.zip"

# Where you want to extract the files
extract_path = "extracted_files"

# Create extraction folder if it doesn't exist
os.makedirs(extract_path, exist_ok=True)

# Unzip
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Extraction completed.")




import json
import glob
import os
import torch
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONFIGURATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

INPUT_FOLDER = "/mnt/Dossier_agence/json_first_page/"
OUTPUT_FOLDER = "/mnt/Dossier_agence/structured_json/"

# Bank document fields (in extraction order)
BANK_FIELDS = [
    "Monnaie",
    "Montant en chiffres",
    "Montant en lettres",
    "Agence",
    "Nom/Pr√©nom",
    "Adresse compl√®te",
    "Num√©ro de compte",
    "Date de valeur",
    "bank code",
    "Si√®ge",
    "Racine",
    "Ordinal",
    "Devise",
    "Cl√©",
    "Adresse SWIFT",
    "Autre (Transfert en EURO)",
]

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# LOAD MODEL (ONCE)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
print("=" * 80)
print("LOADING MODEL")
print("=" * 80)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("‚úì Model loaded successfully\n")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# EXTRACTION FUNCTIONS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def build_prompt(text: str) -> str:
    """
    Build extraction prompt for bank documents
    
    Args:
        text: OCR text from document
        
    Returns:
        Formatted prompt string
    """
    fields_order = " | ".join(BANK_FIELDS)
    
    prompt = f"""You are an expert document information extraction system.
Extract the following fields in EXACT order separated by "|":

Important field format rules:
- bank code: 3 numbers
- Si√®ge: 5 numbers
- Racine: 6 numbers
- Ordinal: 3 numbers
- Devise: letters
- Cl√©: 2 numbers

{fields_order}

Return ONLY values separated by "|".
If a value is missing, return null.
No explanations, no field names.

OCR:
{text[:1000]}
"""
    return prompt.strip()


def extract_from_ocr(ocr_pages: list) -> dict:
    """
    Extract structured data from OCR pages
    
    Args:
        ocr_pages: List of page dicts with "text" key
        
    Returns:
        Dictionary with extracted fields
    """
    # Step 1: Merge all page text
    if isinstance(ocr_pages, dict):
        # Handle single page (not in array)
        full_text = ocr_pages.get("text", "")
    elif isinstance(ocr_pages, list):
        # Handle multiple pages
        full_text = "\n".join(
            p.get("text", "") for p in ocr_pages 
            if isinstance(p, dict) and p.get("text")
        )
    else:
        raise ValueError(f"Unexpected OCR format: {type(ocr_pages)}")
    
    if not full_text.strip():
        raise ValueError("No text found in OCR pages")
    
    # Step 2: Build prompt
    prompt = build_prompt(full_text)
    
    # Step 3: Tokenize
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=1500
    ).to(DEVICE)
    
    # Step 4: Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    # Step 5: Decode
    generated_text = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    # Step 6: Parse pipe-separated values
    values = [v.strip() for v in generated_text.split("|")]
    
    # Step 7: Build result dictionary
    result = {}
    for i, field_name in enumerate(BANK_FIELDS):
        if i < len(values):
            val = values[i]
            result[field_name] = None if val.lower() == "null" or val == "" else val
        else:
            result[field_name] = None
    
    return result

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# BATCH PROCESSING
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Create output folder
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

# Find all JSON files
json_files = sorted(glob.glob(f"{INPUT_FOLDER}/*.json"))

print("=" * 80)
print(f"BATCH PROCESSING: {len(json_files)} FILES")
print("=" * 80)
print(f"Input:  {INPUT_FOLDER}")
print(f"Output: {OUTPUT_FOLDER}")
print(f"Start:  {datetime.now()}\n")

success_count = 0
error_count = 0
results = []

for idx, json_path in enumerate(json_files, 1):
    file_name = os.path.basename(json_path).replace(".json", "")
    
    print(f"[{idx}/{len(json_files)}] {file_name}...")
    
    try:
        # Load JSON
        with open(json_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)
        
        # Debug info
        print(f"  ‚îú‚îÄ Type: {type(ocr_pages)}")
        if isinstance(ocr_pages, list):
            print(f"  ‚îú‚îÄ Pages: {len(ocr_pages)}")
            if len(ocr_pages) > 0:
                first_text = ocr_pages[0].get("text", "")[:80]
                print(f"  ‚îú‚îÄ Preview: {first_text}...")
        elif isinstance(ocr_pages, dict):
            first_text = ocr_pages.get("text", "")[:80]
            print(f"  ‚îú‚îÄ Preview: {first_text}...")
        
        # Extract
        structured_dict = extract_from_ocr(ocr_pages)
        
        # Save
        output_path = os.path.join(OUTPUT_FOLDER, f"{file_name}_structured.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(structured_dict, f, indent=4, ensure_ascii=False)
        
        print(f"  ‚îî‚îÄ ‚úì SUCCESS\n")
        success_count += 1
        results.append({"file": file_name, "status": "success"})
        
    except Exception as e:
        print(f"  ‚îî‚îÄ ‚úó ERROR: {str(e)}\n")
        error_count += 1
        results.append({"file": file_name, "status": "error", "error": str(e)})

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# SUMMARY
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
print("=" * 80)
print("BATCH PROCESSING COMPLETE")
print("=" * 80)
print(f"Finished: {datetime.now()}")
print(f"Success:  {success_count}")
print(f"Errors:   {error_count}")
print(f"Total:    {len(json_files)}\n")

if error_count > 0:
    print("Files with errors:")
    for r in results:
        if r["status"] == "error":
            print(f"  - {r['file']}: {r.get('error', 'Unknown error')}")

print("\n‚úì Results saved to:", OUTPUT_FOLDER)










import json
import glob
import os
import torch
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

INPUT_FOLDER = "/mnt/Dossier_agence/json_first_page/"
OUTPUT_STRUCTURED = "/mnt/Dossier_agence/structured_json/"

# Bank fields
BANK_FIELDS = [
    "Monnaie", "Montant en chiffres", "Montant en lettres", "Agence",
    "Nom/Pr√©nom", "Adresse compl√®te", "Num√©ro de compte", "Date de valeur",
    "bank code", "Si√®ge", "Racine", "Ordinal", "Devise", "Cl√©",
    "Adresse SWIFT", "Autre (Transfert en EURO)",
]

# --------------------------
# Load model once
# --------------------------
print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("‚úì Model loaded.\n")

# --------------------------
# Prompt builder
# --------------------------
def build_prompt(text: str) -> str:
    """Build extraction prompt for FIRST PAGE ONLY"""
    order = " | ".join(BANK_FIELDS)
    
    prompt = f"""You are an expert document information extraction system.
Extract the following fields in EXACT order separated by "|":

{order}

Important rules for bank documents:
- bank code: 3 numbers
- Si√®ge: 5 numbers
- Racine: 6 numbers
- Ordinal: 3 numbers
- Devise: letters
- Cl√©: 2 numbers

Return ONLY values separated by "|".
If a value is missing, return null.
No explanations, no field names.

OCR:
{text[:800]}
"""
    return prompt.strip()

# --------------------------
# Extract from SINGLE PAGE (first page only)
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    """Extract from first page OCR JSON"""
    
    # Merge all text from OCR pages (usually just 1 page in your case)
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
    
    if not full_text.strip():
        raise ValueError("No text found in OCR pages")
    
    # Build prompt
    prompt = build_prompt(full_text)
    
    # Tokenize & generate
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    # Decode output
    result = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    # Parse pipe-separated values
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    structured = {}
    for i, field in enumerate(BANK_FIELDS):
        if i < len(values):
            val = values[i]
            structured[field] = None if val.lower() == "null" or val == "" else val
        else:
            structured[field] = None
    
    return structured

# --------------------------
# Batch process folder
# --------------------------
os.makedirs(OUTPUT_STRUCTURED, exist_ok=True)

json_files = glob.glob(f"{INPUT_FOLDER}/*.json")
print(f"Found {len(json_files)} first-page JSON files.")
print(f"Start: {datetime.now()}\n")

success_count = 0
error_count = 0

for json_path in json_files:
    file_name = os.path.basename(json_path).replace("_page1.json", "").replace(".json", "")
    
    try:
        # Load OCR JSON
        with open(json_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)
        
        # Extract
        structured_dict = extract_structured_data(ocr_pages)
        
        # Save
        output_path = os.path.join(OUTPUT_STRUCTURED, f"{file_name}_structured.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(structured_dict, f, indent=4, ensure_ascii=False)
        
        print(f"‚úì Processed: {file_name}")
        success_count += 1
        
    except Exception as e:
        print(f"‚úó Error processing {file_name}: {e}")
        error_count += 1

print(f"\nFinished: {datetime.now()}")
print(f"Success: {success_count} | Errors: {error_count}")













import os
import glob
import json
from datetime import datetime
import torch
import pandas as pd

# --- LLM / Tokenizer ---
# Assume you have your model & tokenizer already loaded:
# tokenizer = ...
# model = ...

# --- Folders ---
INPUT_FOLDER = "/mnt/Dossier_agence/json_first_page/"
STRUCTURED_FOLDER = "/mnt/Dossier_agence/structured_json/"
os.makedirs(STRUCTURED_FOLDER, exist_ok=True)

OUTPUT_EXCEL = "/mnt/Dossier_agence/final_output.xlsx"

# --- Bank Fields ---
BANK_FIELDS = [
    "Montant en chiffres",
    "Montant en lettres",
    "Nom/Pr√©nom",
    "Adresse compl√®te",
    "Num√©ro de compte",
    "Si√®ge",
    "Racine",
    "Ordinal",
    "Devise",
    "Cl√©",
    "Adresse SWIFT",
    "Autre (Transfert en EURO)",
    "Nom",
    "R√©f√©rences du paiement et observations du donneur d'ordre"
]

# --- Build Prompt ---
def build_prompt(text: str) -> str:
    order = " | ".join(BANK_FIELDS)
    prompt = f"""
You are an expert document information extraction system.

Extract the following fields in EXACT order separated by "|":

{order}

Rules:
- Si√®ge = 5 digits
- Racine = 6 digits
- Ordinal = 3 digits
- Cl√© = 2 digits
- Devise = currency letters (DZD, EUR, etc.)

Return ONLY values separated by "|".
If a value is missing, return null.
No explanations.
No field names.

OCR:
{text}
"""
    return prompt.strip()


# --- Extract Structured Data ---
def extract_structured_data(ocr_pages: list, file_name: str) -> dict:

    # Merge OCR text
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
    
    if not full_text.strip():
        print(f"‚ö†Ô∏è Empty OCR text for {file_name}")
        return None

    # Build prompt
    prompt = build_prompt(full_text)

    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200)

    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=300,
            do_sample=False,
            num_beams=1,
            use_cache=True
        )

    # Decode output
    structured_output = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()

    # Debugging: print first 100 chars if output is empty or weird
    if not structured_output or structured_output.replace("|", "").strip() == "":
        print(f"‚ö†Ô∏è Empty or invalid output for {file_name}")
        print("RAW OUTPUT:", structured_output[:200])

    # Split into fields
    values = [v.strip() for v in structured_output.split("|")]

    if len(values) != len(BANK_FIELDS):
        print(f"‚ö†Ô∏è Field mismatch for {file_name} ({len(values)} fields)")
        print("RAW OUTPUT:", structured_output[:200])
        # Fill missing with null
        while len(values) < len(BANK_FIELDS):
            values.append("null")
        values = values[:len(BANK_FIELDS)]

    structured_dict = dict(zip(BANK_FIELDS, values))
    return structured_dict


# --- Process All JSON Files ---
json_files = glob.glob(os.path.join(INPUT_FOLDER, "*.json"))
print(f"Found {len(json_files)} JSON files")
print("Start:", datetime.now())

all_rows = []

for json_path in json_files:
    file_name = os.path.basename(json_path).replace(".json", "")
    with open(json_path, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    try:
        structured_dict = extract_structured_data(ocr_pages, file_name)
        if structured_dict is None:
            continue

        # Save structured JSON per PDF
        output_path = os.path.join(STRUCTURED_FOLDER, f"{file_name}_structured.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(structured_dict, f, indent=4, ensure_ascii=False)

        # Keep for Excel
        row = structured_dict.copy()
        row["PDF_Name"] = file_name
        all_rows.append(row)

        print(f"Processed: {file_name}")

    except Exception as e:
        print(f"‚ùå Error processing {file_name}: {e}")

print("Finished processing JSONs:", datetime.now())

# --- Merge All Structured JSON into Excel ---
if all_rows:
    df = pd.DataFrame(all_rows)
    df.to_excel(OUTPUT_EXCEL, index=False)
    print(f"‚úÖ Excel created: {OUTPUT_EXCEL}")
else:
    print("‚ö†Ô∏è No data extracted for Excel")











import os
import glob
import json
import pandas as pd
from datetime import datetime

STRUCTURED_FOLDER = "/mnt/Dossier_agence/structured_json/"
OUTPUT_EXCEL = "/mnt/Dossier_agence/final_output.xlsx"

print("Start:", datetime.now())

json_files = glob.glob(f"{STRUCTURED_FOLDER}/*.json")

print(f"Found {len(json_files)} structured JSON files.")

all_rows = []

for json_path in json_files:
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # Add filename column
    file_name = os.path.basename(json_path).replace("_structured.json", "")
    data["PDF_Name"] = file_name

    all_rows.append(data)

# Convert to DataFrame
df = pd.DataFrame(all_rows)

# Save to Excel
df.to_excel(OUTPUT_EXCEL, index=False)

print(f"Excel file created at: {OUTPUT_EXCEL}")
print("Finished:", datetime.now())





def build_prompt(text: str) -> str:

    order = " | ".join(BANK_FIELDS)

    prompt = f"""
You are an expert document information extraction system.

Extract the following fields in EXACT order separated by "|":

{order}

Rules:
- Bank code (Si√®ge) = 5 digits
- Racine = 6 digits
- Ordinal = 3 digits
- Cl√© = 2 digits
- Devise = currency letters (e.g., DZD, EUR)

Return ONLY values separated by "|".
If a value is missing, return null.
No explanations.
No field names.

OCR:
{text}
"""

    return prompt.strip()




def extract_structured_data(ocr_pages: list) -> str:

    # Merge OCR text
    full_text = "\n".join(
        p.get("text", "") for p in ocr_pages if p.get("text")
    )

    if not full_text.strip():
        raise ValueError("Empty OCR text.")

    # Build prompt
    prompt = build_prompt(full_text)

    # Tokenize
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=800
    )

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            num_beams=1,
            use_cache=True
        )

    # Decode ONLY new tokens
    structured = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()

    return structured


import os
import glob
import json
from datetime import datetime

INPUT_FOLDER = "/mnt/Dossier_agence/json_first_page/"
OUTPUT_STRUCTURED = "/mnt/Dossier_agence/structured_json/"

os.makedirs(OUTPUT_STRUCTURED, exist_ok=True)

json_files = glob.glob(f"{INPUT_FOLDER}/*.json")

print(f"Found {len(json_files)} first-page JSON files.")
print("Start:", datetime.now())

for json_path in json_files:

    file_name = os.path.basename(json_path).replace("_page1.json", "")

    with open(json_path, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    try:
        structured_output = extract_structured_data(ocr_pages)

        # Convert "|" output to dictionary
        values = structured_output.split("|")
        structured_dict = dict(zip(BANK_FIELDS, [v.strip() for v in values]))

        output_path = os.path.join(
            OUTPUT_STRUCTURED,
            f"{file_name}_structured.json"
        )

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(structured_dict, f, indent=4, ensure_ascii=False)

        print(f"Processed: {file_name}")

    except Exception as e:
        print(f"Error processing {file_name}: {e}")

print("Finished:", datetime.now())







import os
import glob
import json

JSON_FOLDER = "/mnt/Dossier_agence/json/"
OUTPUT_FOLDER = "/mnt/Dossier_agence/json_first_page/"

os.makedirs(OUTPUT_FOLDER, exist_ok=True)

json_files = glob.glob(f"{JSON_FOLDER}/*.json")

print(f"Found {len(json_files)} JSON files.")

for json_path in json_files:
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # Keep only first page
    if isinstance(data, list) and len(data) > 0:
        first_page = [data[0]]
    else:
        first_page = []

    # Create new file name
    file_name = os.path.basename(json_path)
    new_path = os.path.join(OUTPUT_FOLDER, file_name.replace(".json", "_page1.json"))

    with open(new_path, "w", encoding="utf-8") as f:
        json.dump(first_page, f, indent=4, ensure_ascii=False)

    print(f"Created: {os.path.basename(new_path)}")

print("Done ‚úÖ")





import json
from datetime import datetime

print(datetime.now())

all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}...")

    page_text = extract_text(img)

    all_text.append({
        "page": i+1,
        "text": page_text
    })

    print(f"Page {i+1} done. Preview: {page_text[:80]}...\n")

# Save ONE JSON for this PDF
with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print(f"\nDone! Text saved to {OUTPUT_JSON}")
print(datetime.now())




import os
import glob
import json
from datetime import datetime
from PIL import Image, ImageDraw, ImageFont
import fitz  # PyMuPDF

# ---------- CONFIG ----------
PDF_FOLDER = "/mnt/Dossier_agence/"
IMAGE_SAVE_DIR = "/mnt/Dossier_agence/pages/"
JSON_SAVE_DIR = "/mnt/Dossier_agence/json/"
DPI = 100

os.makedirs(IMAGE_SAVE_DIR, exist_ok=True)
os.makedirs(JSON_SAVE_DIR, exist_ok=True)

# ---------- YOUR OCR FUNCTION ----------
def extract_text(img):
    from pytesseract import image_to_string
    return image_to_string(img, lang="fra")  # change if needed


# ---------- PDF TO IMAGES ----------
def pdf_to_images(pdf_path, save_dir=IMAGE_SAVE_DIR, dpi=DPI):
    doc = fitz.open(pdf_path)
    images = []

    file_name = os.path.splitext(os.path.basename(pdf_path))[0]

    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        # Pad to square
        max_side = max(img.width, img.height)
        img_square = Image.new("RGB", (max_side, max_side), "white")
        img_square.paste(img, (0, 0))

        # Draw filename on image
        draw = ImageDraw.Draw(img_square)
        try:
            font = ImageFont.truetype("arial.ttf", 20)
        except:
            font = ImageFont.load_default()

        text = f"{file_name} - Page {i+1}"
        draw.text((10, 10), text, fill="black", font=font)

        img_path = os.path.join(save_dir, f"{file_name}_page_{i+1}.png")
        img_square.save(img_path)

        images.append(img_square)

    return images


# ---------- MAIN PIPELINE ----------
pdf_files = glob.glob(f"{PDF_FOLDER}/*.pdf")

print(f"Found {len(pdf_files)} PDFs.")
print("Start:", datetime.now())

for pdf_path in pdf_files:
    file_name = os.path.splitext(os.path.basename(pdf_path))[0]
    print(f"\nProcessing PDF: {file_name}")

    images = pdf_to_images(pdf_path)

    pdf_text = []

    for i, img in enumerate(images):
        print(f"  Extracting page {i+1}/{len(images)}...")
        page_text = extract_text(img)

        pdf_text.append({
            "page": i+1,
            "text": page_text
        })

    # Save ONE JSON for THIS PDF
    json_path = os.path.join(JSON_SAVE_DIR, f"{file_name}.json")

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(pdf_text, f, indent=4, ensure_ascii=False)

    print(f"  JSON saved: {json_path}")

print("\nFinished:", datetime.now())













from datetime import datetime
import json

print(datetime.now())

all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}...")
    
    page_text = extract_text(img)
    all_text.append({"page": i+1, "text": page_text})
    
    print(f"Page {i+1} done. Preview: {page_text[:80]}...\n")

with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print(f"\nDone! Text saved to {OUTPUT_JSON}")
print(datetime.now())















4import glob
from datetime import datetime

pdf_folder = "/mnt/Dossier_agence/"
save_dir = "/mnt/Dossier_agence/"

# Get all PDF files in the folder
pdf_files = glob.glob(f"{pdf_folder}/*.pdf")

print(f"Found {len(pdf_files)} PDFs.")

start_time = datetime.now()
print("Start:", start_time)

for pdf_path in pdf_files:
    images = pdf_to_images(pdf_path, save_dir, dpi=100)

end_time = datetime.now()
print("End:", end_time)








import os
from PIL import Image, ImageDraw, ImageFont
import fitz  # PyMuPDF

def pdf_to_images(pdf_path, save_dir="/mnt/pdf_pages", dpi=200):
    os.makedirs(save_dir, exist_ok=True)
    
    doc = fitz.open(pdf_path)
    images = []
    
    # Get the PDF filename without extension
    file_name = os.path.splitext(os.path.basename(pdf_path))[0]
    
    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Pad to square without resizing smaller
        max_side = max(img.width, img.height)
        img_square = Image.new("RGB", (max_side, max_side), "white")
        img_square.paste(img, (0, 0))
        
        # Draw filename on the image
        draw = ImageDraw.Draw(img_square)
        try:
            font = ImageFont.truetype("arial.ttf", 20)  # Change font/size as needed
        except:
            font = ImageFont.load_default()
        text = f"{file_name} - Page {i+1}"
        draw.text((10, 10), text, fill="black", font=font)
        
        # Save image
        img_path = os.path.join(save_dir, f"{file_name}_page_{i+1}.png")
        img_square.save(img_path)
        images.append(img_square)
    
    return images








import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("‚úì Model loaded.\n")

# --------------------------
# Field Schemas
# --------------------------
PAGE_SCHEMAS = {
    "ordre_de_virement": [
        "Monnaie", "Montant en chiffres", "Montant en lettres", "Agence",
        "Nom/Pr√©nom", "Adresse compl√®te", "Num√©ro de compte", "Date de valeur",
        "bank code", "Si√®ge", "Racine", "Ordinal", "Devise", "Cl√©",
        "Adresse SWIFT", "Autre (Transfert en EURO)",
    ],
    "annexe_1": ["Nom", "Date de signature"],
    "annexe_2": ["Mois", "Part transf√©rable", "Nom et pr√©nom"],
    "domiciliation_import": ["Nom"],
    "bulletin_de_paie": ["Nom"],
    "gestion_de_blocages": [],
}

ID_FIELDS = [
    "ÿßŸÑŸÑŸÇÿ®", "ÿßŸÑÿßÿ≥ŸÖ", "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸäŸÑÿßÿØ", "ÿßŸÑÿ¨ŸÜÿ≥",
    "ÿ±ŸÇŸÖ ÿßŸÑŸàÿ∑ŸÜŸä", "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿ•ÿµÿØÿßÿ±", "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ°",
    "ÿ≥ŸÑÿ∑ÿ© ÿßŸÑÿ•ÿµÿØÿßÿ±", "Nom", "Pr√©nom"
]

# --------------------------
# Prompt Builder (YOUR CUSTOM VERSION)
# --------------------------
def build_prompt(text: str, fields: list, is_bank: bool = False) -> str:
    """Build CPU-friendly minimal prompt"""
    order = " | ".join(fields)
    
    # Base prompt
    prompt = f"""You are an expert document information extraction system.
Extract the following fields in EXACT order separated by "|":

{order}
"""
    
    # Add bank-specific rules
    if is_bank:
        prompt += """
Note for bank documents:
- bank code: 3 numbers
- Si√®ge: 5 numbers
- Racine: 6 numbers
- Ordinal: 3 numbers
- Devise: letters
- Cl√©: 2 numbers

"""
    
    # Instructions
    prompt += f"""Return ONLY values separated by "|".
If a value is missing, return null.
No explanations, no field names.

OCR:
{text}
"""
    
    return prompt.strip()

# --------------------------
# Helper: Detect page type
# --------------------------
def detect_page_type(text: str) -> str:
    text_lower = text.lower()
    if "ordre de virement" in text_lower or "virement a l'etranger" in text_lower:
        return "ordre_de_virement"
    elif "annexe 1" in text_lower or "annexe i" in text_lower:
        return "annexe_1"
    elif "annexe 2" in text_lower or "annexe ii" in text_lower:
        return "annexe_2"
    elif "domiciliation" in text_lower and "import" in text_lower:
        return "domiciliation_import"
    elif "bulletin de paie" in text_lower or "bulletin de salaire" in text_lower:
        return "bulletin_de_paie"
    elif "gestion de blocages" in text_lower or "blocage" in text_lower:
        return "gestion_de_blocages"
    return "unknown"

# --------------------------
# Helper: LLM extraction (UPDATED TO USE build_prompt)
# --------------------------
def llm_extract(text: str, fields: list, is_bank: bool = False) -> dict:
    """Call LLM to extract fields from text - ALWAYS returns dict"""
    if not fields:
        return {}
    
    # Use your custom prompt builder
    prompt = build_prompt(text[:800], fields, is_bank)
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs, max_new_tokens=180, do_sample=False,
            use_cache=True, num_beams=1, pad_token_id=tokenizer.eos_token_id
        )
    
    result = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()
    
    # Parse pipe-separated values
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    extracted = {}
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            extracted[field] = None if val.lower() == "null" or val == "" else val
        else:
            extracted[field] = None
    
    return extracted

# --------------------------
# UNIFIED EXTRACTION FUNCTION
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    """
    Single function to extract from both ID cards and multi-page bank documents
    
    Input: ocr_pages = [{"page": 1, "text": "..."}, ...]
    Output: structured dict with all extracted fields
    """
    if not ocr_pages:
        raise ValueError("No OCR pages provided")
    
    # Detect document type from first page
    first_page_text = ocr_pages[0].get("text", "").lower()
    
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    # ID CARD PATH
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    if any(k in first_page_text for k in ["ÿ®ÿ∑ÿßŸÇÿ©", "cni", "carte", "identit√©"]):
        print("Document type: ID Card\n")
        
        full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
        
        extracted = llm_extract(full_text, ID_FIELDS, is_bank=False)
        extracted["document_type"] = "ÿ®ÿ∑ÿßŸÇÿ© ÿßŸÑÿ™ÿπÿ±ŸäŸÅ ÿßŸÑŸàÿ∑ŸÜŸäÿ©"
        
        print("‚úì ID card extraction complete\n")
        return extracted
    
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    # BANK DOCUMENT PATH
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    elif any(k in first_page_text for k in ["ordre de virement", "virement", "banque", "swift"]):
        print(f"Document type: Bank Transfer ({len(ocr_pages)} pages)\n")
        
        all_data = {"document_type": "ORDRE DE VIREMENT A L'ETRANGER"}
        
        # Process each page
        for i, page in enumerate(ocr_pages):
            page_text = page.get("text", "")
            if not page_text:
                continue
            
            print(f"Page {i+1}:", end=" ")
            page_type = detect_page_type(page_text)
            
            if page_type == "unknown":
                print("‚ö† Unknown page type, skipping")
                continue
            
            if page_type == "gestion_de_blocages":
                print(f"{page_type} (skipped)")
                continue
            
            # Get fields for this page type
            fields = PAGE_SCHEMAS.get(page_type, [])
            if not fields:
                print(f"{page_type} (no fields defined)")
                continue
            
            # Extract (pass is_bank=True for bank-specific rules)
            is_bank_page = (page_type == "ordre_de_virement")
            page_data = llm_extract(page_text, fields, is_bank=is_bank_page)
            
            # Verify it's a dict
            if not isinstance(page_data, dict):
                print(f"‚ö† Error: expected dict, got {type(page_data)}")
                continue
            
            # Merge into main dict (don't overwrite existing values)
            for key, value in page_data.items():
                if key not in all_data or all_data[key] is None:
                    all_data[key] = value
            
            print(f"{page_type} ‚Üí extracted {len(page_data)} fields")
        
        print("\n‚úì Bank document extraction complete\n")
        return all_data
    
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    # UNKNOWN DOCUMENT TYPE
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    else:
        raise ValueError("Unknown document type - could not detect ID card or bank document")

# --------------------------
# Run
# --------------------------
if __name__ == "__main__":
    # Load OCR output
    with open("extracted.json", "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)
    
    # Extract
    structured = extract_structured_data(ocr_pages)
    
    # Save JSON
    with open("structured.json", "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)
    
    print(json.dumps(structured, indent=2, ensure_ascii=False))















import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("‚úì Model loaded.\n")

# --------------------------
# Field Schemas
# --------------------------
PAGE_SCHEMAS = {
    "ordre_de_virement": [
        "Monnaie", "Montant en chiffres", "Montant en lettres", "Agence",
        "Nom/Pr√©nom", "Adresse compl√®te", "Num√©ro de compte", "Date de valeur",
        "bank code", "Si√®ge", "Racine", "Ordinal", "Devise", "Cl√©",
        "Adresse SWIFT", "Autre (Transfert en EURO)",
    ],
    "annexe_1": ["Nom", "Date de signature"],
    "annexe_2": ["Mois", "Part transf√©rable", "Nom et pr√©nom"],
    "domiciliation_import": ["Nom"],
    "bulletin_de_paie": ["Nom"],
    "gestion_de_blocages": [],
}

ID_FIELDS = [
    "ÿßŸÑŸÑŸÇÿ®", "ÿßŸÑÿßÿ≥ŸÖ", "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸäŸÑÿßÿØ", "ÿßŸÑÿ¨ŸÜÿ≥",
    "ÿ±ŸÇŸÖ ÿßŸÑŸàÿ∑ŸÜŸä", "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿ•ÿµÿØÿßÿ±", "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ°",
    "ÿ≥ŸÑÿ∑ÿ© ÿßŸÑÿ•ÿµÿØÿßÿ±", "Nom", "Pr√©nom"
]

# --------------------------
# Helper: Detect page type
# --------------------------
def detect_page_type(text: str) -> str:
    text_lower = text.lower()
    if "ordre de virement" in text_lower or "virement a l'etranger" in text_lower:
        return "ordre_de_virement"
    elif "annexe 1" in text_lower or "annexe i" in text_lower:
        return "annexe_1"
    elif "annexe 2" in text_lower or "annexe ii" in text_lower:
        return "annexe_2"
    elif "domiciliation" in text_lower and "import" in text_lower:
        return "domiciliation_import"
    elif "bulletin de paie" in text_lower or "bulletin de salaire" in text_lower:
        return "bulletin_de_paie"
    elif "gestion de blocages" in text_lower or "blocage" in text_lower:
        return "gestion_de_blocages"
    return "unknown"

# --------------------------
# Helper: LLM extraction
# --------------------------
def llm_extract(text: str, fields: list) -> dict:
    """Call LLM to extract fields from text"""
    if not fields:
        return {}
    
    messages = [
        {"role": "system", "content": "Extract document fields. Output ONLY pipe-separated values, no field names."},
        {"role": "user", "content": f"""Extract these fields in order:
{' | '.join(fields)}

Rules:
- Return ONLY values like: value1 | value2 | value3 | ...
- Use "null" for missing fields
- No explanations, no field names

Document:
{text[:800]}"""}
    ]
    
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs, max_new_tokens=180, do_sample=False,
            use_cache=True, num_beams=1, pad_token_id=tokenizer.eos_token_id
        )
    
    result = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    extracted = {}
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            extracted[field] = None if val.lower() == "null" or val == "" else val
        else:
            extracted[field] = None
    
    return extracted

# --------------------------
# UNIFIED EXTRACTION FUNCTION
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    """
    Single function to extract from both ID cards and multi-page bank documents
    
    Input: ocr_pages = [{"page": 1, "text": "..."}, ...]
    Output: structured dict with all extracted fields
    """
    if not ocr_pages:
        raise ValueError("No OCR pages provided")
    
    # Detect document type from first page
    first_page_text = ocr_pages[0].get("text", "").lower()
    
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    # ID CARD PATH (single page or simple multi-page)
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    if any(k in first_page_text for k in ["ÿ®ÿ∑ÿßŸÇÿ©", "cni", "carte", "identit√©"]):
        print("Document type: ID Card\n")
        
        # Merge all pages text
        full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
        
        # Extract
        extracted = llm_extract(full_text, ID_FIELDS)
        extracted["document_type"] = "ÿ®ÿ∑ÿßŸÇÿ© ÿßŸÑÿ™ÿπÿ±ŸäŸÅ ÿßŸÑŸàÿ∑ŸÜŸäÿ©"
        
        print("‚úì ID card extraction complete\n")
        return extracted
    
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    # BANK DOCUMENT PATH (multi-page with page-specific fields)
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    elif any(k in first_page_text for k in ["ordre de virement", "virement", "banque", "swift"]):
        print(f"Document type: Bank Transfer ({len(ocr_pages)} pages)\n")
        
        all_data = {"document_type": "ORDRE DE VIREMENT A L'ETRANGER"}
        
        # Process each page
        for i, page in enumerate(ocr_pages):
            page_text = page.get("text", "")
            if not page_text:
                continue
            
            print(f"Page {i+1}:", end=" ")
            page_type = detect_page_type(page_text)
            
            if page_type == "unknown":
                print("‚ö† Unknown page type, skipping")
                continue
            
            if page_type == "gestion_de_blocages":
                print(f"{page_type} (skipped)")
                continue
            
            # Get fields for this page type
            fields = PAGE_SCHEMAS.get(page_type, [])
            if not fields:
                print(f"{page_type} (no fields defined)")
                continue
            
            # Extract
            page_data = llm_extract(page_text, fields)
            
            # Merge into main dict (don't overwrite existing values)
            for key, value in page_data.items():
                if key not in all_data or all_data[key] is None:
                    all_data[key] = value
            
            print(f"{page_type} ‚Üí extracted {len(page_data)} fields")
        
        print("\n‚úì Bank document extraction complete\n")
        return all_data
    
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    # UNKNOWN DOCUMENT TYPE
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    else:
        raise ValueError("Unknown document type - could not detect ID card or bank document")

# --------------------------
# Run
# --------------------------
if __name__ == "__main__":
    # Load OCR output
    with open("extracted.json", "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)
    
    # Extract (single function handles everything)
    structured = extract_structured_data(ocr_pages)
    
    # Save JSON
    with open("structured.json", "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)
    
    print(json.dumps(structured, indent=2, ensure_ascii=False))















import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("‚úì Model loaded.\n")

# --------------------------
# Page-specific field schemas
# --------------------------
PAGE_SCHEMAS = {
    "ordre_de_virement": [
        "Monnaie",
        "Montant en chiffres",
        "Montant en lettres",
        "Agence",
        "Nom/Pr√©nom",
        "Adresse compl√®te",
        "Num√©ro de compte",
        "Date de valeur",
        "bank code",
        "Si√®ge",
        "Racine",
        "Ordinal",
        "Devise",
        "Cl√©",
        "Adresse SWIFT",
        "Autre (Transfert en EURO)",
    ],
    "annexe_1": [
        "Nom",
        "Date de signature",
    ],
    "annexe_2": [
        "Mois",
        "Part transf√©rable",
        "Nom et pr√©nom",
    ],
    "domiciliation_import": [
        "Nom",
        # Add other fields you need from this page
    ],
    "bulletin_de_paie": [
        "Nom",
        # Add other fields you need from this page
    ],
    "gestion_de_blocages": [
        # No extraction needed
    ],
}

ID_FIELDS = [
    "ÿßŸÑŸÑŸÇÿ®", "ÿßŸÑÿßÿ≥ŸÖ", "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸäŸÑÿßÿØ", "ÿßŸÑÿ¨ŸÜÿ≥",
    "ÿ±ŸÇŸÖ ÿßŸÑŸàÿ∑ŸÜŸä", "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿ•ÿµÿØÿßÿ±", "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ°",
    "ÿ≥ŸÑÿ∑ÿ© ÿßŸÑÿ•ÿµÿØÿßÿ±", "Nom", "Pr√©nom"
]

# --------------------------
# Detect page type by content
# --------------------------
def detect_page_type(text: str) -> str:
    """Detect what type of page this is based on text content"""
    text_lower = text.lower()
    
    # Check for specific page identifiers
    if "ordre de virement" in text_lower or "virement a l'etranger" in text_lower:
        return "ordre_de_virement"
    elif "annexe 1" in text_lower or "annexe i" in text_lower:
        return "annexe_1"
    elif "annexe 2" in text_lower or "annexe ii" in text_lower:
        return "annexe_2"
    elif "domiciliation" in text_lower and "import" in text_lower:
        return "domiciliation_import"
    elif "bulletin de paie" in text_lower or "bulletin de salaire" in text_lower:
        return "bulletin_de_paie"
    elif "gestion de blocages" in text_lower or "blocage" in text_lower:
        return "gestion_de_blocages"
    
    return "unknown"

# --------------------------
# Detect document type (bank vs ID)
# --------------------------
def detect_document_type(ocr_pages: list) -> str:
    """Detect if this is a bank document or ID card"""
    # Check first page
    first_page_text = ocr_pages[0].get("text", "").lower() if ocr_pages else ""
    
    if any(k in first_page_text for k in ["ÿ®ÿ∑ÿßŸÇÿ©", "cni", "carte", "identit√©"]):
        return "id_card"
    elif any(k in first_page_text for k in ["ordre de virement", "virement", "banque", "swift"]):
        return "bank"
    
    return "unknown"

# --------------------------
# Extract from a single page
# --------------------------
def extract_page(page_text: str, page_type: str) -> dict:
    """Extract fields from a single page"""
    if page_type == "gestion_de_blocages" or page_type == "unknown":
        return {}
    
    fields = PAGE_SCHEMAS.get(page_type, [])
    if not fields:
        return {}
    
    # Build prompt
    messages = [
        {
            "role": "system",
            "content": "Extract document fields. Output ONLY pipe-separated values, no field names."
        },
        {
            "role": "user",
            "content": f"""Extract these fields in order:
{' | '.join(fields)}

Rules:
- Return ONLY values like: value1 | value2 | value3 | ...
- Use "null" for missing fields
- No explanations, no field names

Document:
{page_text[:800]}"""
        }
    ]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    print(f"  Page type: {page_type}")
    print(f"  Raw output: {result[:100]}...")
    
    # Parse pipe-separated values
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    page_data = {}
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            page_data[field] = None if val.lower() == "null" or val == "" else val
        else:
            page_data[field] = None
    
    return page_data

# --------------------------
# Extract ID card
# --------------------------
def extract_id_card(ocr_pages: list) -> dict:
    """Extract ID card (single page or multi-page)"""
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
    
    messages = [
        {
            "role": "system",
            "content": "Extract document fields. Output ONLY pipe-separated values, no field names."
        },
        {
            "role": "user",
            "content": f"""Extract these fields in order:
{' | '.join(ID_FIELDS)}

Rules:
- Return ONLY values like: value1 | value2 | value3 | ...
- Use "null" for missing fields
- No explanations, no field names

Document:
{full_text[:800]}"""
        }
    ]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    values = [v.strip() for v in result.split("|")]
    
    structured = {"document_type": "ÿ®ÿ∑ÿßŸÇÿ© ÿßŸÑÿ™ÿπÿ±ŸäŸÅ ÿßŸÑŸàÿ∑ŸÜŸäÿ©"}
    for i, field in enumerate(ID_FIELDS):
        if i < len(values):
            val = values[i]
            structured[field] = None if val.lower() == "null" or val == "" else val
        else:
            structured[field] = None
    
    return structured

# --------------------------
# Extract multi-page bank document
# --------------------------
def extract_bank_document(ocr_pages: list) -> dict:
    """Extract bank document with page-specific fields"""
    all_data = {"document_type": "ORDRE DE VIREMENT A L'ETRANGER"}
    
    print(f"Processing {len(ocr_pages)} pages...\n")
    
    for i, page in enumerate(ocr_pages):
        page_text = page.get("text", "")
        if not page_text:
            continue
        
        print(f"Page {i+1}:")
        page_type = detect_page_type(page_text)
        
        if page_type == "unknown":
            print(f"  ‚ö† Could not detect page type, skipping\n")
            continue
        
        page_data = extract_page(page_text, page_type)
        
        # Merge into main dict (avoid overwriting)
        for key, value in page_data.items():
            if key not in all_data or all_data[key] is None:
                all_data[key] = value
        
        print(f"  ‚úì Extracted {len(page_data)} fields\n")
    
    return all_data

# --------------------------
# Main extraction function
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    """Main entry point: detect document type and extract accordingly"""
    doc_type = detect_document_type(ocr_pages)
    
    if doc_type == "id_card":
        print("Document type: ID Card\n")
        return extract_id_card(ocr_pages)
    elif doc_type == "bank":
        print("Document type: Bank Transfer (Multi-page)\n")
        return extract_bank_document(ocr_pages)
    else:
        raise ValueError("Unknown document type")

# --------------------------
# Run
# --------------------------
if __name__ == "__main__":
    # Load OCR output
    with open("extracted.json", "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)
    
    # Extract
    structured = extract_structured_data(ocr_pages)
    
    # Save JSON
    with open("structured.json", "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)
    
    print("\n‚úì Extraction complete!\n")
    print(json.dumps(structured, indent=2, ensure_ascii=False))


















from concurrent.futures import ProcessPoolExecutor
import os
import json

INPUT_FOLDER = "/mnt/ocr_outputs"

def process_file(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    structured = extract_structured_data(ocr_pages)
    return {
        "file": os.path.basename(file_path),
        "extracted": structured
    }

if __name__ == "__main__":

    json_files = [
        os.path.join(INPUT_FOLDER, f)
        for f in os.listdir(INPUT_FOLDER)
        if f.endswith(".json")
    ]

    with ProcessPoolExecutor() as executor:
        results = list(executor.map(process_file, json_files))

    print(json.dumps(results, indent=2, ensure_ascii=False))



















# Split the string into lines
lines = structured.split("\n")

# Create dictionary
data_dict = {}
for line in lines:
    if ":" in line:
        key, value = line.split(":", 1)  # split at first colon
        data_dict[key.strip()] = value.strip()

print(data_dict)
import pandas as pd

# If you have multiple pages, you can wrap them in a list
df = pd.DataFrame([data_dict])  # wrap in list for single row

# Save to Excel
df.to_excel("payment.xlsx", index=False, encoding="utf-8")








import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("‚úì Model loaded.\n")

# --------------------------
# Schemas
# --------------------------
BANK_FIELDS = [
    "Monnaie",
    "Montant en chiffres",
    "Montant en lettres",
    "Agence",
    "Nom/Pr√©nom",
    "Nature du paiement",
    "Adresse compl√®te",
    "Num√©ro de compte",
    "Date de valeur",
    "Si√®ge",
    "Racine",
    "Ordinal",
    "Devise",
    "Cl√©",
    "Adresse SWIFT",
    "Autre (Transfert en EURO)",
]

ID_FIELDS = [
    "ÿßŸÑŸÑŸÇÿ®", "ÿßŸÑÿßÿ≥ŸÖ", "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸäŸÑÿßÿØ", "ÿßŸÑÿ¨ŸÜÿ≥",
    "ÿ±ŸÇŸÖ ÿßŸÑŸàÿ∑ŸÜŸä", "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿ•ÿµÿØÿßÿ±", "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ°",
    "ÿ≥ŸÑÿ∑ÿ© ÿßŸÑÿ•ÿµÿØÿßÿ±", "Nom", "Pr√©nom"
]

def detect_document_type(text: str) -> str:
    if any(k in text for k in ["ORDRE", "virement", "Banque", "SWIFT"]):
        return "bank"
    elif any(k in text for k in ["ÿ®ÿ∑ÿßŸÇÿ©", "CNI", "Carte"]):
        return "id"
    return "unknown"

# --------------------------
# FIXED PROMPT (uses chat template properly)
# --------------------------
def build_prompt(text: str, doc_type: str) -> str:
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    
    # Qwen chat format
    messages = [
        {
            "role": "system",
            "content": "Extract document fields. Output ONLY pipe-separated values, no field names."
        },
        {
            "role": "user",
            "content": f"""Extract these fields in order:
{' | '.join(fields)}

Rules:
- Return ONLY values like: value1 | value2 | value3 | ...
- Use "null" for missing fields
- No explanations, no field names

Document:
{text[:800]}"""
        }
    ]
    
    return tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

# --------------------------
# Extract
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
    doc_type = detect_document_type(full_text)
    
    if doc_type == "unknown":
        raise ValueError("Unknown document type")
    
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    prompt = build_prompt(full_text, doc_type)
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    print(f"Raw output:\n{result}\n")
    
    # Parse pipe-separated values
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    doc_type_name = "ORDRE DE VIREMENT A L'ETRANGER" if doc_type == "bank" else "ÿ®ÿ∑ÿßŸÇÿ© ÿßŸÑÿ™ÿπÿ±ŸäŸÅ ÿßŸÑŸàÿ∑ŸÜŸäÿ©"
    structured = {"document_type": doc_type_name}
    
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            structured[field] = None if val.lower() == "null" or val == "" else val
        else:
            structured[field] = None
    
    return structured

# --------------------------
# Run
# --------------------------
if __name__ == "__main__":
    # Load your extracted.json
    with open("extracted.json", "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)
    
    structured = extract_structured_data(ocr_pages)
    
    # Save
    with open("structured.json", "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)
    
    print(json.dumps(structured, indent=2, ensure_ascii=False))












import zipfile
import os

folder_path = "dossier"
zip_path = "excel_files.zip"

with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for file in os.listdir(folder_path):
        if file.endswith(".xlsx"):
            zipf.write(os.path.join(folder_path, file),
                       arcname=file)

print("ZIP file created successfully!")







structured = {}

for i in range(len(fields)):
    if i < len(values):
        val = values[i].strip()
        structured[fields[i]] = None if val.lower() == "null" else val
    else:
        structured[fields[i]] = None







structured = {}

for i, field in enumerate(fields):
    if i < len(values):
        structured[field] = values[i]
    else:
        structured[field] = None




import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"  # CPU only
torch.set_num_threads(8)  # adjust to your CPU cores

# Load model
print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("‚úì Model loaded.\n")


# --------------------------
# Document type detection
# --------------------------
def detect_document_type(text: str) -> str:
    """Detect document type from OCR text"""
    if any(k in text for k in ["ORDRE DE VIREMENT", "virement", "Banque"]):
        return "bank"
    elif any(k in text for k in ["ÿ®ÿ∑ÿßŸÇÿ©", "CNI", "Carte"]):
        return "id"
    else:
        return "unknown"


# --------------------------
# Field Schemas
# --------------------------
BANK_FIELDS = [
    "Normale",
    "Montant en chiffres",
    "Montant en lettres",
    "Agence",
    "Nom/Pr√©nom",
    "Nature du paiement",
    "Adresse compl√®te",
    "Num√©ro de compte",
    "Date de valeur",
    "Si√®ge",
    "Racine",
    "Ordinal",
    "Devise",
    "Cl√©",
    "Adresse SWIFT",
    "Autre (Transfert en EURO)",
]

ID_FIELDS = [
    "ÿßŸÑŸÑŸÇÿ®",
    "ÿßŸÑÿßÿ≥ŸÖ",
    "ÿßŸÑÿ™ÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸäŸÑÿßÿØ",
    "ÿßŸÑÿ¨ŸÜÿ≥",
    "ÿ±ŸÇŸÖ ÿßŸÑŸàÿ∑ŸÜŸä",
    "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿ•ÿµÿØÿßÿ±",
    "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ°",
    "ÿ≥ŸÑÿ∑ÿ© ÿßŸÑÿ•ÿµÿØÿßÿ±",
    "Pr√©nom",
]


# --------------------------
# Prompt Builder
# --------------------------
def build_prompt(text: str, doc_type: str) -> str:
    """Build CPU-friendly minimal prompt"""
    if doc_type == "bank":
        order = " | ".join(BANK_FIELDS)
    else:
        order = " | ".join(ID_FIELDS)

    prompt = f"""
You are an expert document information extraction system.
Extract the following fields in EXACT order separated by "|":

{order}

Return ONLY values separated by "|".
If a value is missing, return null.
No explanations.

OCR:
{text}
"""
    return prompt.strip()


# --------------------------
# Extraction Function
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    """
    Input: ocr_pages = [{"page": 1, "text": "..."}, ...]
    Output: structured dict with all fields
    """
    # Merge OCR text
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))

    # Detect document type
    doc_type = detect_document_type(full_text)
    if doc_type == "unknown":
        raise ValueError("Unknown document type.")

    # Build prompt
    prompt = build_prompt(full_text, doc_type)

    # Tokenize & generate
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=800).to(DEVICE)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=120,  # CPU-friendly
            do_sample=False,
            use_cache=True,
            num_beams=1,
        )

    # Decode output
    generated = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()

    # Split pipe-separated values
    values = [v.strip() if v.strip().lower() != "null" else None for v in generated.split("|")]

    # Map to fields
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    structured = dict(zip(fields, values))

    # Add document type
    structured["document_type"] = doc_type

    return structured


# --------------------------
# Example Usage
# --------------------------
if __name__ == "__main__":
    # Example OCR input
    ocr_pages = [
        {"page": 1, "text": "ORDRE DE VIREMENT A L'ETRANGER Date: 05/02/2026 Beneficiaire: John Doe Amount: 1000 EUR Account: FR761234567890 Swift: ABCDFRPP Bank: BNP Paribas Agency: Algiers Nature: Salary"},
    ]

    structured = extract_structured_data(ocr_pages)
    print(json.dumps(structured, indent=2, ensure_ascii=False))

















import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

# ‚îÄ‚îÄ Load model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
print("Loading model...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if device == "cuda" else torch.float32

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=dtype,
    device_map="auto" if device == "cuda" else None,
    trust_remote_code=True,
)
if device == "cpu":
    model = model.to("cpu")
    torch.set_num_threads(8)

model.eval()
print(f"‚úì Loaded on {device} in {time()-t0:.1f}s\n")

# ‚îÄ‚îÄ Schema per document type ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def get_schema(document_type: str) -> dict:
    if document_type == "bank":
        return {
            "document_type": "ORDRE DE VIREMENT A L'ETRANGER",
            "Monnaie": None,
            "Montant en chiffres": None,
            "Montant en lettres": None,
            "Agence": None,
            "Nom/Pr√©nom": None,
            "Nature du paiement": None,
            "Adresse compl√®te": None,
            "Num√©ro de compte": None,
            "Date de valeur": None,
            "Si√®ge": None,      # 5 digits
            "Racine": None,     # 6 digits
            "Ordinal": None,    # 3 digits
            "Devise": None,     # letters
            "Cl√©": None,        # 2 digits
            "Adresse SWIFT": None,
            "Autre (Transfert en EURO)": None,
        }
    elif document_type == "id_card":
        return {
            "document_type": "ÿ®ÿ∑ÿßŸÇÿ© ÿßŸÑÿ™ÿπÿ±ŸäŸÅ ÿßŸÑŸàÿ∑ŸÜŸäÿ©",
            "ÿßŸÑŸÑŸÇÿ®": None,
            "ÿßŸÑÿßÿ≥ŸÖ": None,
            "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸäŸÑÿßÿØ": None,
            "ÿßŸÑÿ¨ŸÜÿ≥": None,
            "ÿ±ŸÇŸÖ ÿßŸÑŸàÿ∑ŸÜŸä": None,
            "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿ•ÿµÿØÿßÿ±": None,
            "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ°": None,
            "ÿ≥ŸÑÿ∑ÿ© ÿßŸÑÿ•ÿµÿØÿßÿ±": None,
            "Nom": None,
            "Pr√©nom": None,
        }
    else:
        raise ValueError(f"Unknown document type: {document_type}")

# ‚îÄ‚îÄ Detect document type ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["ÿ®ÿ∑ÿßŸÇÿ©", "ÿßŸÑÿ™ÿπÿ±ŸäŸÅ", "ÿßŸÑŸàÿ∑ŸÜŸäÿ©", "carte nationale", "cni", "identit√©"]):
        return "id_card"
    return "unknown"

# ‚îÄ‚îÄ Parse JSON from output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def parse_json_from_output(text: str) -> dict:
    if "<|im_start|>assistant" in text:
        text = text.split("<|im_start|>assistant")[-1]
    
    # Try ```json block
    json_match = re.search(r'```json\s*(\{.*?)```', text, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(1))
        except json.JSONDecodeError:
            pass
    
    # Try incomplete ```json block
    json_match = re.search(r'```json\s*(\{.*)', text, re.DOTALL)
    if json_match:
        json_str = json_match.group(1).strip()
        if json_str.count('{') > json_str.count('}'):
            json_str += '}'
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass
    
    # Fallback: find { ... }
    start = text.find('{')
    end = text.rfind('}') + 1
    if start != -1 and end > start:
        json_str = text[start:end]
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            if json_str.count('{') > json_str.count('}'):
                json_str += '}'
            return json.loads(json_str)
    
    raise ValueError(f"No valid JSON found in model output:\n{text[:500]}")

# ‚îÄ‚îÄ Extract structured data ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))[:1000]  # truncate
    document_type = detect_document_type(full_text)
    schema = get_schema(document_type)
    
    # Extract only field names (keys), not the full schema with None values
    field_names = [k for k in schema.keys() if k != "document_type"]
    
    # OPTIMIZED PROMPT - Much shorter
    prompt = f"""<|im_start|>system
Extract fields to JSON. Return only values, no field names repeated.
<|im_end|>
<|im_start|>user
Fields: {', '.join(field_names)}

Text:
{full_text}

Return JSON with these fields. Use null if missing.
<|im_end|>
<|im_start|>assistant
```json
{{
  "document_type": "{schema['document_type']}"
"""

    print(f"Document type: {document_type}")
    print(f"Prompt tokens: ~{len(tokenizer.encode(prompt))}\n")
    
    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1500).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=250,  # reduced from 350-500
            do_sample=False,
            num_beams=1,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    gen_time = time() - t0
    
    print(f"‚è± Generation: {gen_time:.2f}s")
    print(f"Raw output:\n{result[:200]}...\n")
    
    # Complete the JSON if needed
    complete_json = '{\n  "document_type": "' + schema['document_type'] + '"\n' + result
    
    return parse_json_from_output(complete_json)

# ‚îÄ‚îÄ Run ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages\n")

    t_start = time()
    structured = extract_structured_data(ocr_pages)
    
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"\n‚úì Done in {time()-t_start:.1f}s!\n")
    print(json.dumps(structured, indent=2, ensure_ascii=False))












import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

# ‚îÄ‚îÄ Load model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
print("Loading model...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if device == "cuda" else torch.float32

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=dtype,
    device_map="auto" if device == "cuda" else None,
    trust_remote_code=True,
)
if device == "cpu":
    model = model.to("cpu")
    torch.set_num_threads(8)

model.eval()
print(f"‚úì Loaded on {device} in {time()-t0:.1f}s\n")

# ‚îÄ‚îÄ Field order (important!) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
BANK_FIELDS = [
    "Monnaie",
    "Montant en chiffres", 
    "Montant en lettres",
    "Agence",
    "Nom/Pr√©nom",
    "Nature du paiement",
    "Adresse compl√®te",
    "Num√©ro de compte",
    "Date de valeur",
    "Si√®ge",
    "Racine",
    "Ordinal",
    "Devise",
    "Cl√©",
    "Adresse SWIFT",
    "Autre (Transfert en EURO)"
]

ID_FIELDS = [
    "ÿßŸÑŸÑŸÇÿ®",
    "ÿßŸÑÿßÿ≥ŸÖ",
    "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸäŸÑÿßÿØ",
    "ÿßŸÑÿ¨ŸÜÿ≥",
    "ÿ±ŸÇŸÖ ÿßŸÑŸàÿ∑ŸÜŸä",
    "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿ•ÿµÿØÿßÿ±",
    "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ°",
    "ÿ≥ŸÑÿ∑ÿ© ÿßŸÑÿ•ÿµÿØÿßÿ±",
    "Nom",
    "Pr√©nom"
]

# ‚îÄ‚îÄ Detect document type ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["ÿ®ÿ∑ÿßŸÇÿ©", "ÿßŸÑÿ™ÿπÿ±ŸäŸÅ", "ÿßŸÑŸàÿ∑ŸÜŸäÿ©", "carte nationale", "cni", "identit√©"]):
        return "id_card"
    return "unknown"

# ‚îÄ‚îÄ Extract with values-only prompt ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))[:1200]
    doc_type = detect_document_type(full_text)
    
    if doc_type == "unknown":
        raise ValueError("Could not detect document type")
    
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    field_list = "\n".join([f"{i+1}. {field}" for i, field in enumerate(fields)])
    
    if doc_type == "bank":
        example = '"020", "318060.13", "trois cent dix-huit mille soixante virgule treize Dinars", "MOSTAGANEM", "ALI SAYED HASSNAT", "virement de tr√©sorerie", "HARATI, PATS, PASCHIM BARDHAMAN, DURGAPUR, WEST BENGAL PIN 713102", "50100692", null, "97000", "675432", "052", "070", "92", "ABCDINDR", null'
    else:
        example = '"ÿ¥ŸäÿßŸÑ", "ŸÖÿ≠ŸÖÿØ", "1956", "ÿ∞ŸÉÿ±", "100581262011435400", "20180121", "20280121", null, "CHIAL", "M HAMED"'
    
    prompt = f"""<|im_start|>system
Extract document fields. Return ONLY values as a comma-separated list in quotes. No field names.
<|im_end|>
<|im_start|>user
Extract these fields IN ORDER:
{field_list}

Rules:
- Return ONLY values in quotes: "value1", "value2", "value3", ...
- If field is missing or empty, write null (no quotes)
- Do NOT include field names
- Do NOT explain anything

Example output format:
{example}

Document text:
{full_text}
<|im_end|>
<|im_start|>assistant
"""

    print(f"Document type: {doc_type}")
    
    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1800).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=False,
            num_beams=1,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    gen_time = time() - t0
    
    print(f"‚è± Generation: {gen_time:.2f}s")
    print(f"Raw output:\n{result}\n")
    
    # Parse comma-separated values
    result = result.strip()
    if result.startswith('"') or result.startswith("'"):
        # Use regex to split by commas outside quotes
        values = re.findall(r'"([^"]*)"|\bnull\b', result)
        values = [None if v == '' and 'null' in result[i:i+10] else v for i, v in enumerate(values)]
    else:
        # Fallback: simple split
        values = [v.strip().strip('"').strip("'") for v in result.split(",")]
        values = [None if v.lower() == "null" or v == "" else v for v in values]
    
    # Build result dict
    doc_type_name = "ORDRE DE VIREMENT A L'ETRANGER" if doc_type == "bank" else "ÿ®ÿ∑ÿßŸÇÿ© ÿßŸÑÿ™ÿπÿ±ŸäŸÅ ÿßŸÑŸàÿ∑ŸÜŸäÿ©"
    structured = {"document_type": doc_type_name}
    
    for i, field in enumerate(fields):
        structured[field] = values[i] if i < len(values) else None
    
    return structured

# ‚îÄ‚îÄ Run ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages\n")

    t_start = time()
    structured = extract_structured_data(ocr_pages)
    
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"\n‚úì Done in {time()-t_start:.1f}s!\n")
    print(json.dumps(structured, indent=2, ensure_ascii=False))








import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

# ‚îÄ‚îÄ Load model (optimized for CPU if needed) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
print("Loading model...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

# Check if GPU is available
device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if device == "cuda" else torch.float32

if device == "cpu":
    torch.set_num_threads(8)  # adjust to your CPU cores

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=dtype,
    device_map="auto" if device == "cuda" else None,
    trust_remote_code=True,
)
if device == "cpu":
    model = model.to("cpu")
    
model.config.use_cache = True
model.eval()
print(f"‚úì Loaded on {device} in {time()-t0:.1f}s\n")

# ‚îÄ‚îÄ Field definitions (order matters!) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
BANK_FIELDS = [
    "Monnaie",
    "Montant en chiffres",
    "Montant en lettres",
    "Agence",
    "Nom/Pr√©nom",
    "Nature du paiement",
    "Adresse compl√®te",
    "Num√©ro de compte",
    "Date de valeur",
    "Si√®ge",      # 5 digits
    "Racine",     # 6 digits
    "Ordinal",    # 3 digits
    "Devise",     # letters
    "Cl√©",        # 2 digits
    "Adresse SWIFT",
    "Autre (Transfert en EURO)"
]

ID_FIELDS = [
    "ÿßŸÑŸÑŸÇÿ®",
    "ÿßŸÑÿßÿ≥ŸÖ",
    "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸäŸÑÿßÿØ",
    "ÿßŸÑÿ¨ŸÜÿ≥",
    "ÿ±ŸÇŸÖ ÿßŸÑŸàÿ∑ŸÜŸä",
    "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿ•ÿµÿØÿßÿ±",
    "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ°",
    "ÿ≥ŸÑÿ∑ÿ© ÿßŸÑÿ•ÿµÿØÿßÿ±",
    "Nom",
    "Pr√©nom"
]

# ‚îÄ‚îÄ Detect document type ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["ÿ®ÿ∑ÿßŸÇÿ©", "ÿßŸÑÿ™ÿπÿ±ŸäŸÅ", "ÿßŸÑŸàÿ∑ŸÜŸäÿ©", "carte nationale", "cni", "identit√©"]):
        return "id_card"
    return "unknown"

# ‚îÄ‚îÄ Build minimal prompt ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def build_prompt(text: str, fields: list, doc_type: str) -> str:
    field_list = " | ".join(fields)
    
    if doc_type == "bank":
        notes = "Note: Si√®ge=5 digits, Racine=6 digits, Ordinal=3 digits, Devise=letters, Cl√©=2 digits."
    else:
        notes = ""
    
    prompt = f"""<|im_start|>system
Extract document fields.
<|im_end|>
<|im_start|>user
Extract these fields in EXACT order, separated by "|":

{field_list}

Return ONLY values separated by "|". If missing, write "null". No explanation.
{notes}

Text:
{text[:1000]}
<|im_end|>
<|im_start|>assistant
"""
    return prompt

# ‚îÄ‚îÄ Parse pipe-separated output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def parse_output(text: str, fields: list, doc_type: str) -> dict:
    # Clean output
    text = text.strip()
    if "<|im_start|>assistant" in text:
        text = text.split("<|im_start|>assistant")[-1].strip()
    
    # Split by pipe
    values = [v.strip() for v in text.split("|")]
    
    # Build dict
    result = {"document_type": "ORDRE DE VIREMENT A L'ETRANGER" if doc_type == "bank" else "ÿ®ÿ∑ÿßŸÇÿ© ÿßŸÑÿ™ÿπÿ±ŸäŸÅ ÿßŸÑŸàÿ∑ŸÜŸäÿ©"}
    
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            result[field] = None if val.lower() == "null" or val == "" else val
        else:
            result[field] = None
    
    return result

# ‚îÄ‚îÄ Extract ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))
    doc_type = detect_document_type(full_text)
    
    if doc_type == "unknown":
        raise ValueError("Could not detect document type")
    
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    prompt = build_prompt(full_text, fields, doc_type)
    
    print(f"Document: {doc_type}")
    print(f"Prompt length: {len(tokenizer.encode(prompt))} tokens\n")
    
    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1536).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=120,  # ‚Üì from 350 ‚Äî enough for pipe-separated
            do_sample=False,
            num_beams=1,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    gen_time = time() - t0
    
    print(f"‚è± Generation: {gen_time:.2f}s")
    print(f"Raw output: {result}\n")
    
    return parse_output(result, fields, doc_type)

# ‚îÄ‚îÄ Run ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages\n")

    t_start = time()
    structured = extract_structured_data(ocr_pages)
    
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"‚úì Done in {time()-t_start:.1f}s!\n")
    print(json.dumps(structured, indent=2, ensure_ascii=False))













import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# ‚îÄ‚îÄ Use 0.5B model (4x faster than 1.5B) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-0.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

print("Loading 0.5B model (fast)...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
)
model.eval()
print(f"‚úì Loaded in {time()-t0:.1f}s\n")

def detect_document_type(text: str) -> str:
    return "id_card" if any(k in text for k in ["ÿ®ÿ∑ÿßŸÇÿ©", "CNI", "Carte"]) else "bank"

SCHEMAS = {
    "bank": {"Montant": None, "Nom": None, "Compte": None},
    "id_card": {"ÿßŸÑŸÑŸÇÿ®": None, "ÿßŸÑÿßÿ≥ŸÖ": None, "ÿ±ŸÇŸÖ ÿßŸÑŸàÿ∑ŸÜŸä": None, "Nom": None, "Pr√©nom": None}
}

def parse_json(text: str) -> dict:
    match = re.search(r'\{[^{}]*\}', text)
    return json.loads(match.group(0)) if match else {}

def extract_structured_data(ocr_pages: list) -> dict:
    text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))[:600]
    doc_type = detect_document_type(text)
    schema = SCHEMAS[doc_type]

    prompt = f"""Extract to JSON:
{json.dumps(schema, ensure_ascii=False)}

{text}

{{"""

    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt", max_length=800, truncation=True).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=150, do_sample=False, use_cache=True)
    
    result = "{" + tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    print(f"‚è± {time()-t0:.2f}s | {result[:100]}...")
    
    return parse_json(result)

if __name__ == "__main__":
    with open(INPUT_JSON, "r") as f:
        ocr_pages = json.load(f)

    structured = extract_structured_data(ocr_pages)
    
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"\n‚úì Done!\n{json.dumps(structured, indent=2, ensure_ascii=False)}")






import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

# ‚îÄ‚îÄ Load model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
print("Loading tokenizer and model...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map="auto",
    trust_remote_code=True,
)
model.eval()
print(f"Model loaded in {time()-t0:.1f}s\n")

# ‚îÄ‚îÄ Detect document type ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["ÿ®ÿ∑ÿßŸÇÿ©", "ÿßŸÑÿ™ÿπÿ±ŸäŸÅ", "ÿßŸÑŸàÿ∑ŸÜŸäÿ©", "carte nationale", "cni", "identit√©"]):
        return "id_card"
    return "unknown"

# ‚îÄ‚îÄ Schema per document type ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def get_schema(document_type: str) -> dict:
    if document_type == "bank":
        return {
            "document_type": "ORDRE DE VIREMENT A L'ETRANGER",
            "Monnaie": None,
            "Montant en chiffres": None,
            "Montant en lettres": None,
            "Agence": None,
            "Nom/Pr√©nom": None,
            "Nature du paiement": None,
            "Adresse compl√®te": None,
            "Num√©ro de compte": None,
            "Date de valeur": None,
            "Si√®ge": None,
            "Racine": None,
            "Ordinal": None,
            "Devise": None,
            "Cl√©": None,
            "Adresse SWIFT": None,
            "Autre (Transfert en EURO)": None,
        }
    elif document_type == "id_card":
        return {
            "document_type": "ÿ®ÿ∑ÿßŸÇÿ© ÿßŸÑÿ™ÿπÿ±ŸäŸÅ ÿßŸÑŸàÿ∑ŸÜŸäÿ©",
            "ÿßŸÑŸÑŸÇÿ®": None,
            "ÿßŸÑÿßÿ≥ŸÖ": None,
            "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸäŸÑÿßÿØ": None,
            "ÿßŸÑÿ¨ŸÜÿ≥": None,
            "ÿ±ŸÇŸÖ ÿßŸÑŸàÿ∑ŸÜŸä": None,
            "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿ•ÿµÿØÿßÿ±": None,
            "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ°": None,
            "ÿ≥ŸÑÿ∑ÿ© ÿßŸÑÿ•ÿµÿØÿßÿ±": None,
            "Nom": None,
            "Pr√©nom": None,
        }
    else:
        raise ValueError(f"Unknown document type: {document_type}")

# ‚îÄ‚îÄ Clean and parse JSON from model output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def parse_json_from_output(text: str) -> dict:
    if "<|im_start|>assistant" in text:
        text = text.split("<|im_start|>assistant")[-1]

    json_match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
    if json_match:
        return json.loads(json_match.group(1))

    start = text.find("{")
    end   = text.rfind("}") + 1
    if start != -1 and end > start:
        return json.loads(text[start:end])

    raise ValueError(f"No valid JSON found in model output:\n{text[:500]}")

# ‚îÄ‚îÄ Main extraction function ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))
    document_type = detect_document_type(full_text)
    schema = get_schema(document_type)

    # Shorter, more direct prompt
    prompt = f"""<|im_start|>system
Extract document fields into JSON. Return only valid JSON, no explanation.
<|im_end|>
<|im_start|>user
Schema:
{json.dumps(schema, ensure_ascii=False, indent=2)}

Text:
{full_text[:2000]}
<|im_end|>
<|im_start|>assistant
"""

    print(f"Document type: {document_type}")
    print(f"Prompt length: {len(prompt)} chars\n")
    
    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    prep_time = time() - t0
    print(f"‚è± Tokenization: {prep_time:.2f}s")

    t0 = time()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=400,        # ‚Üì from 600
            do_sample=False,
            num_beams=1,               # explicit greedy decode
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    gen_time = time() - t0
    print(f"‚è± Generation: {gen_time:.2f}s")

    generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    print(f"‚è± Total: {prep_time + gen_time:.2f}s\n")
    print(f"‚îÄ‚îÄ Raw output ‚îÄ‚îÄ\n{result[:300]}...\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n")

    return parse_json_from_output(result)

# ‚îÄ‚îÄ Run ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    t_start = time()
    
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages from {INPUT_JSON}\n")

    structured = extract_structured_data(ocr_pages)

    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    t_total = time() - t_start
    print(f"‚úì Done in {t_total:.1f}s! Structured data saved to {OUTPUT_JSON}\n")
    print(json.dumps(structured, indent=4, ensure_ascii=False))










import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"   # output from the OCR step
OUTPUT_JSON = "structured.json"

# ‚îÄ‚îÄ Load model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
print("Loading tokenizer and model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map="auto",
    trust_remote_code=True,
)
model.eval()
print("Model loaded.")

# ‚îÄ‚îÄ Detect document type ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["ÿ®ÿ∑ÿßŸÇÿ©", "ÿßŸÑÿ™ÿπÿ±ŸäŸÅ", "ÿßŸÑŸàÿ∑ŸÜŸäÿ©", "carte nationale", "cni", "identit√©"]):
        return "id_card"
    return "unknown"

# ‚îÄ‚îÄ Schema per document type ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def get_schema(document_type: str) -> dict:
    if document_type == "bank":
        return {
            "document_type": "ORDRE DE VIREMENT A L'ETRANGER",
            "Monnaie": None,
            "Montant en chiffres": None,
            "Montant en lettres": None,
            "Agence": None,
            "Nom/Pr√©nom": None,
            "Nature du paiement": None,
            "Adresse compl√®te": None,
            "Num√©ro de compte": None,
            "Date de valeur": None,
            "Si√®ge": None,
            "Racine": None,
            "Ordinal": None,
            "Devise": None,
            "Cl√©": None,
            "Adresse SWIFT": None,
            "Autre (Transfert en EURO)": None,
        }
    elif document_type == "id_card":
        return {
            "document_type": "ÿ®ÿ∑ÿßŸÇÿ© ÿßŸÑÿ™ÿπÿ±ŸäŸÅ ÿßŸÑŸàÿ∑ŸÜŸäÿ©",
            "ÿßŸÑŸÑŸÇÿ®": None,
            "ÿßŸÑÿßÿ≥ŸÖ": None,
            "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑŸÖŸäŸÑÿßÿØ": None,
            "ÿßŸÑÿ¨ŸÜÿ≥": None,
            "ÿ±ŸÇŸÖ ÿßŸÑŸàÿ∑ŸÜŸä": None,
            "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿ•ÿµÿØÿßÿ±": None,
            "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ°": None,
            "ÿ≥ŸÑÿ∑ÿ© ÿßŸÑÿ•ÿµÿØÿßÿ±": None,
            "Nom": None,
            "Pr√©nom": None,
        }
    else:
        raise ValueError(f"Unknown document type: {document_type}")

# ‚îÄ‚îÄ Clean and parse JSON from model output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def parse_json_from_output(text: str) -> dict:
    # Strip the prompt echo if present (keep only after last assistant turn)
    if "<|im_start|>assistant" in text:
        text = text.split("<|im_start|>assistant")[-1]

    # Try to find a JSON block (handles ```json ... ``` or raw {...})
    json_match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
    if json_match:
        return json.loads(json_match.group(1))

    # Fallback: find first { ... } block
    start = text.find("{")
    end   = text.rfind("}") + 1
    if start != -1 and end > start:
        return json.loads(text[start:end])

    raise ValueError(f"No valid JSON found in model output:\n{text[:500]}")

# ‚îÄ‚îÄ Main extraction function ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))
    document_type = detect_document_type(full_text)
    schema = get_schema(document_type)

    prompt = f"""<|im_start|>system
You are an expert document information extraction system.
The document language may be French, Arabic, or both.
The OCR text may contain noise or formatting errors ‚Äî correct them carefully.
Return ONLY valid JSON matching the schema exactly.
Do not explain anything. If a field is missing, use null.
<|im_end|>
<|im_start|>user
Schema:
{json.dumps(schema, ensure_ascii=False, indent=2)}

OCR TEXT:
{full_text}
<|im_end|>
<|im_start|>assistant
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=600,  # increased slightly for 1.5B model
            do_sample=False,
            temperature=None,
            top_p=None,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Decode only newly generated tokens
    generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    print(f"\n‚îÄ‚îÄ Raw model output ‚îÄ‚îÄ\n{result}\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n")

    return parse_json_from_output(result)

# ‚îÄ‚îÄ Run ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages from {INPUT_JSON}")

    structured = extract_structured_data(ocr_pages)

    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"\n‚úì Done! Structured data saved to {OUTPUT_JSON}")
    print(json.dumps(structured, indent=4, ensure_ascii=False))







import torch
import json
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM

# =========================================
# 1Ô∏è‚É£ LOAD MODEL
# =========================================

model_name = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-32B-Instruct/main"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# =========================================
# 2Ô∏è‚É£ MERGE OCR PAGES
# =========================================

def merge_ocr_pages(ocr_output):
    full_text = ""
    for page in ocr_output:
        full_text += page["text"] + "\n"
    return full_text


# =========================================
# 3Ô∏è‚É£ DOCUMENT TYPE DETECTION
# =========================================

def detect_document_type(text):

    text_lower = text.lower()

    if "virement" in text_lower and "etranger" in text_lower:
        return "bank"

    elif "ÿßŸÑÿ™ÿπÿ±ŸäŸÅ" in text and "ÿßŸÑŸàÿ∑ŸÜŸäÿ©" in text:
        return "id"

    else:
        return "unknown"


# =========================================
# 4Ô∏è‚É£ STRUCTURED EXTRACTION
# =========================================

def extract_structured_data(ocr_output):

    full_text = merge_ocr_pages(ocr_output)

    document_type = detect_document_type(full_text)

    if document_type == "bank":
        schema = """
{
  "document_type": "foreign_transfer_order",
  "date": "",
  "amount": "",
  "currency": "",
  "beneficiary_name": "",
  "account_number": "",
  "swift_code": "",
  "bank_name": "",
  "agency": "",
  "nature_of_payment": ""
}
"""
    elif document_type == "id":
        schema = """
{
  "document_type": "algerian_id_card",
  "full_name": "",
  "date_of_birth": "",
  "place_of_birth": "",
  "id_number": "",
  "expiration_date": "",
  "nationality": "",
  "blood_group": ""
}
"""
    else:
        raise ValueError("‚ùå Unknown document type")

    prompt = f"""
You are an expert document information extraction system.

The OCR text may contain noise or formatting errors.
Extract the correct structured data carefully.

Return ONLY valid JSON.
Do not explain anything.

If a field is missing, return null.

Schema:
{schema}

OCR TEXT:
{full_text}
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=500,
        temperature=0.1
    )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

    json_start = result.find("{")
    json_end = result.rfind("}") + 1
    clean_json = result[json_start:json_end]

    return json.loads(clean_json)


# =========================================
# 5Ô∏è‚É£ EXECUTION
# =========================================

# Example OCR output
ocr_output = [
    {"page": 1, "text": "ORDRE DE VIREMENT A L'ETRANGER ..."},
    {"page": 2, "text": "Beneficiaire ..."}
]

structured_data = extract_structured_data(ocr_output)

df = pd.DataFrame([structured_data])

print(df)

df.to_excel("structured_output.xlsx", index=False)

print("‚úÖ Structured extraction completed")




















OCR text ‚Üí Structured JSON (via Qwen2.5) ‚Üí Pandas Table ‚Üí Excel export




import torch
import json
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from pdf2image import convert_from_path
import pytesseract

# =========================================
# 1Ô∏è‚É£ LOAD MODEL
# =========================================

model_name = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-32B-Instruct/main"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# =========================================
# 2Ô∏è‚É£ OCR FUNCTION
# =========================================

def extract_text_from_scanned_pdf(pdf_path, lang="eng"):
    pages = convert_from_path(pdf_path)
    full_text = ""

    for page in pages:
        text = pytesseract.image_to_string(page, lang=lang)
        full_text += text + "\n"

    return full_text


# =========================================
# 3Ô∏è‚É£ AUTO DOCUMENT TYPE DETECTION
# =========================================

def detect_document_type(ocr_text):

    text_lower = ocr_text.lower()

    if "ordre de virement" in text_lower:
        return "bank"

    elif "ÿ®ÿ∑ÿßŸÇÿ© ÿßŸÑÿ™ÿπÿ±ŸäŸÅ ÿßŸÑŸàÿ∑ŸÜŸäÿ©" in ocr_text:
        return "id"

    else:
        return "unknown"


# =========================================
# 4Ô∏è‚É£ STRUCTURED EXTRACTION
# =========================================

def extract_structured_data(ocr_text):

    document_type = detect_document_type(ocr_text)

    if document_type == "bank":
        schema = """
{
  "document_type": "foreign_transfer_order",
  "date": "",
  "amount": "",
  "currency": "",
  "beneficiary_name": "",
  "account_number": "",
  "swift_code": "",
  "bank_name": "",
  "agency": "",
  "nature_of_payment": ""
}
"""

    elif document_type == "id":
        schema = """
{
  "document_type": "algerian_id_card",
  "full_name": "",
  "date_of_birth": "",
  "place_of_birth": "",
  "id_number": "",
  "expiration_date": "",
  "nationality": "",
  "blood_group": ""
}
"""
    else:
        raise ValueError("‚ùå Unknown document type")

    prompt = f"""
You are an expert information extraction system.

The OCR text may contain noise.
Extract the correct structured data.

Return ONLY valid JSON.
Do not explain anything.

If a field is missing, return null.

Schema:
{schema}

OCR TEXT:
{ocr_text}
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=500,
        temperature=0.1
    )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

    json_start = result.find("{")
    json_end = result.rfind("}") + 1
    clean_json = result[json_start:json_end]

    return json.loads(clean_json)


# =========================================
# 5Ô∏è‚É£ MAIN
# =========================================

pdf_path = "your_scanned_document.pdf"

# OCR
ocr_text = extract_text_from_scanned_pdf(pdf_path, lang="fra")  # change if needed

# Auto extraction
structured_data = extract_structured_data(ocr_text)

# Convert to table
df = pd.DataFrame([structured_data])

print(df)

df.to_excel("structured_output.xlsx", index=False)

print("‚úÖ Auto detection + extraction completed")






from pypdf import PdfReader, PdfWriter

input_pdf = "input.pdf"        # your original PDF
output_pdf = "first_page.pdf"  # new PDF with only page 1

reader = PdfReader(input_pdf)
writer = PdfWriter()

# Add first page (index 0)
writer.add_page(reader.pages[0])

# Save as new PDF
with open(output_pdf, "wb") as f:
    writer.write(f)

print("First page saved as PDF successfully!")






print(type(image))
print(image.size)
image.save("/mnt/test_page.png")

def extract_text(image):

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": "Extract all visible text from this French document page. Return plain text only."}
            ],
        }
    ]

    # Build proper multimodal prompt
    prompt = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = processor(
        text=[prompt],        # MUST be list
        images=[image],       # MUST be list
        padding=True,
        return_tensors="pt"
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048
        )

    result = processor.batch_decode(
        output, skip_special_tokens=True
    )[0]

    return result









import os
import json
import torch
import fitz  # PyMuPDF
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq

# Paths
MODEL_PATH = "/domina/cdv/modelhub/Modelhub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"

# Load model and processor
processor = AutoProcessor.from_pretrained(
    MODEL_PATH, 
    trust_remote_code=True
)

model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,
    device_map="cpu",
    trust_remote_code=True
)

model.eval()

def pdf_to_images(pdf_path, save_dir="/mnt/pdf_pages", dpi=300):
    """Convert PDF pages to images"""
    os.makedirs(save_dir, exist_ok=True)
    
    doc = fitz.open(pdf_path)
    images = []
    
    for i, page in enumerate(doc):
        # Render page to pixmap
        pix = page.get_pixmap(dpi=dpi)
        
        # Convert to PIL Image
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Save image
        img_path = f"{save_dir}/page_{i+1}.png"
        img.save(img_path)
        print(f"Saved page {i+1}: {img_path} ({img.width}x{img.height})")
        
        images.append(img)
    
    doc.close()
    return images

def extract_text(image):
    """Extract text from image using Qwen2.5-VL"""
    
    # Qwen2.5-VL specific prompt format
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text", 
                    "text": "Extract ALL visible text from this document page. Preserve the structure and formatting. Return plain text only."
                }
            ],
        }
    ]
    
    # Apply chat template
    text = processor.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    # Process inputs
    inputs = processor(
        text=[text],
        images=[image],
        padding=True,
        return_tensors="pt"
    )
    
    # Generate
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=4096,
            temperature=0.1,
            do_sample=False
        )
    
    # Decode output
    generated_ids = [
        output_ids[len(input_ids):]
        for input_ids, output_ids in zip(inputs.input_ids, output_ids)
    ]
    
    result = processor.batch_decode(
        generated_ids, 
        skip_special_tokens=True, 
        clean_up_tokenization_spaces=False
    )[0]
    
    return result

# Main execution
def main():
    OUTPUT_IMG_DIR = "/mnt/pdf_pages"
    
    # Create output directory
    if not os.path.exists(OUTPUT_IMG_DIR):
        os.makedirs(OUTPUT_IMG_DIR)
    
    # Convert PDF to images
    print(f"Converting PDF: {PDF_PATH}")
    images = pdf_to_images(PDF_PATH, save_dir=OUTPUT_IMG_DIR, dpi=300)
    
    # Extract text from each page
    all_text = []
    for i, img in enumerate(images):
        print(f"\nProcessing page {i+1}/{len(images)}")
        try:
            page_text = extract_text(img)
            all_text.append({
                "page": i+1,
                "text": page_text
            })
            print(f"‚úì Page {i+1} completed")
        except Exception as e:
            print(f"‚úó Error on page {i+1}: {str(e)}")
            all_text.append({
                "page": i+1,
                "text": f"ERROR: {str(e)}"
            })
    
    # Save results
    output_file = "extracted.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_text, f, indent=4, ensure_ascii=False)
    
    print(f"\n‚úì All text saved to {output_file}")

if __name__ == "__main__":
    main()








import os
import json
import torch
import fitz  # PyMuPDF
from PIL import Image
from transformers import AutoProcessor, Qwen2VLForConditionalGeneration

# Paths
MODEL_PATH = "/domina/cdv/modelhub/Modelhub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"

# Load model and processor
processor = AutoProcessor.from_pretrained(
    MODEL_PATH, 
    trust_remote_code=True
)

model = Qwen2VLForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,  # Use float16 instead of float32 for better performance
    device_map="auto",  # Auto device mapping
    trust_remote_code=True
)

def pdf_to_images(pdf_path, save_dir="/mnt/pdf_pages", dpi=300):
    """Convert PDF pages to images"""
    os.makedirs(save_dir, exist_ok=True)
    
    doc = fitz.open(pdf_path)
    images = []
    
    for i, page in enumerate(doc):
        # Render page to pixmap
        pix = page.get_pixmap(dpi=dpi)
        
        # Convert to PIL Image
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Save image
        img_path = f"{save_dir}/page_{i+1}.png"
        img.save(img_path)
        print(f"Saved page {i+1}: {img_path} ({img.width}x{img.height})")
        
        images.append(img)
    
    doc.close()
    return images

def extract_text(image):
    """Extract text from image using Qwen2.5-VL"""
    
    # Qwen2.5-VL specific prompt format
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text", 
                    "text": "Extract ALL visible text from this document page. Preserve the structure and formatting. Return plain text only."
                }
            ],
        }
    ]
    
    # Apply chat template
    text = processor.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    # Process inputs
    inputs = processor(
        text=[text],
        images=[image],
        padding=True,
        return_tensors="pt"
    )
    
    # Move inputs to same device as model
    inputs = inputs.to(model.device)
    
    # Generate
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=4096,
            temperature=0.1,
            do_sample=False
        )
    
    # Decode output
    generated_ids = [
        output_ids[len(input_ids):]
        for input_ids, output_ids in zip(inputs.input_ids, output_ids)
    ]
    
    result = processor.batch_decode(
        generated_ids, 
        skip_special_tokens=True, 
        clean_up_tokenization_spaces=False
    )[0]
    
    return result

# Main execution
def main():
    OUTPUT_IMG_DIR = "/mnt/pdf_pages"
    
    # Create output directory
    if not os.path.exists(OUTPUT_IMG_DIR):
        os.makedirs(OUTPUT_IMG_DIR)
    
    # Convert PDF to images
    print(f"Converting PDF: {PDF_PATH}")
    images = pdf_to_images(PDF_PATH, save_dir=OUTPUT_IMG_DIR, dpi=300)
    
    # Extract text from each page
    all_text = []
    for i, img in enumerate(images):
        print(f"\nProcessing page {i+1}/{len(images)}")
        try:
            page_text = extract_text(img)
            all_text.append({
                "page": i+1,
                "text": page_text
            })
            print(f"‚úì Page {i+1} completed")
        except Exception as e:
            print(f"‚úó Error on page {i+1}: {str(e)}")
            all_text.append({
                "page": i+1,
                "text": f"ERROR: {str(e)}"
            })
    
    # Save results
    output_file = "extracted.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_text, f, indent=4, ensure_ascii=False)
    
    print(f"\n‚úì All text saved to {output_file}")

if __name__ == "__main__":
    main()




import fitz  # PyMuPDF
from PIL import Image
import torch
from transformers import DonutProcessor, VisionEncoderDecoderModel
import json
import os

# =====================
# CONFIG
# =====================
MODEL_NAME = "naver-clova-ix/donut-base-finetuned-docvqa"  # Donut model
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"
OUTPUT_DIR = "/mnt/pdf_images"  # where we save images to check readability
os.makedirs(OUTPUT_DIR, exist_ok=True)

# =====================
# Load Donut Model
# =====================
processor = DonutProcessor.from_pretrained(MODEL_NAME)
model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)
model.eval()

# =====================
# Convert PDF ‚Üí Images
# =====================
def pdf_to_images(pdf_path, dpi=400):
    doc = fitz.open(pdf_path)
    images = []
    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        image_path = os.path.join(OUTPUT_DIR, f"page_{i+1}.png")
        img.save(image_path)  # Save image so you can check
        images.append(img)
        print(f"Saved page {i+1} image at: {image_path}")
    return images

# =====================
# Extract text from image
# =====================
def extract_text(image):
    # Donut expects a "task prompt" to describe what you want to extract
    prompt = "<s_docvqa> Please extract all text from this page. </s_docvqa>"
    pixel_values = processor(image, return_tensors="pt").pixel_values

    with torch.no_grad():
        output_ids = model.generate(pixel_values, max_length=4096)
    
    result = processor.batch_decode(output_ids, skip_special_tokens=True)[0]
    return result

# =====================
# MAIN
# =====================
images = pdf_to_images(PDF_PATH, dpi=400)

all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}...")
    page_text = extract_text(img)
    all_text.append({
        "page": i+1,
        "text": page_text
    })

# Save results
with open("/mnt/extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("Done! Text extracted to /mnt/extracted.json")















def pdf_to_model_images(pdf_path, size=1024):
    import os
    import fitz
    from PIL import Image

    save_dir = "/mnt/pdf_pages"
    os.makedirs(save_dir, exist_ok=True)
    doc = fitz.open(pdf_path)
    images = []

    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)  # keep 300 DPI, not too big
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        # Resize to model-friendly square
        img.thumbnail((size, size), Image.LANCZOS)
        square_img = Image.new("RGB", (size, size), "white")
        square_img.paste(img, (0, 0))

        img_path = f"{save_dir}/page_{i+1}.png"
        square_img.save(img_path)
        print(f"Saved {img_path} ({square_img.width}x{square_img.height})")
        images.append(square_img)

    return images





def pdf_to_images_readable(pdf_path, save_dir="/mnt/pdf_pages", dpi=400):
    os.makedirs(save_dir, exist_ok=True)
    doc = fitz.open(pdf_path)
    images = []

    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        # Pad to square without resizing smaller
        max_side = max(img.width, img.height)
        img_square = Image.new("RGB", (max_side, max_side), "white")
        img_square.paste(img, (0, 0))

        img_path = f"{save_dir}/page_{i+1}.png"
        img_square.save(img_path)
        print(f"Saved readable page {i+1} image: {img_path} ({img_square.width}x{img_square.height})")
        images.append(img_square)

    return images




def pdf_to_images(pdf_path, save_dir="/mnt/pdf_pages"):
    doc = fitz.open(pdf_path)
    images = []
    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        # --- Resize and convert to RGB for Qwen ---
        img = img.convert("RGB")
        img = img.resize((512, 512))  # adjust size if needed

        images.append(img)

        # Save the image to check
        img_path = f"{save_dir}/page_{i+1}.png"
        img.save(img_path)
        print(f"‚úì Saved page {i+1} as image: {img_path}")

    return images






OUTPUT_IMG_DIR = "/mnt/pdf_pages"

# Make sure output folder exists
if not os.path.exists(OUTPUT_IMG_DIR):
    os.makedirs(OUTPUT_IMG_DIR)

img_path = f"{OUTPUT_IMG_DIR}/page_{i+1}.png"





OUTPUT_IMG_DIR = "/mnt/user-data/pdf_pages"  # folder to save images
Path(OUTPUT_IMG_DIR).mkdir(parents=True, exist_ok=True)


def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)

        # Save the image to disk for inspection
        img_path = Path(OUTPUT_IMG_DIR) / f"page_{i+1}.png"
        img.save(img_path)
        print(f"‚úì Saved page {i+1} as image: {img_path}")

    return images











def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.

<image>
"""  # <-- crucial marker

    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result





import fitz  # PyMuPDF
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq
import torch
import json

# =====================
# CONFIG
# =====================
MODEL_PATH = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"

# =====================
# LOAD MODEL
# =====================
print("Loading model...")
processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,  # CPU safe
    trust_remote_code=True
)
model.eval()
print("Model loaded.")

# =====================
# PDF ‚Üí Images
# =====================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page in doc:
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# =====================
# Extract text from an image
# =====================
def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.
"""

    # Prepare input
    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    )

    # Forward pass
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0
        )

    # Decode result
    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# =====================
# MAIN
# =====================
images = pdf_to_images(PDF_PATH)

all_text = []
for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}")
    page_text = extract_text(img)
    all_text.append({
        "page": i+1,
        "text": page_text
    })

# Save results
with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("‚úÖ Done! Text saved to extracted.json")













import fitz  # PyMuPDF
from PIL import Image
import torch
from transformers import AutoProcessor, AutoModelForVision2Seq
import json

# =====================
# CONFIG
# =====================
MODEL_PATH = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"
DEVICE = "cpu"  # force CPU

# =====================
# Load Qwen Model
# =====================
print("Loading model...")
processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,  # CPU safe
    trust_remote_code=True
).to(DEVICE)
model.eval()
print("Model loaded.")

# =====================
# PDF ‚Üí Images
# =====================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page in doc:
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# =====================
# Extract Text per Page
# =====================
def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.
"""
    # Prepare inputs
    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    ).to(DEVICE)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# =====================
# Main
# =====================
images = pdf_to_images(PDF_PATH)
all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}")
    page_text = extract_text(img)
    all_text.append({
        "page": i+1,
        "text": page_text
    })

# Save JSON
with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("‚úÖ Done! Saved to extracted.json")









import fitz  # PyMuPDF
from PIL import Image
import torch
from transformers import AutoProcessor, AutoModelForVision2Seq
import json

# =====================
# CONFIG
# =====================
MODEL_PATH = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"
DEVICE = "cpu"  # force CPU

# =====================
# Load Qwen Model
# =====================
print("Loading model...")
processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,  # CPU safe
    trust_remote_code=True
).to(DEVICE)
model.eval()
print("Model loaded.")

# =====================
# PDF ‚Üí Images
# =====================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page in doc:
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# =====================
# Extract Text per Page
# =====================
def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.
"""
    # Prepare inputs
    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    ).to(DEVICE)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# =====================
# Main
# =====================
images = pdf_to_images(PDF_PATH)
all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}")
    page_text = extract_text(img)
    all_text.append({
        "page": i+1,
        "text": page_text
    })

# Save JSON
with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("‚úÖ Done! Saved to extracted.json")







import fitz  # PyMuPDF
from PIL import Image
import easyocr
import json

PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"

# Initialize EasyOCR reader (French + English)
reader = easyocr.Reader(['fr', 'en'], gpu=False)

# Convert PDF ‚Üí Images
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page in doc:
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# OCR each image
def extract_text_from_images(images):
    results = []
    for i, img in enumerate(images):
        print(f"Processing page {i+1}/{len(images)}")
        ocr_result = reader.readtext(np.array(img), detail=0)  # detail=0 ‚Üí just text
        page_text = "\n".join(ocr_result)
        results.append({"page": i+1, "text": page_text})
    return results

# Main
images = pdf_to_images(PDF_PATH)
all_text = extract_text_from_images(images)

# Save JSON
with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("‚úÖ Done! Saved to extracted.json")






pip install --upgrade --no-cache-dir torch

pip install --upgrade --no-cache-dir transformers>=4.40.0



import torch
import fitz
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq
import json

# =====================
# CONFIG
# =====================
MODEL_PATH = "/domino/edv/modelhub/.../Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/your_file.pdf"

# =====================
# Load Model (CPU ONLY)
# =====================
print("Loading model on CPU...")

processor = AutoProcessor.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True
)

model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,   # CPU safe
    trust_remote_code=True
)

model.eval()

print("Model loaded.")

# =====================
# Convert PDF ‚Üí Images
# =====================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []

    for page in doc:
        pix = page.get_pixmap(dpi=200)  # lower DPI for CPU
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)

    return images

# =====================
# OCR Function
# =====================
def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.
"""

    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=1024,   # smaller for CPU
            temperature=0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# =====================
# MAIN
# =====================
images = pdf_to_images(PDF_PATH)

all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)} (CPU mode, may be slow)")
    page_text = extract_text(img)

    all_text.append({
        "page": i + 1,
        "text": page_text
    })

with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("Done. Saved to extracted.json")











from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "Qwen/Qwen2.5-3B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

inputs = tokenizer("Explain transformers briefly:", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))






from transformers import AutoProcessor, AutoModelForVision2Seq

model = AutoModelForVision2Seq.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    trust_remote_code=True
)

processor = AutoProcessor.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    trust_remote_code=True
)

pip uninstall qwen -y
pip uninstall zeta -y

pip install transformers accelerate torch




Bonjour,

Veuillez trouver en pi√®ce jointe les r√©sultats de Sentinella pour le trimestre arr√™t√© √† fin f√©vrier 2026.

Les donn√©es couvrent la p√©riode du 01/01/2024 au 28/02/2026 et pr√©sentent la r√©partition des communaut√©s selon le nombre de clients ainsi que les montants par pays.

En r√©sum√© :

- 64 communaut√©s au total
- Montant global : 5 346 762 ‚Ç¨

La ventilation d√©taill√©e par rubrique et par pays est disponible dans le fichier joint.

Pour toute question ou besoin de compl√©ment d'information, Nihad prendra en charge la suite.

Cordialement,
Omar



#!/usr/bin/env python3
"""
Extract text from multi-page scanned PDFs using OpenAI GPT-4V (Vision).
Each PDF page is converted to an image and processed individually.
"""

import base64
from pathlib import Path
from typing import List, Dict
import fitz  # PyMuPDF
from openai import OpenAI


# -----------------------------
# 1Ô∏è‚É£ Convert PDF pages to images
# -----------------------------
def pdf_to_images(pdf_path: str) -> List[str]:
    """
    Convert each page of PDF to a PNG image and return list of image file paths.
    """
    pdf_file = Path(pdf_path)
    if not pdf_file.exists():
        raise FileNotFoundError(f"PDF not found: {pdf_path}")
    
    doc = fitz.open(pdf_file)
    images = []

    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)
        img_path = f"{pdf_file.stem}_page_{i+1}.png"
        pix.save(img_path)
        images.append(img_path)
    
    print(f"‚úì Converted {len(images)} pages to images.")
    return images


# -----------------------------
# 2Ô∏è‚É£ Encode image to base64
# -----------------------------
def encode_image_to_base64(image_path: str) -> str:
    """
    Encode image to base64 for GPT-4V input.
    """
    path = Path(image_path)
    if not path.exists():
        raise FileNotFoundError(f"Image not found: {image_path}")

    with open(path, "rb") as f:
        encoded = base64.b64encode(f.read()).decode("utf-8")
    
    return encoded


# -----------------------------
# 3Ô∏è‚É£ Extract text from image using GPT-4V
# -----------------------------
def extract_text_gpt4v(api_key: str, base64_image: str, model: str = "gpt-4o") -> Dict:
    """
    Send image to GPT-4V and return extracted text.
    """
    client = OpenAI(api_key=api_key)

    # Default to JPEG MIME
    mime_type = "image/png"  # since we converted pages to PNG

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": """Extract ALL text from this image with maximum accuracy.
Preserve structure, tables, line breaks, and French text exactly.
Output markdown for tables."""
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:{mime_type};base64,{base64_image}"}
                    }
                ]
            }],
            max_tokens=4096,
            temperature=0
        )
        extracted = response.choices[0].message.content
        return {"success": True, "text": extracted, "tokens_used": response.usage.total_tokens}
    except Exception as e:
        return {"success": False, "error": str(e)}


# -----------------------------
# 4Ô∏è‚É£ Process multi-page PDF
# -----------------------------
def extract_text_from_pdf(api_key: str, pdf_path: str) -> str:
    """
    Convert PDF to images, extract text for each page, and combine results.
    """
    images = pdf_to_images(pdf_path)
    full_text = ""

    for i, img_path in enumerate(images):
        print(f"üîÑ Processing page {i+1}/{len(images)}: {img_path}")
        b64 = encode_image_to_base64(img_path)
        result = extract_text_gpt4v(api_key, b64)

        if result["success"]:
            full_text += f"\n\n---\n\n## Page {i+1}\n\n"
            full_text += result["text"]
        else:
            full_text += f"\n\n---\n\n## Page {i+1}\n\n‚ùå Extraction failed: {result['error']}\n"
    
    return full_text


# -----------------------------
# 5Ô∏è‚É£ Save extracted text
# -----------------------------
def save_text(text: str, output_path: str):
    """
    Save extracted text to a file.
    """
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(text)
    print(f"‚úì Extracted text saved to {output_path}")


# -----------------------------
# 6Ô∏è‚É£ Main function
# -----------------------------
def main():
    PDF_PATH = "/mnt/user-data/uploads/scanned_document.pdf"
    OUTPUT_PATH = "/mnt/user-data/outputs/extracted_text_gpt4v.md"
    OPENAI_API_KEY = input("Enter your OpenAI API key: ").strip()

    print("üîπ Starting PDF text extraction using GPT-4V...")
    extracted_text = extract_text_from_pdf(OPENAI_API_KEY, PDF_PATH)
    save_text(extracted_text, OUTPUT_PATH)
    print("‚úÖ Extraction complete!")


if __name__ == "__main__":
    main()




#!/usr/bin/env python3
"""
Text extraction from images using Vision-Language Models (VLMs)
Supports: OpenAI GPT-4V, Anthropic Claude, Google Gemini
No OCR preprocessing required
"""

import os
import base64
from pathlib import Path
from typing import Optional, Dict, Any
import json

# Installation instructions:
# pip install openai anthropic google-generativeai pillow --break-system-packages

class VLMTextExtractor:
    """Extract text from images using various VLM models"""
    
    def __init__(self):
        self.image_path = None
        self.base64_image = None
    
    def load_image(self, image_path: str) -> None:
        """Load and encode image to base64"""
        self.image_path = Path(image_path)
        if not self.image_path.exists():
            raise FileNotFoundError(f"Image not found: {image_path}")
        
        with open(self.image_path, "rb") as image_file:
            self.base64_image = base64.b64encode(image_file.read()).decode('utf-8')
        
        print(f"‚úì Loaded image: {self.image_path.name}")
    
    def extract_with_gpt4v(self, api_key: str, model: str = "gpt-4o") -> Dict[str, Any]:
        """
        Extract text using OpenAI GPT-4 Vision
        Models: gpt-4o, gpt-4o-mini, gpt-4-turbo
        """
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=api_key)
            
            # Determine image format
            ext = self.image_path.suffix.lower()
            mime_type = {
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.png': 'image/png',
                '.webp': 'image/webp',
                '.gif': 'image/gif'
            }.get(ext, 'image/jpeg')
            
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": """Extract ALL text from this image with maximum accuracy. 
                                
Instructions:
1. Preserve the exact structure and layout
2. Maintain all tables, headers, and formatting
3. Include ALL numerical values, percentages, and currency amounts
4. Keep French text exactly as shown
5. Preserve line breaks and spacing where meaningful
6. Output in markdown format for tables

Please extract the complete text."""
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:{mime_type};base64,{self.base64_image}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=4096,
                temperature=0
            )
            
            extracted_text = response.choices[0].message.content
            
            return {
                "success": True,
                "model": model,
                "text": extracted_text,
                "tokens_used": response.usage.total_tokens
            }
            
        except Exception as e:
            return {
                "success": False,
                "model": model,
                "error": str(e)
            }
    
    def extract_with_claude(self, api_key: str, model: str = "claude-3-5-sonnet-20241022") -> Dict[str, Any]:
        """
        Extract text using Anthropic Claude Vision
        Models: claude-3-5-sonnet-20241022, claude-3-opus-20240229, claude-3-haiku-20240307
        """
        try:
            from anthropic import Anthropic
            
            client = Anthropic(api_key=api_key)
            
            # Determine media type
            ext = self.image_path.suffix.lower()
            media_type = {
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.png': 'image/png',
                '.webp': 'image/webp',
                '.gif': 'image/gif'
            }.get(ext, 'image/jpeg')
            
            message = client.messages.create(
                model=model,
                max_tokens=4096,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": media_type,
                                    "data": self.base64_image,
                                },
                            },
                            {
                                "type": "text",
                                "text": """Extract ALL text from this image with maximum accuracy.

Instructions:
1. Preserve the exact structure and layout
2. Maintain all tables, headers, and formatting
3. Include ALL numerical values, percentages, and currency amounts
4. Keep French text exactly as shown
5. Preserve line breaks and spacing where meaningful
6. Output in markdown format for tables

Please extract the complete text."""
                            }
                        ],
                    }
                ],
            )
            
            extracted_text = message.content[0].text
            
            return {
                "success": True,
                "model": model,
                "text": extracted_text,
                "tokens_used": message.usage.input_tokens + message.usage.output_tokens
            }
            
        except Exception as e:
            return {
                "success": False,
                "model": model,
                "error": str(e)
            }
    
    def extract_with_gemini(self, api_key: str, model: str = "gemini-1.5-pro") -> Dict[str, Any]:
        """
        Extract text using Google Gemini Vision
        Models: gemini-1.5-pro, gemini-1.5-flash, gemini-pro-vision
        """
        try:
            import google.generativeai as genai
            from PIL import Image
            
            genai.configure(api_key=api_key)
            
            model_instance = genai.GenerativeModel(model)
            
            # Load image with PIL
            img = Image.open(self.image_path)
            
            prompt = """Extract ALL text from this image with maximum accuracy.

Instructions:
1. Preserve the exact structure and layout
2. Maintain all tables, headers, and formatting
3. Include ALL numerical values, percentages, and currency amounts
4. Keep French text exactly as shown
5. Preserve line breaks and spacing where meaningful
6. Output in markdown format for tables

Please extract the complete text."""
            
            response = model_instance.generate_content([prompt, img])
            
            return {
                "success": True,
                "model": model,
                "text": response.text,
                "tokens_used": "N/A (not provided by Gemini API)"
            }
            
        except Exception as e:
            return {
                "success": False,
                "model": model,
                "error": str(e)
            }
    
    def save_results(self, results: Dict[str, Any], output_path: str) -> None:
        """Save extraction results to file"""
        output_file = Path(output_path)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"# Text Extraction Results\n\n")
            f.write(f"**Source Image:** {self.image_path.name}\n")
            f.write(f"**Model:** {results['model']}\n")
            f.write(f"**Success:** {results['success']}\n")
            
            if results['success']:
                f.write(f"**Tokens Used:** {results.get('tokens_used', 'N/A')}\n\n")
                f.write("---\n\n")
                f.write("## Extracted Text\n\n")
                f.write(results['text'])
            else:
                f.write(f"**Error:** {results['error']}\n")
        
        print(f"‚úì Results saved to: {output_file}")


def main():
    """Main execution function"""
    print("=" * 70)
    print("VLM Text Extraction Tool")
    print("=" * 70)
    print("\nSupported Models:")
    print("  1. OpenAI GPT-4V (gpt-4o, gpt-4o-mini)")
    print("  2. Anthropic Claude (claude-3-5-sonnet, claude-3-opus)")
    print("  3. Google Gemini (gemini-1.5-pro, gemini-1.5-flash)")
    print("\n")
    
    # Configuration
    IMAGE_PATH = "/mnt/user-data/uploads/1000024457.webp"
    
    # Initialize extractor
    extractor = VLMTextExtractor()
    extractor.load_image(IMAGE_PATH)
    
    print("\n" + "=" * 70)
    print("Select extraction method:")
    print("=" * 70)
    print("1. OpenAI GPT-4V")
    print("2. Anthropic Claude")
    print("3. Google Gemini")
    print("4. Extract with all models (requires all API keys)")
    print("\n")
    
    choice = input("Enter your choice (1-4): ").strip()
    
    results = []
    
    if choice == "1":
        api_key = input("\nEnter your OpenAI API key: ").strip()
        model = input("Enter model name (default: gpt-4o): ").strip() or "gpt-4o"
        print(f"\nüîÑ Extracting text using {model}...")
        result = extractor.extract_with_gpt4v(api_key, model)
        results.append(("gpt4v", result))
        
    elif choice == "2":
        api_key = input("\nEnter your Anthropic API key: ").strip()
        model = input("Enter model name (default: claude-3-5-sonnet-20241022): ").strip() or "claude-3-5-sonnet-20241022"
        print(f"\nüîÑ Extracting text using {model}...")
        result = extractor.extract_with_claude(api_key, model)
        results.append(("claude", result))
        
    elif choice == "3":
        api_key = input("\nEnter your Google API key: ").strip()
        model = input("Enter model name (default: gemini-1.5-pro): ").strip() or "gemini-1.5-pro"
        print(f"\nüîÑ Extracting text using {model}...")
        result = extractor.extract_with_gemini(api_key, model)
        results.append(("gemini", result))
        
    elif choice == "4":
        openai_key = input("\nEnter your OpenAI API key: ").strip()
        claude_key = input("Enter your Anthropic API key: ").strip()
        gemini_key = input("Enter your Google API key: ").strip()
        
        print("\nüîÑ Extracting with GPT-4V...")
        results.append(("gpt4v", extractor.extract_with_gpt4v(openai_key)))
        
        print("üîÑ Extracting with Claude...")
        results.append(("claude", extractor.extract_with_claude(claude_key)))
        
        print("üîÑ Extracting with Gemini...")
        results.append(("gemini", extractor.extract_with_gemini(gemini_key)))
    
    else:
        print("‚ùå Invalid choice!")
        return
    
    # Save results
    print("\n" + "=" * 70)
    print("Saving Results")
    print("=" * 70)
    
    for name, result in results:
        if result['success']:
            output_path = f"/mnt/user-data/outputs/extracted_text_{name}.md"
            extractor.save_results(result, output_path)
            print(f"‚úì {name.upper()}: Extraction successful")
            print(f"  Preview: {result['text'][:100]}...")
        else:
            print(f"‚ùå {name.upper()}: Extraction failed - {result['error']}")
    
    print("\n‚úÖ Process complete!")


if __name__ == "__main__":
    main()






import fitz  # PyMuPDF
from PIL import Image
from transformers import DonutProcessor, VisionEncoderDecoderModel

# -----------------------------
# üî• CHANGE THIS
# -----------------------------
MODEL_PATH = "/path_to_your_local_donut_model"
PDF_PATH = "your_scanned_file.pdf"

# -----------------------------
# 1Ô∏è‚É£ Convert PDF ‚Üí Images
# -----------------------------
def pdf_to_images(pdf_path, dpi=200):
    doc = fitz.open(pdf_path)
    images = []

    for page in doc:
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)

    return images

# -----------------------------
# 2Ô∏è‚É£ Load Model (CPU Only)
# -----------------------------
print("Loading local Donut model (CPU mode)...")

processor = DonutProcessor.from_pretrained(MODEL_PATH, local_files_only=True)
model = VisionEncoderDecoderModel.from_pretrained(MODEL_PATH, local_files_only=True)
model.eval()

print("Model loaded successfully ‚úÖ")

# -----------------------------
# 3Ô∏è‚É£ Donut Extraction Function
# -----------------------------
def run_donut(image):
    # Resize for CPU stability
    image = image.resize((1280, 1280))

    prompt = (
        "<s_docvqa>"
        "<s_question>Extract all information from this document</s_question>"
        "<s_answer>"
    )

    pixel_values = processor(image, return_tensors="pt").pixel_values
    decoder_input_ids = processor.tokenizer(
        prompt,
        add_special_tokens=False,
        return_tensors="pt"
    ).input_ids

    outputs = model.generate(
        pixel_values,
        decoder_input_ids=decoder_input_ids,
        max_length=512,  # lower for CPU
        early_stopping=True,
        pad_token_id=processor.tokenizer.pad_token_id,
        eos_token_id=processor.tokenizer.eos_token_id,
    )

    result = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return result

# -----------------------------
# 4Ô∏è‚É£ Process PDF
# -----------------------------
pages = pdf_to_images(PDF_PATH)
results = []

for i, page in enumerate(pages):
    print(f"Processing page {i+1}/{len(pages)}")
    text = run_donut(page)
    results.append(text)

final_text = "\n\n".join(results)
print("\n========== FINAL RESULT ==========\n")
print(final_text)






import torch
import fitz  # PyMuPDF
from PIL import Image
from transformers import DonutProcessor, VisionEncoderDecoderModel

# -----------------------------
# 1Ô∏è‚É£ Convert PDF to Images
# -----------------------------
def pdf_to_images(pdf_path, dpi=300):
    doc = fitz.open(pdf_path)
    images = []

    for page in doc:
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)

    return images


# -----------------------------
# 2Ô∏è‚É£ Load Donut Model
# -----------------------------
model_name = "naver-clova-ix/donut-base"

processor = DonutProcessor.from_pretrained(model_name)
model = VisionEncoderDecoderModel.from_pretrained(model_name)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)


# -----------------------------
# 3Ô∏è‚É£ Run Donut on Image
# -----------------------------
def run_donut(image, task_prompt="<s_docvqa><s_question>Extract all information</s_question><s_answer>"):
    
    pixel_values = processor(image, return_tensors="pt").pixel_values.to(device)

    decoder_input_ids = processor.tokenizer(
        task_prompt,
        add_special_tokens=False,
        return_tensors="pt"
    ).input_ids.to(device)

    outputs = model.generate(
        pixel_values,
        decoder_input_ids=decoder_input_ids,
        max_length=1024,
        early_stopping=True,
        pad_token_id=processor.tokenizer.pad_token_id,
        eos_token_id=processor.tokenizer.eos_token_id,
    )

    result = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return result


# -----------------------------
# 4Ô∏è‚É£ Process Full PDF
# -----------------------------
pdf_path = "your_scanned.pdf"

images = pdf_to_images(pdf_path)

all_results = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}...")
    result = run_donut(img)
    all_results.append(result)

# Combine all pages
final_output = "\n\n".join(all_results)

print("\n===== FINAL OUTPUT =====\n")
print(final_output)







pip install transformers torch torchvision pillow pymupdf sentencepiece



task_prompt = """
<s_docvqa>
<s_question>
Extract all structured information and return valid JSON.
</s_question>
<s_answer>
"""






pip install paddlepaddle paddleocr


import fitz
from PIL import Image

doc = fitz.open("your_scanned.pdf")
page = doc[0]

pix = page.get_pixmap(dpi=300)
img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
img.save("page.png")


from paddleocr import PaddleOCR

ocr = PaddleOCR(lang='en')
result = ocr.ocr("page.png")

text = "\n".join([line[1][0] for line in result[0]])
print(text)





import fitz
from PIL import Image

def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

images = pdf_to_images("scanned.pdf")





pip uninstall qwen zeta transformers torch triton -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install qwen==0.1.1 qwen-agent qwen-vl-utils
pip install transformers==4.39.1 triton==2.2.0


# Uninstall potentially conflicting packages
pip uninstall qwen zeta -y

# Install PyTorch first (if not already installed)
pip install torch torchvision --break-system-packages

# Install the correct Qwen package
pip install transformers --break-system-packages
pip install qwen-vl-utils --break-system-packages




pip install torch torchvision torchaudio  # Core PyTorch
pip install pillow                         # Image handling
pip install pymupdf                        # PDF ‚Üí images
pip install qwen                           # Official Qwen VL package




pip install tqdm                            # Progress bars
pip install numpy                           # Array operations (often required by Qwen)
pip install transformers                     # Only if you need extra transformer utils


pip install tqdm                            # Progress bars
pip install numpy                           # Array operations (often required by Qwen)
pip install transformers                     # Only if you need extra transformer utils






import fitz  # PyMuPDF
import json
from PIL import Image
from qwen import QwenVL, QwenProcessor  # official Qwen VL loader

# ===============================
# CONFIG
# ===============================
MODEL_PATH = "/path/to/Qwen2.5-VL-32B-Instruct"  # your local folder
PDF_PATH = "scanned.pdf"
OUTPUT_FILE = "output.json"

# ===============================
# LOAD MODEL
# ===============================
print("Loading Qwen2.5-VL-32B-Instruct...")
processor = QwenProcessor.from_pretrained(MODEL_PATH)
model = QwenVL.from_pretrained(MODEL_PATH)
model.eval()  # CPU by default
print("Model loaded successfully.")

# ===============================
# CONVERT PDF TO IMAGES
# ===============================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page_number in range(len(doc)):
        page = doc[page_number]
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# ===============================
# EXTRACT JSON FROM IMAGE
# ===============================
def extract_json_from_image(image):
    prompt = """
You are a document extraction engine.

Extract ALL visible text and structure from this scanned document.

Return strictly valid JSON only.
Do not add explanations or summaries.
"""

    # Preprocess the image + prompt
    inputs = processor(prompt, image, return_tensors="pt")

    # Generate the model output
    with torch.no_grad():
        output = model.generate(**inputs, max_new_tokens=2048, temperature=0.0)

    # Decode text
    text_output = processor.decode(output[0])
    return text_output

# ===============================
# MAIN PIPELINE
# ===============================
def main():
    images = pdf_to_images(PDF_PATH)
    all_pages = []

    for i, img in enumerate(images):
        print(f"Processing page {i+1}...")
        result_text = extract_json_from_image(img)

        # Attempt to parse JSON, fallback to raw output
        try:
            parsed = json.loads(result_text)
        except json.JSONDecodeError:
            parsed = {"raw_output": result_text}

        all_pages.append({
            "page_number": i + 1,
            "content": parsed
        })

    final_output = {
        "document_name": PDF_PATH,
        "total_pages": len(all_pages),
        "pages": all_pages
    }

    # Save to JSON file
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)

    print(f"Extraction complete. Saved to {OUTPUT_FILE}")

# ===============================
# ENTRY POINT
# ===============================
if __name__ == "__main__":
    main()




import fitz  # PyMuPDF
import json
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq

# ===============================
# CONFIG
# ===============================
MODEL_PATH = "/path/to/Qwen2.5-VL-32B-Instruct"  # Your local folder
PDF_PATH = "scanned.pdf"
OUTPUT_FILE = "output.json"

# ===============================
# Load Model
# ===============================
print("Loading model...")
processor = AutoProcessor.from_pretrained(MODEL_PATH)
model = AutoModelForVision2Seq.from_pretrained(MODEL_PATH)
model.eval()
print("Model loaded.")

# ===============================
# Convert PDF to Images
# ===============================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page_number in range(len(doc)):
        page = doc[page_number]
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# ===============================
# Extract JSON from Image
# ===============================
def extract_json_from_image(image):
    prompt = """
You are a document extraction engine.

Extract ALL visible text and structure from this scanned document.

Return strictly valid JSON.
Do not add explanations.
"""

    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0.0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# ===============================
# MAIN
# ===============================
def main():
    images = pdf_to_images(PDF_PATH)
    all_pages = []

    for i, img in enumerate(images):
        print(f"Processing page {i+1}...")
        result = extract_json_from_image(img)

        try:
            parsed = json.loads(result)
        except:
            parsed = {"raw_output": result}

        all_pages.append({
            "page_number": i + 1,
            "content": parsed
        })

    final_output = {
        "document_name": PDF_PATH,
        "total_pages": len(all_pages),
        "pages": all_pages
    }

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)

    print(f"Extraction complete. Saved to {OUTPUT_FILE}")

if __name__ == "__main__":
    main()





pip install torch torchvision transformers pymupdf pillow




if "chat_sessions" not in st.session_state:
    st.session_state.chat_sessions = load_chat_sessions()

import json
import os

CHAT_STORAGE_PATH = "chat_sessions.json"


def load_chat_sessions():
    if os.path.exists(CHAT_STORAGE_PATH):
        with open(CHAT_STORAGE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_chat_sessions(chat_sessions):
    with open(CHAT_STORAGE_PATH, "w", encoding="utf-8") as f:
        json.dump(chat_sessions, f, indent=2, ensure_ascii=False)









if st.sidebar.button("‚ûï Start New Chat"):
    chat_id = str(datetime.datetime.now())
    st.session_state.current_chat = chat_id
    st.session_state.chat_sessions[chat_id] = {
        "title": "New Chat",
        "messages": []
    }
    save_chat_sessions(st.session_state.chat_sessions)  # ‚úÖ ADD THIS





chat_data["messages"].append({
    "role": "user",
    "content": user_input,
    "timestamp": timestamp
})

save_chat_sessions(st.session_state.chat_sessions)  # ‚úÖ ADD THIS






import os
import glob

from langchain.document_loaders import (
    PyPDFLoader,
    CSVLoader,
    UnstructuredExcelLoader,
    UnstructuredWordDocumentLoader
)

# Your real documents directory
DOCS_DIR = "ant/"

def load_documents():
    documents = []

    procedures_dir = os.path.join(DOCS_DIR, "Procedures")

    # ---------- Load PDFs ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.pdf")):
            try:
                loader = PyPDFLoader(file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "pdf"

                documents.extend(docs)
                print(f"Loaded PDF: {file_name}")

            except Exception as e:
                print(f"Error loading PDF {file}: {e}")

    # ---------- Load CSVs ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.csv")):
            try:
                loader = CSVLoader(file_path=file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "csv"

                documents.extend(docs)
                print(f"Loaded CSV: {file_name}")

            except Exception as e:
                print(f"Error loading CSV {file}: {e}")

    # ---------- Load Excel ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.xlsx")):
            try:
                loader = UnstructuredExcelLoader(file_path=file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "excel"

                documents.extend(docs)
                print(f"Loaded Excel: {file_name}")

            except Exception as e:
                print(f"Error loading Excel {file}: {e}")

    # ---------- Load Word (.docx) ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.docx")):
            try:
                loader = UnstructuredWordDocumentLoader(file_path=file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "word"

                documents.extend(docs)
                print(f"Loaded Word: {file_name}")

            except Exception as e:
                print(f"Error loading Word {file}: {e}")

    print(f"\nLoaded {len(documents)} documents in total.")
    return documents













def main():
    st.title("üí¨ Chatbot BNP ED")

    # -----------------------------
    # 1Ô∏è‚É£ Session initialization
    # -----------------------------
    if "chat_sessions" not in st.session_state:
        st.session_state.chat_sessions = {}

    if "current_chat" not in st.session_state:
        chat_id = str(datetime.datetime.now())
        st.session_state.current_chat = chat_id
        st.session_state.chat_sessions[chat_id] = {
            "title": "New Chat",
            "messages": []
        }

    # -----------------------------
    # 2Ô∏è‚É£ Start new chat
    # -----------------------------
    if st.sidebar.button("‚ûï Start New Chat"):
        chat_id = str(datetime.datetime.now())
        st.session_state.current_chat = chat_id
        st.session_state.chat_sessions[chat_id] = {
            "title": "New Chat",
            "messages": []
        }

    # -----------------------------
    # 3Ô∏è‚É£ Chat selector
    # -----------------------------
    chat_ids = list(st.session_state.chat_sessions.keys())

    selected_chat = st.sidebar.radio(
        "Chat Sessions",
        chat_ids,
        format_func=lambda k: st.session_state.chat_sessions[k]["title"],
        index=chat_ids.index(st.session_state.current_chat)
        if st.session_state.current_chat in chat_ids else 0
    )

    st.session_state.current_chat = selected_chat
    chat_data = st.session_state.chat_sessions[selected_chat]

    # -----------------------------
    # 4Ô∏è‚É£ Display chat history
    # -----------------------------
    for message in chat_data["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"], unsafe_allow_html=True)

    # -----------------------------
    # 5Ô∏è‚É£ User input
    # -----------------------------
    user_input = st.chat_input("Type your message...")

    if user_input:
        timestamp = datetime.datetime.now().isoformat()

        # Rename chat on first message
        if chat_data["title"] == "New Chat":
            chat_data["title"] = user_input[:30] + "..."

        # Save user message
        chat_data["messages"].append({
            "role": "user",
            "content": user_input,
            "timestamp": timestamp
        })

        with st.chat_message("user"):
            st.markdown(user_input)

        # Generate assistant response
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = rag_chatbot(user_input)
                answer = response["answer"]
                st.markdown(answer, unsafe_allow_html=True)

        # Save assistant message
        chat_data["messages"].append({
            "role": "assistant",
            "content": answer,
            "timestamp": timestamp
        })










def get_chat_history(max_turns=6):
    messages = st.session_state.chat_sessions[
        st.session_state.current_chat
    ]["messages"]

    history = []
    for msg in messages[-max_turns:]:
        role = msg["role"].capitalize()
        history.append(f"{role}: {msg['content']}")

    return history

from openai import AzureOpenAI
import httpx


def generate_response(query, docs):
    context = build_context(docs)
    chat_history = get_chat_history()

    history_text = "\n".join(chat_history)

    prompt = f"""
Vous √™tes un assistant RAG sp√©cialis√© dans les syst√®mes bancaires,
l‚Äôarchitecture de donn√©es et les produits financiers.

[Historique]
{history_text}

[Contexte]
{context}

[Question]
{query}

INSTRUCTIONS:
- Utilisez exclusivement le contexte
- Citez les sources: [Source: fichier.pdf, Page X]
- Si l'information n'existe pas, dites-le clairement
"""

    client = AzureOpenAI(
        api_key=AZURE_OPENAI_API_KEY,
        api_version=AZURE_OPENAI_API_VERSION,
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        http_client=httpx.Client(verify=False)
    )

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "Assistant RAG bancaire"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3
    )

    return response.choices[0].message.content


def rag_chatbot(query):
    chat_history = get_chat_history()

    retriever = CustomRetrieverWithHistory(
        collection=collection,
        chat_history=chat_history,
        k=2500,
        rerank_k=50
    )

    docs = retriever.get_relevant_documents(query)
    answer = generate_response(query, docs)

    return {
        "answer": answer,
        "documents": docs
    }














def generate_response(query, context):
    # Collect sources
    sources = list(set([chunk["metadata"]["source"] for chunk in context]))

    # Build prompt
    context_text = build_context(context)
    prompt = f"""
Vous √™tes un assistant RAG sp√©cialis√© dans les syst√®mes bancaires,
l'architecture de donn√©es, et les produits financiers. Votre r√¥le est de r√©pondre et d'expliquer clairement les
informations issues des documents, en les rendant compr√©hensibles pour tout type d'utilisateur, du plus g√©n√©ral au plus technique.

[Contexte]
{context_text}

[Requ√™te]
{query}

[INSTRUCTIONS GENERALES]
1. Utilisez exclusivement les informations pr√©sentes dans le contexte ci-dessus.
2. Fournissez une r√©ponse claire, structur√©e et d√©taill√©e, adapt√©e √† la compr√©hension de tout utilisateur.
3. Citez imm√©diatement la source sous le format [Source: <nom du fichier.pdf, Page <num√©ro>].
4. Si l'information n'existe pas dans les documents, indiquez-le clairement.
5. Structure attendue:
   a. R√©ponse d√©taill√©e
   b. Synth√®se (si utile)
   c. Sources
6. Objectif: rendre la r√©ponse informative, explicative et v√©rifiable.
"""

    # Initialize Azure/OpenAI client
    # Make sure the API key, endpoint, and version are correct
    import httpx
    from openai import OpenAI  # or your wrapper

    client = OpenAI(
        api_key="YOUR_API_KEY",
        api_base="YOUR_AZURE_ENDPOINT",
        api_type="azure",
        api_version="2023-03-15-preview"  # replace with your version
    )

    # Call the model
    completion = client.chat.completions.create(
        model="gpt4o",
        messages=[
            {"role": "system", "content": "You are an assistant based on RAG for banking system architecture."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.5  # 0.5 is reasonable; 8.5 is invalid
    )

    # Handle return type (Azure wrapper may return string or dict)
    if isinstance(completion, str):
        response = completion
    elif hasattr(completion, "choices"):
        response = completion.choices[0].message["content"]
    else:
        raise ValueError(f"Unexpected completion type: {type(completion)}")

    # Clean up whitespace for readability
    lines = [line.strip() for line in response.split("\n") if line.strip()]
    readable_response = "\n".join(lines)

    return readable_response










Here‚Äôs a polished French version of your email:


---

Objet : Mise √† jour de la documentation ¬´ Usine de Fiable ¬ª

Bonjour [Nom du coll√®gue],

Veuillez trouver ci-joint la version mise √† jour de la documentation ¬´ Usine de Fiable ¬ª, √† laquelle j‚Äôai ajout√© le point [ou ‚Äúcette section‚Äù selon ce que vous avez ajout√©].

Merci de bien vouloir confirmer la r√©ception et n‚Äôh√©sitez pas √† m‚Äôajouter tout √©l√©ment que vous souhaitez discuter ou revoir.

Cordialement,
[Votre nom]


---

If you want, I can also make a slightly more friendly and collaborative version that encourages discussion without sounding too formal. Do you want me to do that?






appliqu√©es apr√®s la production de ce mois.


Ajout d'une nouvelle donn√©e / nouveau contr√¥le ¬´ DCC ¬ª :
Ainsi, si vous avez re√ßu un nouveau contr√¥le de donn√©e, effectu√© une modification sur un contr√¥le existant, ou r√©activ√© un ancien contr√¥le de donn√©e dans le p√©rim√®tre, ou effectu√© toute autre forme de modification, merci de ne pas appliquer ces modifications apr√®s le 20 de chaque mois.




# Initialize retriever
retriever = CustomRetrieverWithHistory(
    collection=collection,      # your Chroma collection
    query_model=query_model,    # BGE-M3 with instruction
    doc_model=doc_model,        # BGE-M3 without instruction
    chat_history=[],            # optional
    k=100,                      # fetch top 100 dense docs
    rerank_k=50                 # rerank top 50 with BM25
)

query = "Which table stores AML risk classification for retail clients?"

docs = retriever.get_relevant_documents(query)

# Output results
for d in docs:
    print(d["metadata"], "\n", d["text"])




import string
import re
import numpy as np
from rank_bm25 import BM25Okapi

# ---------- BM25 Reranker ----------
class BM25Reranker:
    def __init__(self, k=50):
        self.k = k
        self.bm25 = None
        self.document_map = {}
        self.tokenizer = str.maketrans("", "", string.punctuation)

    def _tokenize(self, text):
        if not isinstance(text, str):
            return []
        cleaned = text.translate(self.tokenizer).lower()
        return [t for t in cleaned.split() if t.strip()]

    def fit(self, documents):
        if not documents:
            raise ValueError("Empty document list given to BM25Reranker")
        tokenized_corpus = []
        valid_docs = []
        for i, doc in enumerate(documents):
            text = doc.get("text") or doc.get("page content") or doc.get("content")
            tokens = self._tokenize(text)
            if tokens:
                tokenized_corpus.append(tokens)
                valid_docs.append((i, doc))
        if not tokenized_corpus:
            raise ValueError("No valid documents to fit BM25 model")
        self.document_map = {i: doc for i, doc in valid_docs}
        self.bm25 = BM25Okapi(tokenized_corpus)

    def rerank(self, query, documents):
        if not documents:
            return []
        self.fit(documents)
        tokens = self._tokenize(query)
        if not tokens:
            return documents[:self.k]
        scores = self.bm25.get_scores(tokens)
        ranked = sorted(
            zip(scores, self.document_map.values()),
            key=lambda x: x[0],
            reverse=True
        )
        return [doc for _, doc in ranked[:self.k]]



# ---------- Custom Retriever with Query & Document Embeddings ----------
class CustomRetrieverWithHistory:
    def __init__(self, collection, query_model, doc_model, chat_history=None, k=50, rerank_k=50):
        """
        collection: Chroma collection
        query_model: BGE-M3 model with query_instruction
        doc_model: BGE-M3 model without instruction
        chat_history: optional chat context
        k: number of dense results to fetch
        rerank_k: number of results for BM25 reranking
        """
        self.collection = collection
        self.chat_history = chat_history or []
        self.k = k
        self.rerank_k = rerank_k
        self.query_model = query_model
        self.doc_model = doc_model
        self.reranker = BM25Reranker(k=rerank_k)

    def get_relevant_documents(self, query):
        # --- combine chat history if exists ---
        if self.chat_history:
            full_query = " ".join(self.chat_history) + " " + query
        else:
            full_query = query

        # --- encode the query using the correct query embedding model ---
        query_embedding = self.query_model.encode([full_query])[0].tolist()

        # --- dense retrieval using query embedding ---
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=self.k,
            include=["documents", "metadatas"]
        )

        # --- collect dense docs ---
        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(results["documents"][0], results["metadatas"][0])
        ]

        # --- BM25 rerank over dense docs as hybrid step ---
        reranked_docs = self.reranker.rerank(full_query, dense_docs)

        return reranked_docs


query = "Which table stores AML risk classification for retail clients?"

query_embedding = query_model.encode([query])[0].tolist()

results = collection.query(
    query_embeddings=[query_embedding],
    n_results=10,
    include=["documents", "metadatas"]
)




MODEL_PATH = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"

# ---- Document embedding model (NO instruction) ----
doc_model = FlagModel(
    MODEL_PATH,
    use_fp16=True
)

# ---- Query embedding model (WITH instruction) ----
query_model = FlagModel(
    MODEL_PATH,
    query_instruction_for_retrieval=(
        "Generate representation for this sentence to retrieve relevant articles:"
    ),
    use_fp16=True
)












class BGE_M3_EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        self.dimension = 1024  # BGE-M3 dimension

    def __call__(self, inputs: List[str]) -> List[List[float]]:
        if isinstance(inputs, str):
            inputs = [inputs]

        embeddings = self.model.encode(inputs)

        if len(embeddings[0]) != self.dimension:
            raise ValueError(
                f"Embedding dimension mismatch: expected {self.dimension}, "
                f"got {len(embeddings[0])}"
            )

        return embeddings.tolist()collection = chroma_client.get_or_create_collection(
    name="my_documents",
    embedding_function=BGE_M3_EmbeddingFunction(doc_model),
    metadata={
        "hnsw:space": "cosine",
        "hnsw:construction_ef": 200,
        "hnsw:M": 16
    }
)




query = "CLIENT_ACCOUNT risk_level AML"

dense = chroma_collection.query(
    query_texts=[query],
    n_results=1000,   # very large on purpose
    include=["documents"]
)

dense_docs = dense["documents"][0]

found = any(
    any(chunk_id in d for chunk_id in ["CLIENT_ACCOUNT", "risk_level"])
    for d in dense_docs
)

print("Found in dense@1000:", found)




class BM25Reranker:
    def __init__(self, documents, k=50):
        """
        documents: list of {"text": ..., "metadata": ...}
        """
        self.k = k
        self.documents = documents
        self.tokenized_corpus = [self._tokenize(d["text"]) for d in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)

    def _tokenize(self, text: str):
        # preserves table names, columns, acronyms
        return re.findall(r"[A-Za-z0-9_\.]+", text.lower())

    def retrieve(self, query: str):
        tokens = self._tokenize(query)
        scores = self.bm25.get_scores(tokens)

        ranked = sorted(
            zip(scores, self.documents),
            key=lambda x: x[0],
            reverse=True
        )

        return [doc for _, doc in ranked[:self.k]]


class HybridRetriever:
    def __init__(
        self,
        chroma_collection,
        bm25_reranker,
        k_dense=200,
        k_final=20
    ):
        self.collection = chroma_collection
        self.bm25 = bm25_reranker
        self.k_dense = k_dense
        self.k_final = k_final

    def retrieve(self, query: str):
        # ---- 1. Query expansion (light, safe) ----
        expanded_query = (
            query
            + " table database schema column risk AML architecture"
        )

        # ---- 2. Dense retrieval (high recall) ----
        dense = self.collection.query(
            query_texts=[expanded_query],
            n_results=self.k_dense,
            include=["documents", "metadatas"]
        )

        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(dense["documents"][0], dense["metadatas"][0])
        ]

        # ---- 3. Sparse retrieval (BM25 rescue) ----
        sparse_docs = self.bm25.retrieve(query)

        # ---- 4. Union + dedup ----
        merged = {}
        for d in dense_docs + sparse_docs:
            merged[hash(d["text"])] = d

        merged_docs = list(merged.values())

        return merged_docs[:self.k_final]

def output_matching_chunks(collection, keyword: str):
    data = collection.get(include=["documents", "metadatas"])

    documents = data["documents"]
    metadatas = data["metadatas"]

    # handle list-of-lists
    if documents and isinstance(documents[0], list):
        documents = documents[0]
        metadatas = metadatas[0]

    hits = 0
    for i, (doc, meta) in enumerate(zip(documents, metadatas)):
        if keyword.lower() in doc.lower():
            print(f"\n--- MATCHING CHUNK {i} ---")
            print("Metadata:", meta)
            print(doc)
            hits += 1

    print("\nTotal matching chunks:", hits)

def build_context(docs):
    blocks = []
    for i, d in enumerate(docs):
        meta = d.get("metadata", {})
        header = f"""
[Chunk {i+1}]
Database: {meta.get('db', 'N/A')}
Schema: {meta.get('schema', 'N/A')}
Table: {meta.get('table', 'N/A')}
"""
        blocks.append(header + d["text"])
    return "\n\n".join(blocks)

# documents = same docs used for Chroma ingestion
bm25 = BM25Reranker(documents, k=100)

retriever = HybridRetriever(
    chroma_collection=chroma_collection,
    bm25_reranker=bm25,
    k_dense=250,
    k_final=20
)

query = "Which table stores AML risk classification for retail clients?"

docs = retriever.retrieve(query)

for d in docs:
    print(d["metadata"])query = "Which table stores AML risk classification for retail clients?"

docs = retriever.retrieve(query)

for d in docs:
    print(d["metadata"])

context = build_context(docs)

prompt = f"""
You are a banking information system architect.

Answer ONLY using the context.
If not found, say "Information not found".

Context:
{context}

Question:
{query}
"""








keyword = "CLIENT_ACCOUNT"

data = chroma_collection.get(
    include=["documents", "metadatas"]
)

hits = 0

documents = data["documents"]
metadatas = data["metadatas"]

# Handle both flat list and list-of-lists
if documents and isinstance(documents[0], list):
    documents = documents[0]
    metadatas = metadatas[0]

for i, (doc, meta) in enumerate(zip(documents, metadatas)):
    if keyword.lower() in doc.lower():
        print(f"\n--- MATCHING CHUNK {i} ---")
        print("Metadata:", meta)
        print(doc)
        hits += 1

print("\nTotal matching chunks:", hits)











data = chroma_collection.get(
    where={"source": "core_banking_architecture.pdf"},
    include=["documents", "metadatas"]
)

for i, doc in enumerate(data["documents"]):
    print(f"\n--- CHUNK {i} ---\n")
    print(doc)





bm25_reranker = BM25Reranker(
    documents=documents,
    k=50   # how many BM25 docs you want
)


hybrid_retriever = HybridRetriever(
    collection=chroma_collection,
    bm25_reranker=bm25_reranker,
    k_dense=120,   # large recall
    k_final=20     # final docs sent to LLM
)

query = "Which database stores AML risk classification for retail clients?"

retrieved_docs = hybrid_retriever.retrieve(query)



from rank_bm25 import BM25Okapi
import re

class BM25Reranker:
    def __init__(self, documents, k=20):
        self.k = k
        self.documents = documents
        self.tokenized_corpus = [self._tokenize(d["text"]) for d in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)

    def _tokenize(self, text):
        return re.findall(r"[A-Za-z0-9_\.]+", text.lower())

    def rerank(self, query):
        query_tokens = self._tokenize(query)
        scores = self.bm25.get_scores(query_tokens)

        ranked = sorted(
            zip(scores, self.documents),
            key=lambda x: x[0],
            reverse=True
        )
        return [doc for _, doc in ranked[:self.k]]class HybridRetriever:
    def __init__(self, collection, bm25_reranker, k_dense=100, k_final=20):
        self.collection = collection
        self.bm25 = bm25_reranker
        self.k_dense = k_dense
        self.k_final = k_final

    def retrieve(self, query):
        dense = self.collection.query(
            query_texts=[query],
            n_results=self.k_dense,
            include=["documents", "metadatas"]
        )

        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(dense["documents"][0], dense["metadatas"][0])
        ]

        # BM25 rescue
        sparse_docs = self.bm25.rerank(query)

        # Union + dedup
        seen = set()
        merged = []
        for d in dense_docs + sparse_docs:
            h = hash(d["text"])
            if h not in seen:
                seen.add(h)
                merged.append(d)

        return merged[:self.k_final]from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_community.document_compressors.rankllm_rerank import RankLLMRerank
from typing import List

def hybrid_retrieve_langchain(
    collection,           # Your vector DB collection (Chroma/FAISS)
    llm_client,           # Azure/OpenAI client (LLM)
    query: str,
    k_dense: int = 50,    # Number of top docs from embeddings
    k_final: int = 20     # Number of final docs after reranking
) -> List[dict]:
    """
    Hybrid Retrieval using:
    Step 1: Dense retrieval from vector DB
    Step 2: RankLLM reranking (via LangChain)
    Returns top documents (text + metadata)
    """

    # --------------------------
    # Step 1: Dense retrieval
    # --------------------------
    dense_results = collection.query(
        query_texts=[query],
        n_results=k_dense,
        include=["documents", "metadatas"]
    )

    candidates = []
    for text, meta in zip(dense_results["documents"][0], dense_results["metadatas"][0]):
        candidates.append({"text": text, "metadata": meta})

    if not candidates:
        return []

    # --------------------------
    # Step 2: RankLLM reranker
    # --------------------------
    reranker = RankLLMRerank.from_llm(
        llm=llm_client,  # your Azure/OpenAI client
        top_n=k_final,
        verbose=True
    )

    # Optional: wrap with ContextualCompressionRetriever if you want
    # to compress context before passing to reranker (useful for long chunks)
    retriever = ContextualCompressionRetriever(
        base_retriever=candidates,  # here just the list of candidates
        compressor=reranker
    )

    # Run reranking
    reranked_docs = reranker.rerank(query=query, documents=candidates)

    return reranked_docs[:k_final]






query = "How to configure user permissions in the system"

top_docs = hybrid_retrieve_langchain(
    collection=chroma_collection,
    llm_client=azure_openai_client,
    query=query,
    k_dense=80,
    k_final=20
)

for doc in top_docs:
    print(doc["text"][:300], "...\n")







import streamlit as st
import chromadb
from chromadb.utils import embedding_functions
import fitz  # PyMuPDF
from docx import Document
import pandas as pd
from datetime import datetime
import json
import os
from typing import List, Dict, Tuple, Optional
import re
from collections import defaultdict
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
import hashlib

# Azure OpenAI imports
from openai import AzureOpenAI
import httpx

# ==============================================================================
# CONFIGURATION
# ==============================================================================

class Config:
    """Centralized configuration management"""
    
    # Azure OpenAI Configuration
    OIDC_CLIENT_ID = os.getenv("OIDC_CLIENT_ID", "your_client_id")
    OIDC_CLIENT_SECRET = os.getenv("OIDC_CLIENT_SECRET", "your_client_secret")
    OIDC_ENDPOINT = os.getenv("OIDC_ENDPOINT", "https://alfactory.api.staging.schonet/auth/oauth2/v2/token")
    OIDC_SCOPE = os.getenv("OIDC_SCOPE", "genai-model")
    
    AZURE_ENDPOINT = os.getenv("AZURE_ENDPOINT", "https://alfactory.api.staging.schonet/genai-model/v1")
    AZURE_API_VERSION = os.getenv("AZURE_API_VERSION", "2024-05-01-preview")
    AZURE_API_KEY = os.getenv("AZURE_API_KEY", "FAKE_KEY")
    AZURE_MODEL_DEPLOYMENT = os.getenv("AZURE_MODEL_DEPLOYMENT", "gpt-4o")
    
    # Embedding Configuration
    EMBEDDING_MODEL_PATH = "/domino/datasets/local/test-prd-base"
    EMBEDDING_DIMENSION = 1024
    
    # ChromaDB Configuration
    CHROMA_PERSIST_PATH = "/domino/datasets/local/chroma_persistent_db"
    COLLECTION_NAME = "banking_documents"
    
    # Chunking Configuration for Banking Documents
    CHUNK_SIZES = {
        'header': 300,      # For sections with headers
        'table': 500,       # For tables and structured data
        'paragraph': 400,   # For regular paragraphs
        'list': 350,        # For lists and enumerations
    }
    CHUNK_OVERLAP = 100
    
    # BM25 Configuration
    BM25_INDEX_PATH = "/domino/datasets/local/bm25_index.pkl"
    BM25_K1 = 1.5
    BM25_B = 0.75
    
    # Retrieval Configuration
    INITIAL_RETRIEVAL_K = 20  # Retrieve more for reranking
    FINAL_RESULTS_K = 5       # Final results after reranking
    HYBRID_ALPHA = 0.5        # Balance between semantic (0) and keyword (1)
    
    # Batch Processing Configuration
    BATCH_SIZE = 100
    MAX_WORKERS = 4

# ==============================================================================
# DOCUMENT PROCESSING & CHUNKING
# ==============================================================================

class BankingDocumentProcessor:
    """Advanced document processor optimized for banking/technical documents"""
    
    def __init__(self):
        self.chunk_patterns = {
            'section_header': re.compile(r'^(Sect\.|Section|Chapitre|Article|¬ß)\s*[\dIVX]+[\.\s]', re.IGNORECASE),
            'subsection': re.compile(r'^[A-Z]{1,3}\.\d+(\.\d+)*\s+', re.MULTILINE),
            'table_marker': re.compile(r'(Description Rubrique|Num\s+donn\.|Val\s+init)', re.IGNORECASE),
            'code_block': re.compile(r'(Code|R√©f√©rence|Type de contrat):\s*\d+', re.IGNORECASE),
            'list_item': re.compile(r'^\s*[-‚Ä¢*]\s+|\d+\.\s+', re.MULTILINE),
        }
    
    def extract_text_from_file(self, uploaded_file) -> Optional[Dict]:
        """Extract text with metadata from uploaded files"""
        filename = uploaded_file.name.lower()
        
        try:
            if filename.endswith('.pdf'):
                return self._extract_from_pdf(uploaded_file)
            elif filename.endswith('.txt'):
                text = uploaded_file.read().decode('utf-8')
                return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
            elif filename.endswith('.docx'):
                return self._extract_from_docx(uploaded_file)
            elif filename.endswith(('.csv', '.xlsx')):
                return self._extract_from_spreadsheet(uploaded_file, filename)
            else:
                return None
        except Exception as e:
            st.error(f"Error extracting from {uploaded_file.name}: {str(e)}")
            return None
    
    def _extract_from_pdf(self, uploaded_file) -> Dict:
        """Extract text from PDF with page-level granularity"""
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        pages = []
        
        for page_num, page in enumerate(doc, start=1):
            text = page.get_text()
            pages.append({
                'page_num': page_num,
                'text': text
            })
        
        full_text = "\n".join([p['text'] for p in pages])
        return {'text': full_text, 'pages': pages}
    
    def _extract_from_docx(self, uploaded_file) -> Dict:
        """Extract text from DOCX"""
        doc = Document(uploaded_file)
        text = "\n".join([p.text for p in doc.paragraphs])
        return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
    
    def _extract_from_spreadsheet(self, uploaded_file, filename: str) -> Dict:
        """Extract text from CSV/Excel"""
        if filename.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)
        
        text = df.to_string(index=False)
        return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
    
    def clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove excessive whitespace
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        
        # Normalize line breaks
        text = text.replace('\r\n', '\n')
        
        # Remove page headers/footers (common patterns)
        text = re.sub(r'Page \d+( of \d+)?', '', text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def smart_chunk_document(self, doc_data: Dict, source_file: str) -> List[Dict]:
        """
        Intelligent chunking that preserves document structure
        Optimized for banking/technical documents
        """
        chunks = []
        pages = doc_data.get('pages', [])
        
        for page_info in pages:
            page_num = page_info['page_num']
            text = self.clean_text(page_info['text'])
            
            # Detect document structure
            if self._is_table_content(text):
                page_chunks = self._chunk_table_content(text, source_file, page_num)
            elif self._has_clear_sections(text):
                page_chunks = self._chunk_by_sections(text, source_file, page_num)
            else:
                page_chunks = self._chunk_semantic(text, source_file, page_num)
            
            chunks.extend(page_chunks)
        
        return chunks
    
    def _is_table_content(self, text: str) -> bool:
        """Detect if content is primarily tabular"""
        return bool(self.chunk_patterns['table_marker'].search(text))
    
    def _has_clear_sections(self, text: str) -> bool:
        """Detect if content has clear section structure"""
        headers = self.chunk_patterns['section_header'].findall(text)
        return len(headers) >= 2
    
    def _chunk_table_content(self, text: str, source: str, page: int) -> List[Dict]:
        """Chunk table content preserving structure"""
        chunks = []
        lines = text.split('\n')
        current_chunk = []
        current_size = 0
        
        for line in lines:
            line_size = len(line)
            
            if current_size + line_size > Config.CHUNK_SIZES['table']:
                if current_chunk:
                    chunks.append(self._create_chunk(
                        '\n'.join(current_chunk), source, page, 'table'
                    ))
                    # Overlap: keep last few lines
                    overlap_lines = current_chunk[-3:] if len(current_chunk) > 3 else current_chunk
                    current_chunk = overlap_lines
                    current_size = sum(len(l) for l in current_chunk)
            
            current_chunk.append(line)
            current_size += line_size
        
        if current_chunk:
            chunks.append(self._create_chunk(
                '\n'.join(current_chunk), source, page, 'table'
            ))
        
        return chunks
    
    def _chunk_by_sections(self, text: str, source: str, page: int) -> List[Dict]:
        """Chunk by document sections"""
        chunks = []
        sections = re.split(r'(\n(?:Sect\.|Section|Chapitre|Article)\s+[\dIVX]+)', text, flags=re.IGNORECASE)
        
        current_section = ""
        
        for i, section in enumerate(sections):
            if i % 2 == 0:  # Content
                current_section += section
            else:  # Header
                if current_section.strip():
                    chunks.append(self._create_chunk(
                        current_section.strip(), source, page, 'section'
                    ))
                current_section = section
        
        if current_section.strip():
            chunks.append(self._create_chunk(
                current_section.strip(), source, page, 'section'
            ))
        
        return chunks
    
    def _chunk_semantic(self, text: str, source: str, page: int) -> List[Dict]:
        """Semantic chunking with overlap for regular content"""
        chunks = []
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            if current_size + sentence_size > Config.CHUNK_SIZES['paragraph']:
                if current_chunk:
                    chunks.append(self._create_chunk(
                        ' '.join(current_chunk), source, page, 'paragraph'
                    ))
                    # Overlap: keep last 2 sentences
                    overlap = current_chunk[-2:] if len(current_chunk) > 2 else current_chunk
                    current_chunk = overlap
                    current_size = sum(len(s) for s in current_chunk)
            
            current_chunk.append(sentence)
            current_size += sentence_size
        
        if current_chunk:
            chunks.append(self._create_chunk(
                ' '.join(current_chunk), source, page, 'paragraph'
            ))
        
        return chunks
    
    def _create_chunk(self, text: str, source: str, page: int, chunk_type: str) -> Dict:
        """Create a standardized chunk object"""
        chunk_id = hashlib.md5(f"{source}_{page}_{text[:50]}".encode()).hexdigest()
        
        return {
            'id': chunk_id,
            'text': text,
            'metadata': {
                'source_file': source,
                'page': page,
                'chunk_type': chunk_type,
                'char_count': len(text),
                'timestamp': str(datetime.now())
            }
        }

# ==============================================================================
# BM25 IMPLEMENTATION WITH PERSISTENT INDEX
# ==============================================================================

class PersistentBM25:
    """BM25 implementation with disk persistence for large-scale retrieval"""
    
    def __init__(self, k1: float = Config.BM25_K1, b: float = Config.BM25_B):
        self.k1 = k1
        self.b = b
        self.corpus_size = 0
        self.avgdl = 0
        self.doc_freqs = defaultdict(int)
        self.idf = {}
        self.doc_len = []
        self.doc_ids = []
        self.tokenized_corpus = []
        
    def tokenize(self, text: str) -> List[str]:
        """Tokenize with banking-specific preservation"""
        # Preserve codes and references
        text = re.sub(r'([A-Z]{2,}\d+)', r' \1 ', text)
        
        # Remove punctuation but keep hyphens in codes
        text = re.sub(r'[^\w\s-]', ' ', text.lower())
        
        # Tokenize
        tokens = text.split()
        
        # Filter stopwords (French banking context)
        stopwords = {'le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'ou', 'dans', 'pour', 'sur', 'avec'}
        tokens = [t for t in tokens if t and t not in stopwords]
        
        return tokens
    
    def fit(self, documents: List[Dict]):
        """Build BM25 index from documents"""
        self.corpus_size = len(documents)
        self.doc_ids = [doc['id'] for doc in documents]
        self.tokenized_corpus = []
        
        # Tokenize all documents
        for doc in documents:
            tokens = self.tokenize(doc['text'])
            self.tokenized_corpus.append(tokens)
            self.doc_len.append(len(tokens))
            
            # Count document frequencies
            unique_tokens = set(tokens)
            for token in unique_tokens:
                self.doc_freqs[token] += 1
        
        # Calculate average document length
        self.avgdl = sum(self.doc_len) / self.corpus_size if self.corpus_size > 0 else 0
        
        # Calculate IDF values
        for token, freq in self.doc_freqs.items():
            self.idf[token] = np.log((self.corpus_size - freq + 0.5) / (freq + 0.5) + 1)
    
    def get_scores(self, query: str) -> Dict[str, float]:
        """Calculate BM25 scores for a query"""
        query_tokens = self.tokenize(query)
        scores = {}
        
        for idx, (doc_id, doc_tokens, doc_length) in enumerate(
            zip(self.doc_ids, self.tokenized_corpus, self.doc_len)
        ):
            score = 0
            for token in query_tokens:
                if token not in self.idf:
                    continue
                
                # Calculate term frequency in document
                tf = doc_tokens.count(token)
                
                # BM25 formula
                numerator = tf * (self.k1 + 1)
                denominator = tf + self.k1 * (1 - self.b + self.b * (doc_length / self.avgdl))
                score += self.idf[token] * (numerator / denominator)
            
            scores[doc_id] = score
        
        return scores
    
    def save(self, filepath: str):
        """Save BM25 index to disk"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'k1': self.k1,
                'b': self.b,
                'corpus_size': self.corpus_size,
                'avgdl': self.avgdl,
                'doc_freqs': dict(self.doc_freqs),
                'idf': self.idf,
                'doc_len': self.doc_len,
                'doc_ids': self.doc_ids,
                'tokenized_corpus': self.tokenized_corpus
            }, f)
    
    @classmethod
    def load(cls, filepath: str) -> 'PersistentBM25':
        """Load BM25 index from disk"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        
        bm25 = cls(k1=data['k1'], b=data['b'])
        bm25.corpus_size = data['corpus_size']
        bm25.avgdl = data['avgdl']
        bm25.doc_freqs = defaultdict(int, data['doc_freqs'])
        bm25.idf = data['idf']
        bm25.doc_len = data['doc_len']
        bm25.doc_ids = data['doc_ids']
        bm25.tokenized_corpus = data['tokenized_corpus']
        
        return bm25

# ==============================================================================
# QUERY EXPANSION FOR BANKING TERMINOLOGY
# ==============================================================================

class BankingQueryExpander:
    """Query expansion specialized for banking/financial terminology"""
    
    def __init__(self):
        # Banking-specific synonym mappings
        self.synonyms = {
            'compte': ['account', 'compte support', 'compte client'],
            'contrat': ['contract', 'agreement', 'convention'],
            'd√©p√¥t': ['deposit', 'versement', 'apport'],
            'garantie': ['guarantee', 'caution', 'warranty'],
            '√©ch√©ance': ['maturity', 'deadline', 'due date', 'expiration'],
            'nantissement': ['pledge', 'collateral', 'guarantee'],
            'saisie': ['entry', 'input', 'capture'],
            'grille': ['grid', 'screen', 'form'],
            'transaction': ['operation', 'mouvement', 'transfer'],
            'client': ['customer', 'account holder', 'beneficiary'],
            'b√©n√©ficiaire': ['beneficiary', 'recipient', 'payee'],
            'taux': ['rate', 'interest rate', 'percentage'],
            'plafond': ['ceiling', 'limit', 'cap', 'maximum'],
            'retrait': ['withdrawal', 'extraction'],
            'virement': ['transfer', 'wire', 'payment'],
        }
        
        # Common banking abbreviations
        self.abbreviations = {
            'DAT': 'D√©p√¥t √† Terme',
            'CNT': 'Contrat',
            'GDI': 'Gestion De Interface',
            'BDC': 'Bon De Caisse',
            'SGE': 'Code si√®ge',
            'DEV': 'Code devise',
        }
    
    def expand_query(self, query: str) -> List[str]:
        """Expand query with synonyms and banking terms"""
        expanded_queries = [query]
        
        query_lower = query.lower()
        
        # Add synonym expansions
        for term, synonyms in self.synonyms.items():
            if term in query_lower:
                for synonym in synonyms:
                    expanded = query_lower.replace(term, synonym)
                    if expanded != query_lower:
                        expanded_queries.append(expanded)
        
        # Expand abbreviations
        for abbr, full_form in self.abbreviations.items():
            if abbr in query:
                expanded = query.replace(abbr, full_form)
                expanded_queries.append(expanded)
            elif full_form.lower() in query_lower:
                expanded = query_lower.replace(full_form.lower(), abbr)
                expanded_queries.append(expanded)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_queries = []
        for q in expanded_queries:
            if q not in seen:
                seen.add(q)
                unique_queries.append(q)
        
        return unique_queries[:5]  # Limit to top 5 expansions

# ==============================================================================
# HYBRID RETRIEVAL SYSTEM
# ==============================================================================

class HybridRetriever:
    """Hybrid retrieval combining semantic search (ChromaDB) and keyword search (BM25)"""
    
    def __init__(self, chroma_collection, bm25_index: PersistentBM25, alpha: float = Config.HYBRID_ALPHA):
        self.chroma_collection = chroma_collection
        self.bm25_index = bm25_index
        self.alpha = alpha  # Weight between semantic (0) and keyword (1)
        self.query_expander = BankingQueryExpander()
    
    def retrieve(self, query: str, k: int = Config.INITIAL_RETRIEVAL_K) -> List[Dict]:
        """
        Hybrid retrieval with query expansion
        Returns top k documents based on weighted combination of semantic + keyword scores
        """
        # Expand query for better recall
        expanded_queries = self.query_expander.expand_query(query)
        
        all_results = {}
        
        for exp_query in expanded_queries:
            # 1. Semantic search via ChromaDB
            semantic_results = self.chroma_collection.query(
                query_texts=[exp_query],
                n_results=k,
                include=["documents", "metadatas", "distances"]
            )
            
            # Normalize semantic scores (distances -> similarities)
            semantic_scores = {}
            if semantic_results['ids'] and semantic_results['ids'][0]:
                max_distance = max(semantic_results['distances'][0]) if semantic_results['distances'][0] else 1
                for idx, doc_id in enumerate(semantic_results['ids'][0]):
                    distance = semantic_results['distances'][0][idx]
                    similarity = 1 - (distance / max_distance) if max_distance > 0 else 1
                    semantic_scores[doc_id] = similarity
            
            # 2. Keyword search via BM25
            bm25_scores = self.bm25_index.get_scores(exp_query)
            
            # Normalize BM25 scores
            max_bm25 = max(bm25_scores.values()) if bm25_scores else 1
            normalized_bm25 = {k: v / max_bm25 for k, v in bm25_scores.items()} if max_bm25 > 0 else bm25_scores
            
            # 3. Combine scores with weighted sum
            all_doc_ids = set(semantic_scores.keys()) | set(normalized_bm25.keys())
            
            for doc_id in all_doc_ids:
                sem_score = semantic_scores.get(doc_id, 0)
                bm25_score = normalized_bm25.get(doc_id, 0)
                
                # Hybrid score
                hybrid_score = (1 - self.alpha) * sem_score + self.alpha * bm25_score
                
                # Accumulate scores across expanded queries
                if doc_id in all_results:
                    all_results[doc_id]['score'] = max(all_results[doc_id]['score'], hybrid_score)
                else:
                    # Get document metadata
                    try:
                        doc_data = self.chroma_collection.get(
                            ids=[doc_id],
                            include=["documents", "metadatas"]
                        )
                        if doc_data['ids']:
                            all_results[doc_id] = {
                                'id': doc_id,
                                'text': doc_data['documents'][0],
                                'metadata': doc_data['metadatas'][0],
                                'score': hybrid_score,
                                'semantic_score': sem_score,
                                'bm25_score': bm25_score
                            }
                    except:
                        pass
        
        # Sort by score and return top k
        ranked_results = sorted(all_results.values(), key=lambda x: x['score'], reverse=True)
        return ranked_results[:k]

# ==============================================================================
# BATCH DOCUMENT INGESTION
# ==============================================================================

class BatchDocumentIngestion:
    """Optimized batch processing for large document collections"""
    
    def __init__(self, chroma_collection, processor: BankingDocumentProcessor):
        self.collection = chroma_collection
        self.processor = processor
        self.bm25_documents = []
    
    def ingest_documents(self, uploaded_files: List, progress_callback=None) -> Dict:
        """
        Batch ingest documents with progress tracking
        Returns statistics about the ingestion process
        """
        stats = {
            'total_files': len(uploaded_files),
            'processed_files': 0,
            'total_chunks': 0,
            'failed_files': [],
            'processing_time': 0
        }
        
        start_time = datetime.now()
        
        # Get existing documents to avoid duplicates
        existing_sources = self._get_existing_sources()
        
        batch_chunks = []
        
        for file_idx, uploaded_file in enumerate(uploaded_files):
            try:
                filename = uploaded_file.name
                
                # Skip if already indexed
                if filename in existing_sources:
                    if progress_callback:
                        progress_callback(file_idx + 1, len(uploaded_files), f"Skipped (already indexed): {filename}")
                    continue
                
                if progress_callback:
                    progress_callback(file_idx + 1, len(uploaded_files), f"Processing: {filename}")
                
                # Extract and chunk document
                doc_data = self.processor.extract_text_from_file(uploaded_file)
                
                if doc_data is None:
                    stats['failed_files'].append(filename)
                    continue
                
                chunks = self.processor.smart_chunk_document(doc_data, filename)
                
                # Add to batch
                batch_chunks.extend(chunks)
                self.bm25_documents.extend(chunks)
                
                # Process batch if it reaches batch size
                if len(batch_chunks) >= Config.BATCH_SIZE:
                    self._process_batch(batch_chunks)
                    stats['total_chunks'] += len(batch_chunks)
                    batch_chunks = []
                
                stats['processed_files'] += 1
                
            except Exception as e:
                stats['failed_files'].append(f"{uploaded_file.name}: {str(e)}")
        
        # Process remaining chunks
        if batch_chunks:
            self._process_batch(batch_chunks)
            stats['total_chunks'] += len(batch_chunks)
        
        stats['processing_time'] = (datetime.now() - start_time).total_seconds()
        
        return stats
    
    def _process_batch(self, chunks: List[Dict]):
        """Process a batch of chunks into ChromaDB"""
        if not chunks:
            return
        
        ids = [chunk['id'] for chunk in chunks]
        documents = [chunk['text'] for chunk in chunks]
        metadatas = [chunk['metadata'] for chunk in chunks]
        
        self.collection.add(
            ids=ids,
            documents=documents,
            metadatas=metadatas
        )
    
    def _get_existing_sources(self) -> set:
        """Get list of already indexed source files"""
        try:
            all_data = self.collection.get(include=["metadatas"])
            if all_data and all_data['metadatas']:
                return set(meta.get('source_file', '') for meta in all_data['metadatas'])
        except:
            pass
        return set()
    
    def build_bm25_index(self) -> PersistentBM25:
        """Build and save BM25 index from ingested documents"""
        bm25 = PersistentBM25()
        bm25.fit(self.bm25_documents)
        bm25.save(Config.BM25_INDEX_PATH)
        return bm25

# ==============================================================================
# RAG GENERATION WITH IMPROVED CONTEXT
# ==============================================================================

class BankingRAGGenerator:
    """Enhanced RAG generation for banking documentation"""
    
    def __init__(self, azure_client):
        self.client = azure_client
    
    def generate_response(self, query: str, context_docs: List[Dict]) -> Dict:
        """Generate response with improved context assembly"""
        
        # Assemble context with source citations
        context_parts = []
        sources = []
        
        for idx, doc in enumerate(context_docs, 1):
            metadata = doc.get('metadata', {})
            source_file = metadata.get('source_file', 'Unknown')
            page = metadata.get('page', 'N/A')
            
            context_parts.append(
                f"[Document {idx} - {source_file}, Page {page}]\n{doc['text']}\n"
            )
            
            if source_file not in sources:
                sources.append(source_file)
        
        context = "\n".join(context_parts)
        
        # Enhanced prompt for banking documentation
        prompt = f"""Vous √™tes un assistant RAG sp√©cialis√© dans les syst√®mes bancaires, l'architecture de donn√©es, et les produits financiers.

Votre r√¥le est de r√©pondre et d'expliquer clairement les informations issues des documents, en les rendant compr√©hensibles pour tout type d'utilisateur, du plus g√©n√©ral au plus technique.

[Contexte]
{context}

[Requ√™te]
{query}

[INSTRUCTIONS G√âN√âRALES]

1. Utilisez exclusivement les informations pr√©sentes dans le contexte ci-dessus.

2. Fournissez une r√©ponse claire, structur√©e et explicite, adapt√©e √† la compr√©hension de tout utilisateur.
   - Si le sujet est technique, expliquez les notions en termes simples.
   - Si la question est g√©n√©rale, donnez une r√©ponse compl√®te mais concise.
   - Si la question est complexe, d√©taillez le raisonnement et le fonctionnement.

3. Chaque fois que vous mentionnez une donn√©e, un m√©canisme ou un fait:
   - Citez imm√©diatement la source sous le format: [Source: nom_du_fichier.pdf, Page <num√©ro>]

4. Si l'information n'existe pas dans les documents, indiquez-le clairement.

5. Structure attendue:
   a. R√©ponse expliqu√©e: D√©taillez le contenu et son interpr√©tation.
   b. Synth√®se (si utile): R√©sumez la logique ou le fonctionnement global.
   c. Sources: Liste compl√®te des documents utilis√©s.

6. Objectif:
   Rendre la r√©ponse √† la fois informative, explicative et v√©rifiable,
   qu'il s'agisse d'un utilisateur curieux ou d'un expert technique.

[Exemple de format]
Les proc√©dures de v√©rification d'identit√©... [Source: procedure_kyc.pdf, Page 12]. 
Selon le chapitre 3... [Source: reglement_financier.pdf, Page 45].

Sources utilis√©es:
- procedure_kyc.pdf (Pages 12, 15)
- reglement_financier.pdf (Page 45)
"""
        
        try:
            completion = self.client.chat.completions.create(
                model=Config.AZURE_MODEL_DEPLOYMENT,
                messages=[
                    {"role": "system", "content": "You are an expert assistant for banking system documentation based on RAG (Retrieval-Augmented Generation)."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            answer = completion.choices[0].message.content
            
            return {
                'answer': answer,
                'sources': sources,
                'context_docs': context_docs,
                'num_tokens': completion.usage.total_tokens if hasattr(completion, 'usage') else None
            }
            
        except Exception as e:
            return {
                'answer': f"Error generating response: {str(e)}",
                'sources': [],
                'context_docs': [],
                'num_tokens': None
            }

# ==============================================================================
# STREAMLIT APPLICATION
# ==============================================================================

def initialize_session_state():
    """Initialize Streamlit session state"""
    if 'chroma_client' not in st.session_state:
        st.session_state.chroma_client = chromadb.PersistentClient(path=Config.CHROMA_PERSIST_PATH)
    
    if 'collection' not in st.session_state:
        # Load embedding model
        embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL_PATH)
        
        # Create embedding function
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=Config.EMBEDDING_MODEL_PATH
        )
        
        st.session_state.collection = st.session_state.chroma_client.get_or_create_collection(
            name=Config.COLLECTION_NAME,
            embedding_function=embedding_function,
            metadata={"hnsw:space": "cosine"}
        )
    
    if 'bm25_index' not in st.session_state:
        # Load or create BM25 index
        if os.path.exists(Config.BM25_INDEX_PATH):
            st.session_state.bm25_index = PersistentBM25.load(Config.BM25_INDEX_PATH)
        else:
            st.session_state.bm25_index = PersistentBM25()
    
    if 'chat_sessions' not in st.session_state:
        st.session_state.chat_sessions = {}
    
    if 'current_chat' not in st.session_state:
        st.session_state.current_chat = str(datetime.now())
        st.session_state.chat_sessions[st.session_state.current_chat] = {
            "title": "New Chat",
            "messages": []
        }
    
    if 'processor' not in st.session_state:
        st.session_state.processor = BankingDocumentProcessor()
    
    if 'azure_client' not in st.session_state:
        # Initialize Azure OpenAI client
        http_client = httpx.Client(verify=False)
        st.session_state.azure_client = AzureOpenAI(
            api_version=Config.AZURE_API_VERSION,
            azure_endpoint=Config.AZURE_ENDPOINT,
            api_key=Config.AZURE_API_KEY,
            http_client=http_client
        )
    
    if 'rag_generator' not in st.session_state:
        st.session_state.rag_generator = BankingRAGGenerator(st.session_state.azure_client)

def main():
    st.set_page_config(page_title="BNP Banking RAG Chatbot", layout="wide")
    
    initialize_session_state()
    
    st.title("üè¶ BNP Banking Documentation Chatbot")
    st.caption("Optimized RAG system with hybrid search and batch processing")
    
    # Sidebar for document management
    with st.sidebar:
        st.header("üìÅ Document Management")
        
        # Get collection stats
        try:
            collection_count = st.session_state.collection.count()
            st.metric("Indexed Documents", collection_count)
        except:
            collection_count = 0
            st.metric("Indexed Documents", "0")
        
        # Batch upload
        st.subheader("Upload Documents")
        uploaded_files = st.file_uploader(
            "Upload multiple documents",
            type=["pdf", "txt", "docx", "csv", "xlsx"],
            accept_multiple_files=True
        )
        
        if uploaded_files and st.button("üì§ Process All Files"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(current, total, message):
                progress = current / total
                progress_bar.progress(progress)
                status_text.text(message)
            
            with st.spinner("Processing documents..."):
                batch_ingestion = BatchDocumentIngestion(
                    st.session_state.collection,
                    st.session_state.processor
                )
                
                stats = batch_ingestion.ingest_documents(
                    uploaded_files,
                    progress_callback=update_progress
                )
                
                # Rebuild BM25 index
                status_text.text("Building BM25 index...")
                st.session_state.bm25_index = batch_ingestion.build_bm25_index()
                
                # Display results
                st.success(f"""
                ‚úÖ Processing complete!
                - Files processed: {stats['processed_files']}/{stats['total_files']}
                - Total chunks: {stats['total_chunks']}
                - Time: {stats['processing_time']:.2f}s
                """)
                
                if stats['failed_files']:
                    st.error(f"Failed files: {', '.join(stats['failed_files'])}")
        
        st.divider()
        
        # Chat management
        st.subheader("üí¨ Chat Sessions")
        if st.button("‚ûï New Chat"):
            new_chat_id = str(datetime.now())
            st.session_state.current_chat = new_chat_id
            st.session_state.chat_sessions[new_chat_id] = {
                "title": "New Chat",
                "messages": []
            }
            st.rerun()
        
        # List existing chats
        for chat_id, chat_data in list(st.session_state.chat_sessions.items()):
            if st.button(f"üìù {chat_data['title'][:30]}", key=f"chat_{chat_id}"):
                st.session_state.current_chat = chat_id
                st.rerun()
    
    # Main chat interface
    current_chat = st.session_state.chat_sessions.get(st.session_state.current_chat, {
        "title": "New Chat",
        "messages": []
    })
    
    # Display chat messages
    for message in current_chat['messages']:
        with st.chat_message(message['role']):
            st.markdown(message['content'])
            
            # Show sources if available
            if message['role'] == 'assistant' and 'sources' in message:
                with st.expander("üìö Sources"):
                    for source in message['sources']:
                        st.text(f"‚Ä¢ {source}")
    
    # Chat input
    if prompt := st.chat_input("Ask about banking documentation..."):
        # Add user message
        current_chat['messages'].append({
            "role": "user",
            "content": prompt,
            "timestamp": str(datetime.now())
        })
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("üîç Searching documentation..."):
                # Create hybrid retriever
                retriever = HybridRetriever(
                    st.session_state.collection,
                    st.session_state.bm25_index
                )
                
                # Retrieve relevant documents
                retrieved_docs = retriever.retrieve(prompt, k=Config.FINAL_RESULTS_K)
                
                # Generate response
                response = st.session_state.rag_generator.generate_response(
                    prompt,
                    retrieved_docs
                )
                
                # Display answer
                st.markdown(response['answer'])
                
                # Show sources
                if response['sources']:
                    with st.expander("üìö Sources"):
                        for source in response['sources']:
                            st.text(f"‚Ä¢ {source}")
                
                # Add to chat history
                current_chat['messages'].append({
                    "role": "assistant",
                    "content": response['answer'],
                    "sources": response['sources'],
                    "timestamp": str(datetime.now())
                })
                
                # Update chat title if first message
                if current_chat['title'] == "New Chat" and len(current_chat['messages']) > 0:
                    current_chat['title'] = prompt[:50] + "..."

if __name__ == "__main__":
    main()







Nouvelle slide ‚Äì Public cible
√Ä qui s‚Äôadresse la solution RAG BNP ED ?
RAG BNP ED a √©t√© d√©velopp√©e pour accompagner diff√©rents profils impliqu√©s dans l‚Äôexploitation et la transformation du syst√®me d‚Äôinformation :
üë• √âquipes m√©tiers
Acc√®s rapide et fiable √† la documentation ATLAS
Recherche simplifi√©e sans connaissance technique
R√©ponses claires, sourc√©es et contextualis√©es
üèóÔ∏è √âquipes IT / DSI
Compr√©hension de l‚Äôarchitecture de la documentation ATLAS
Acc√®s structur√© aux r√©f√©rentiels et aux d√©pendances documentaires
Support √† l‚Äôanalyse et √† la maintenance du SI
üîÑ √âquipes Transformation du Syst√®me d‚ÄôInformation
Accompagnement des projets de migration et d‚Äô√©volution du SI
Vision transverse de la documentation existante
R√©duction du temps de recherche et d‚Äôanalyse documentaire
üí° Variante plus synth√©tique (si tu veux une slide tr√®s l√©g√®re)
RAG BNP ED s‚Äôadresse aux √©quipes m√©tiers, √† la DSI et aux acteurs de la transformation du syst√®me d‚Äôinformation, en facilitant l‚Äôacc√®s, la compr√©hension et l‚Äôexploitation de la documentation ATLAS.














RAG BNP ED est un chatbot intelligent bas√© sur la m√©thode RAG (Retrieval Augmented Generation) et sur une base de connaissances centralis√©e contenant la documentation ATLAS.
Il permet une recherche conversationnelle rapide et fiable, avec des r√©ponses syst√©matiquement sourc√©es √† partir des documents officiels.
La solution √©volue automatiquement avec les nouvelles documentations, garantissant des r√©ponses toujours √† jour.






RAG BNP ED est un chatbot intelligent bas√© sur une base de connaissances centralis√©e (documentation ATLAS).
Il permet une recherche conversationnelle rapide et fiable, avec des r√©ponses syst√©matiquement sourc√©es.
La solution √©volue automatiquement avec les nouvelles documentations, garantissant des r√©ponses toujours √† jour.








La documentation ATLAS est volumineuse et critique pour les √©quipes m√©tiers
L‚Äôacc√®s √† l‚Äôinformation repose aujourd‚Äôhui sur une recherche manuelle chronophage
Objectif : faciliter l‚Äôacc√®s rapide, fiable et sourc√© √† l‚Äôinformation interne via un outil intelligent




Bonjour,

Pourriez-vous s‚Äôil vous pla√Æt proc√©der √† la r√©initialisation de mon mot de passe ? Je vous remercie par avance.

Cordialement,
Nihad Senhadji


Bonjour √† tous,

Suite √† notre pr√©c√©dent √©change, je vous informe que nous avons pu identifier et renseigner les dates d‚Äôimpay√©s en nous basant sur l‚Äôextraction transmise par Chaima.

Toutefois, 11 cas restent pour lesquels les dates d‚Äôimpay√©s n‚Äôont pas pu √™tre renseign√©es, malgr√© l‚Äôutilisation de cette extraction. Ces cas demeurent donc identifi√©s comme anomalies.

Je reste √† votre disposition pour toute pr√©cision compl√©mentaire ou pour √©changer sur ces cas si besoin.

Cordialement,
Nihad Senhadji
GA-15








Bonjour √† tous,

Veuillez trouver ci-joint la nouvelle version du fichier √âtat Central des Engagements arr√™t√© au 31/12/2025.

Lors du traitement, nous avons constat√© que certaines dates d‚Äôimpay√©s n‚Äôont pas √©t√© retrouv√©es dans les donn√©es d‚Äôextraction disponibles.
Pour cette raison, une nouvelle feuille a √©t√© ajout√©e au fichier, intitul√©e :

¬´ √âtat des engagements ‚Äì Anomalies dates d‚Äôimpay√©s ¬ª

Cette feuille regroupe l‚Äôensemble des engagements pour lesquels les dates d‚Äôimpay√©s n‚Äôont pas pu √™tre identifi√©es √† partir de l‚Äôextraction.

Le fichier contient d√©sormais les feuilles suivantes :

1. √âtat Central des Engagements
2. √âtat des engagements ‚Äì Anomalies dates d‚Äôimpay√©s
3. √âtat d√©taill√© par chapitre
4. Rapprochement des cr√©ances ‚Äì Bilan
5. Rapprochement des cr√©ances hors bilan
6. Rapprochement CR / AR / CB
7. Reporting social BG
8. √âtat d√©taill√© par ratios prudentiels
9. Risques ‚Äì Rapprochement RP
10. Risques ‚Äì Grands Risques
11. Risques ‚Äì Grands Risques par entit√©
12. Risques ‚Äì Annexe 9000
13. Rapprochement reporting provisions
14. Rapprochement affectation des garanties
15. √âtat des garanties

Je reste √† votre disposition pour toute information compl√©mentaire ou clarification si besoin.

Cordialement,
Nihad Senhadji
GA-15







Bien s√ªr ! Voici une version r√©√©crite, fluide, professionnelle et claire de ton email :


---

Objet : M√©thodologie ‚Äì D√©tection de communaut√©s de fraude (AI Sentinilia)

Bonjour Ryma,

Dans le cadre du projet AI Sentinilia d√©di√© √† la d√©tection de communaut√©s de fraude, la m√©thodologie utilis√©e repose sur les crit√®res suivants :

Les liens entre clients sont identifi√©s √† partir de co√Øncidences lorsqu‚Äôils effectuent des retraits :

√† la m√™me date ;

au m√™me GAB (ATM) ;

pour un montant sup√©rieur ou √©gal √† 500 euros ;

avec une fr√©quence d‚Äôau moins 5 retraits entre deux clients ou plus.



Lorsque ces conditions sont remplies, les clients sont consid√©r√©s comme li√©s, car leurs comportements de retrait pr√©sentent des similitudes r√©p√©t√©es et inhabituelles.

Afin de d√©tecter les communaut√©s de fraude, nous utilisons un algorithme appel√© Louvain, qui permet de regrouper automatiquement les clients ayant des comportements similaires. Chaque communaut√© ainsi identifi√©e repr√©sente un groupe potentiel de fraude, bas√© sur des comportements de retraits coordonn√©s.

Cette solution n‚Äôa pas √©t√© modifi√©e depuis sa mise en place, et le suivi √©tant trimestriel, nous enverrons les prochains r√©sultats en f√©vrier 2026.

Je reste √† votre disposition pour toute information compl√©mentaire ou clarification si n√©cessaire.

Cordialement,
Nihad Senhadji


---

Si tu veux, je peux aussi te faire une version encore plus courte et simple, parfaite pour des non-techniques, qui tient en 5-6 phrases.
Veux‚Äëtu que je fasse √ßa‚ÄØ?











Comme le suivi est trimestriel, nous enverrons les prochains r√©sultats en f√©vrier 2025.


Cet algorithme n‚Äôa pas √©t√© modifi√© depuis sa mise en place


Afin de d√©tecter les communaut√©s de fraude, nous utilisons un algorithme appel√© Louvain, qui permet de regrouper automatiquement les clients ayant des comportements similaires.


Bonjour,

Dans le cadre du projet AI Sentinilia d√©di√© √† la d√©tection de communaut√©s de fraude, la m√©thodologie utilis√©e repose sur les √©tapes suivantes :

- Les liens entre clients sont d‚Äôabord identifi√©s √† partir des crit√®res suivants :

   - la localisation du GAB (ATM) o√π les retraits ont √©t√© effectu√©s ;
   - la fr√©quence des retraits ;
   - des co√Øncidences entre plusieurs clients lorsque les retraits ont eu lieu :
      - √† la m√™me date,
      - au m√™me GAB,
      - avec un montant sup√©rieur ou √©gal √† 500 euros,
      - et ce au moins 5 fois entre deux clients ou plus.

- Lorsque ces conditions sont r√©unies, les clients sont consid√©r√©s comme li√©s, car leurs comportements de retrait pr√©sentent des similitudes r√©p√©t√©es et inhabituelles.

- La m√©thode de Louvain est ensuite utilis√©e pour regrouper automatiquement les clients li√©s :

   - elle permet de constituer des communaut√©s de clients fortement connect√©s ;
   - chaque communaut√© repr√©sente un groupe potentiel de fraude, bas√© sur des comportements de retraits coordonn√©s.

Je reste √† votre disposition pour toute information compl√©mentaire ou clarification si n√©cessaire.

Cordialement,

Nihad Senhadji



















Bonjour,

Je vous informe que j‚Äôai annul√© la version du fichier d√©j√† envoy√©e pr√©c√©demment.
Nous sommes actuellement en train de finaliser la nouvelle version, et je vous l‚Äôenverrai d√®s que ce sera termin√©.

Merci pour votre compr√©hension.

Cordialement,
Nihad Senhadji

Bonjour,

Veuillez trouver ci-joint le fichier demand√©, comme convenu.

Nous vous enverrons la version finale, contenant l‚Äôensemble des feuilles, dans la soir√©e.

Cordialement,
Nihad Senhadji


Les colonnes du fichier fournissent toutes les informations n√©cessaires ‚Äî identit√©, dates, lieux et r√©f√©rences clients ‚Äî afin de faciliter et structurer l‚Äôanalyse des √©quipes sur ces cas sensibles.


La deuxi√®me feuille, avec un score de similarit√© inf√©rieur √† 95 %, correspond aux cas o√π le mod√®le d‚ÄôIA a permis de r√©duire une grande partie de la recherche manuelle.
Dans cette feuille, tous les noms qui ne figurent pas dans notre base clients Atlas sont automatiquement √©cart√©s, ce qui √©limine les faux positifs et all√®ge consid√©rablement le travail des √©quipes op√©rationnelles.
üîπ La troisi√®me feuille, avec un score de similarit√© sup√©rieur √† 95 %, regroupe les noms n√©cessitant une analyse approfondie par l‚Äô√©quipe op√©rationnelle, car ils pr√©sentent une forte probabilit√© de correspondance.










üé¨ Script vid√©o ‚Äì Pr√©sentation de Decad AI Solution

> Bonjour,

Dans cette vid√©o, je vais vous pr√©senter Decad AI Solution, accessible via un lien de web application Domino.

On commence par l‚Äôinsertion du fichier DECAD, envoy√© par la Banque d‚ÄôAlg√©rie, directement dans l‚Äôapplication.

Ensuite, un simple clic permet de lancer le traitement. La solution s‚Äôappuie sur un mod√®le d‚Äôintelligence artificielle ex√©cut√© en arri√®re-plan, avec un temps d‚Äôex√©cution d‚Äôenviron 10 minutes.

Une fois le traitement termin√©, on clique sur ‚ÄúR√©sultats‚Äù afin de t√©l√©charger le fichier Excel de sortie, comme vous pouvez le voir √† l‚Äô√©cran.

Le fichier Excel g√©n√©r√© contient trois feuilles distinctes :

üîπ La premi√®re feuille, appel√©e Pi√®ce d‚Äôaudit, regroupe les r√©sultats avec des scores de similarit√© inf√©rieurs et sup√©rieurs √† 95 %.

üîπ La deuxi√®me feuille contient les r√©sultats avec un score inf√©rieur √† 95 %, correspondant aux cas non identifi√©s par le mod√®le d‚ÄôIA.

üîπ La troisi√®me feuille regroupe les r√©sultats avec un score sup√©rieur √† 95 %, qui repr√©sentent les cas prioritaires √† analyser par l‚Äô√©quipe op√©rationnelle.

Il est important de noter que les trois feuilles contiennent exactement les m√™mes colonnes.

Voici la liste des colonnes suivantes :

COLONNE 1 et 2 DECAD

Nom interdits chequiers

Raison sociale base atlas

Nom abr√©g√© tiers

Date naissance interdits chequiers ET DE NOTRE BASE ATLAS

Date de cr√©ation (entreprise)

Date naissance Match

Date de cr√©ation Match

Lieu naissance interdits chequier ET DE LA BASE atlas

Lieu naissance

Lieu naissance Match

Date d'effet et de lev√©e

Compte client

Code cat√©gorie client

Id tiers

Score de similarit√©


Gr√¢ce √† cette structuration, Decad AI Solution permet de r√©duire le temps de traitement, de prioriser les analyses, et de s√©curiser le processus de contr√¥le gr√¢ce √† l‚Äôintelligence artificielle.

Merci pour votre attention.




---

If you want, I can also make a version more ‚Äúoral / dynamique‚Äù, so it sounds natural when you speak it in the video, like a real demo narration.

Do you want me to do that?



















Bonjour,

Merci pour votre message et pour la transmission de l‚ÄôExpression de Besoin (EDB) relative √† l‚Äôautomatisation du processus de traitement des agios r√©serv√©s et de leurs reprises li√©es aux clients douteux ‚Äì p√©rim√®tre EPS.

Suite √† l‚Äôanalyse de cette EDB et conform√©ment aux √©l√©ments partag√©s, je vous prie de trouver en pi√®ce jointe les r√©sultats de l‚ÄôAPA relatifs au mois de d√©cembre 2025.

Ces r√©sultats correspondent uniquement au traitement du mois de d√©cembre 2025 et constituent la sortie actuelle de l‚ÄôAPA √† ce stade.
Le fichier Excel de r√©sultats contient √† la fois des clients sains et des clients douteux.

Ils sont le fruit de plusieurs ateliers de travail men√©s avec Mouna, au cours desquels nous avons r√©alis√© de nombreuses v√©rifications manuelles, ainsi que des rapprochements d√©taill√©s entre les r√©sultats APA et les r√©sultats manuels du mois d‚Äôoctobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les r√®gles de gestion appliqu√©es dans le cadre de cette APA, en coh√©rence avec les besoins fonctionnels et techniques d√©crits dans l‚ÄôEDB et avec la recommandation de la Banque d‚ÄôAlg√©rie visant √† remplacer le traitement manuel actuellement r√©alis√© par l‚Äô√©quipe DRAC.

Il s‚Äôagit actuellement d‚Äôune phase de test. Il est propos√© de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilit√© du dispositif et de permettre la validation d√©finitive des r√©sultats par vos soins.

Par ailleurs, l‚ÄôAPA sera livr√©e la semaine prochaine afin d‚Äô√™tre d√©ploy√©e et ex√©cut√©e sur les postes des utilisateurs concern√©s.
√Ä cet effet, nous aurons besoin :

- de l‚Äôinstallation de l‚ÄôAPA sur les postes utilisateurs,
- ainsi que de la mise √† disposition d‚Äôun lien d‚Äôacc√®s vers l‚Äôemplacement de d√©p√¥t des fichiers de sortie.

Je reste bien entendu √† votre disposition, ainsi qu‚ÄôAmina, pour tout compl√©ment d‚Äôinformation ou si vous souhaitez planifier un point d‚Äô√©change.

Cordialement,
Nihad Senhadji

---

If you want, I can now:

- make a very formal / regulatory version (signature‚Äìengagement context), or
- shorten it for a manager-level reply.












Le fichier Excel de r√©sultats contient √† la fois des clients sains et des clients douteux


Bonjour,

Merci pour votre message et pour les √©l√©ments transmis lors de notre √©change de jeudi.

Suite √† l‚Äôanalyse de l‚Äôexpression de besoin relative √† l‚Äôautomatisation du traitement des agios r√©serv√©s leasing, je vous prie de trouver en pi√®ce jointe les r√©sultats de l‚ÄôAPA relatifs au mois de d√©cembre 2025.

Ces r√©sultats correspondent uniquement au traitement du mois de d√©cembre 2025 et constituent la sortie actuelle de l‚ÄôAPA √† ce stade.

Ils sont le fruit de plusieurs ateliers de travail men√©s avec Mouna, au cours desquels nous avons r√©alis√© de nombreuses v√©rifications manuelles, ainsi que des rapprochements d√©taill√©s entre les r√©sultats APA et les r√©sultats manuels du mois d‚Äôoctobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les r√®gles de gestion appliqu√©es dans le cadre de cette APA.

Il s‚Äôagit actuellement d‚Äôune phase de test. Il est propos√© de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilit√© du dispositif et de permettre la validation d√©finitive des r√©sultats par vos soins.

Par ailleurs, l‚ÄôAPA sera livr√©e la semaine prochaine afin d‚Äô√™tre d√©ploy√©e et ex√©cut√©e sur les postes des utilisateurs concern√©s.
√Ä cet effet, nous aurons besoin :

- de l‚Äôinstallation de l‚ÄôAPA sur les postes utilisateurs,
- ainsi que de la mise √† disposition d‚Äôun lien d‚Äôacc√®s vers l‚Äôemplacement de d√©p√¥t des fichiers de sortie.

Je reste bien entendu √† votre disposition pour tout compl√©ment d‚Äôinformation, retour ou ajustement n√©cessaire.

Cordialement,
Nihad Senhadji

---

If you want, I can also adapt the wording to:

- explicitly request






Bonjour,

Merci pour votre message et pour les √©l√©ments transmis lors de notre √©change de jeudi.

Suite √† l‚Äôanalyse de l‚Äôexpression de besoin relative √† l‚Äôautomatisation du traitement des agios r√©serv√©s leasing, je vous prie de trouver en pi√®ce jointe les r√©sultats de l‚ÄôAPA.

Ces r√©sultats correspondent uniquement au traitement du mois en cours et constituent la sortie actuelle de l‚ÄôAPA √† ce stade.

Ils sont le fruit de plusieurs ateliers de travail men√©s avec Mouna, au cours desquels nous avons r√©alis√© de nombreuses v√©rifications manuelles, ainsi que des rapprochements d√©taill√©s entre les r√©sultats APA et les r√©sultats manuels du mois d‚Äôoctobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les r√®gles de gestion appliqu√©es dans le cadre de cette APA.

Il s‚Äôagit actuellement d‚Äôune phase de test. Il est propos√© de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilit√© du dispositif et de permettre la validation d√©finitive des r√©sultats par vos soins.

Par ailleurs, l‚ÄôAPA sera livr√©e la semaine prochaine afin d‚Äô√™tre d√©ploy√©e et ex√©cut√©e sur les postes des utilisateurs concern√©s.
√Ä cet effet, nous aurons besoin :

- de l‚Äôinstallation de l‚ÄôAPA sur les postes utilisateurs,
- ainsi que de la mise √† disposition d‚Äôun lien d‚Äôacc√®s vers l‚Äôemplacement de d√©p√¥t des fichiers de sortie.

Je reste bien entendu √† votre disposition pour tout compl√©ment d‚Äôinformation, retour ou ajustement n√©cessaire.

Cordialement,
Nihad Senhadji

---

If you want, I can also:

- slightly strengthen the request for validation, or
- adapt the wording to a more regulatory / audit-compliant style for DRAC.






Bonjour [Nom du manager],

Je souhaite vous partager un point de situation concernant l‚Äôavancement des projets APA.

1. Avancement g√©n√©ral (APA ESP & APA Arr√™t√©)
Suite aux diff√©rents workshops organis√©s avec la DRAC, l‚Äôensemble des r√®gles de gestion a √©t√© confirm√© et valid√© pour les deux projets.
Un rapprochement a √©galement √©t√© r√©alis√© sur la base de leurs fichiers de novembre 2025.

2. Projet APA ESP
Un seul point reste identifi√© : un probl√®me de code fusion concernant un compte unique.
La DRAC m‚Äôa confirm√© que ce point est d√©j√† port√© √† votre connaissance.

3. Projet APA Arr√™t√©
Un seul point est actuellement en suspens :

- Un √©cart li√© au solde du compte √† vue.
√Ä ce stade, la DRAC n‚Äôest pas en mesure d‚Äôidentifier la m√©thode ou la source utilis√©e pour le calcul de ce solde et doit effectuer des v√©rifications de son c√¥t√© avant de nous apporter un retour d√©finitif.

4. Projet APA Leasing
Le projet APA Leasing n‚Äôa pas encore √©t√© d√©marr√© √† ce stade.

Vous trouverez ci-dessous le lien regroupant les √©l√©ments pour les projets APA ESP et APA Arr√™t√© :
[Lien √† ins√©rer]

Je reste √† votre disposition pour toute pr√©cision compl√©mentaire ou pour un point de suivi.

Cordialement,
Nihad Senhadji






your thanks:

"Je tiens √† vous remercier pour votre accompagnement et vos conseils tout au long de l‚Äôann√©e, et je souhaite continuer √† contribuer activement √† la r√©ussite des objectifs de la Banque, tout en participant aux projets IA et m√©tiers."

It‚Äôs professional, appreciative, and forward-looking.

I can also make a slightly shorter, sharper version if you want it for an email reply.

You can write it like this:

"Je souhaite continuer √† contribuer activement √† la r√©ussite des objectifs de la Banque, tout en participant aux projets IA et m√©tiers."

It‚Äôs concise, professional, and inclusive of both AI and other banking projects.

If you want, I can also suggest 2‚Äì3 alternative formulations that sound slightly more executive.


Understood. We need a leadership-oriented tone without implying you want to replace him‚Äîshowing responsibility, initiative, and team support while still highlighting AI. Here‚Äôs a refined version:


---

Bonjour [Nom du manager],

Merci pour votre retour et pour l‚Äô√©valuation de ma contribution en 2025. J‚Äôai bien pris en compte les axes d‚Äôam√©lioration identifi√©s, notamment sur le cadrage des besoins m√©tiers, la structuration des travaux et la clart√© de la communication.

Je souhaite exprimer mon objectif professionnel, qui est d‚Äô√©voluer vers un r√¥le sur des projets en Intelligence Artificielle, avec des responsabilit√©s clairement d√©finies et des livrables √† impact op√©rationnel.

Je reste par ailleurs pleinement engag√©e √† soutenir l‚Äôensemble des projets bancaires et √† accompagner l‚Äô√©quipe dans l‚Äôorganisation et la coordination des travaux, afin de garantir la qualit√© et l‚Äôefficacit√© des livrables, tout en contribuant activement √† la r√©ussite des projets IA et m√©tiers.

Cordialement,
Nihad Senhadji


---

This version:

Shows initiative and readiness to support the team

Positions you for AI projects

Avoids sounding like you want to replace him


If you want, I can make an even snappier, executive-style version that fits in one paragraph. Do you want me to do that?
