"""
RAG Chatbot â€“ Bank Documentation
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Stack:
  â€¢ Embedding  : Qwen3-Embedding  on A40 3G MIG slice  (CUDA:0 in its own process)
  â€¢ LLM        : Qwen2.5-72B-Instruct via vLLM         (12Ã— H100 48GB, tensor_parallel=12)
  â€¢ Vector DB  : ChromaDB  (persistent, cosine)
  â€¢ Reranker   : BM25Okapi (rank_bm25)
  â€¢ Frontend   : Streamlit
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Edit the CONFIG block below, then run:
    streamlit run rag_bank_chatbot.py
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# IMPORTS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
import re
import json
import uuid
import string
import datetime
import warnings
warnings.filterwarnings("ignore")

import fitz                          # PyMuPDF
import pandas as pd
import streamlit as st
from PIL import Image

import chromadb
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
import docx as docx_lib

from vllm import LLM, SamplingParams

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIG  â† edit paths here
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CFG = {
    # â”€â”€ Model paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "LLM_PATH":       "/models/Qwen2.5-72B-Instruct",
    "EMBED_PATH":     "/models/Qwen3-Embedding",

    # â”€â”€ Hardware â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 12Ã— H100 for LLM tensor parallelism
    "TENSOR_PARALLEL":  12,
    # A40 MIG slice for embeddings (set CUDA_VISIBLE_DEVICES externally
    # or hardcode device here; SentenceTransformer picks it up)
    "EMBED_DEVICE":     "cuda:0",   # the MIG slice appears as cuda:0 in its own env

    # â”€â”€ Retrieval â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "DENSE_K":          1500,       # docs fetched from ChromaDB
    "RERANK_K":         50,         # docs kept after BM25

    # â”€â”€ ChromaDB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "CHROMA_PATH":      "/mnt/chroma_db",
    "COLLECTION_NAME":  "bank_docs",

    # â”€â”€ Persistence â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "CHAT_STORAGE":     "/mnt/chat_sessions.json",
    "FEEDBACK_CSV":     "/mnt/chat_feedback.csv",
    "LOGO_PATH":        "/mnt/bnp_logo.png",

    # â”€â”€ Generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "MAX_NEW_TOKENS":   2048,
    "TEMPERATURE":      0.5,
    "TOP_P":            0.9,

    # â”€â”€ Embedding instruction (Qwen3 asymmetric) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "QUERY_INSTRUCTION": (
        "Instruct: Retrieve the most relevant banking document passage "
        "for the following query\nQuery: "
    ),

    # â”€â”€ Chunking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "CHUNK_SIZE":       5000,
    "CHUNK_OVERLAP":    500,
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RESOURCE LOADING  (cached â€“ loaded once per Streamlit session)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner="Loading embedding model on A40 MIGâ€¦")
def load_embedding_model():
    """
    Qwen3-Embedding on the A40 3G MIG slice.
    Run this process with CUDA_VISIBLE_DEVICES pointing to the MIG slice,
    or set EMBED_DEVICE to the correct cuda index.
    """
    model = SentenceTransformer(
        CFG["EMBED_PATH"],
        device=CFG["EMBED_DEVICE"],
        trust_remote_code=True,
    )
    return model


@st.cache_resource(show_spinner="Loading Qwen2.5-72B on 12Ã— H100 via vLLMâ€¦")
def load_llm():
    """
    vLLM with tensor parallelism across 12 H100s.
    Qwen2.5-72B in float16 uses ~144 GB VRAM â†’ fits comfortably in 576 GB.
    """
    llm = LLM(
        model=CFG["LLM_PATH"],
        tensor_parallel_size=CFG["TENSOR_PARALLEL"],
        dtype="float16",
        max_model_len=8192,
        gpu_memory_utilization=0.90,
        trust_remote_code=True,
        enforce_eager=False,          # use CUDA graphs for speed
    )
    return llm


@st.cache_resource(show_spinner="Connecting to ChromaDBâ€¦")
def load_collection():
    client = chromadb.PersistentClient(path=CFG["CHROMA_PATH"])
    collection = client.get_or_create_collection(
        name=CFG["COLLECTION_NAME"],
        metadata={"hnsw:space": "cosine"},
    )
    return collection


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# BM25 RERANKER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class BM25Reranker:
    def __init__(self, k: int = CFG["RERANK_K"]):
        self.k             = k
        self.bm25          = None
        self.doc_map       = {}
        self._strip_punct  = str.maketrans("", "", string.punctuation)

    def _tokenize(self, text: str) -> list[str]:
        if not isinstance(text, str):
            return []
        cleaned = text.translate(self._strip_punct).lower()
        return [t for t in cleaned.split() if t.strip()]

    def fit(self, documents: list[dict]):
        if not documents:
            raise ValueError("BM25Reranker.fit() received an empty document list.")
        tokenized_corpus, valid_docs = [], []
        for i, doc in enumerate(documents):
            text   = doc.get("text") or doc.get("page_content") or doc.get("content", "")
            tokens = self._tokenize(text)
            if tokens:
                tokenized_corpus.append(tokens)
                valid_docs.append((i, doc))
        if not tokenized_corpus:
            raise ValueError("All documents were empty â€“ cannot fit BM25.")
        self.doc_map = {i: doc for i, doc in valid_docs}
        self.bm25    = BM25Okapi(tokenized_corpus)

    def rerank(self, query: str, documents: list[dict]) -> list[dict]:
        if not documents:
            return []
        self.fit(documents)
        tokens = self._tokenize(query)
        if not tokens:
            return documents[: self.k]
        scores = self.bm25.get_scores(tokens)
        ranked = sorted(
            zip(scores, self.doc_map.values()),
            key=lambda x: x[0],
            reverse=True,
        )
        return [doc for _, doc in ranked[: self.k]]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RETRIEVER  (ChromaDB dense + BM25 rerank)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class BankRetriever:
    def __init__(
        self,
        collection,
        embed_model,
        chat_history: list[str] | None = None,
        k: int       = CFG["DENSE_K"],
        rerank_k: int = CFG["RERANK_K"],
    ):
        self.collection   = collection
        self.embed_model  = embed_model
        self.chat_history = chat_history or []
        self.k            = k
        self.reranker     = BM25Reranker(k=rerank_k)

    def _embed_query(self, text: str) -> list[float]:
        """Qwen3 asymmetric: query gets instruction prefix."""
        vec = self.embed_model.encode(
            [CFG["QUERY_INSTRUCTION"] + text],
            normalize_embeddings=True,
            batch_size=1,
        )
        return vec[0].tolist()

    def get_relevant_documents(self, query: str) -> list[dict]:
        # Include recent history for context-aware retrieval
        full_query = (
            " ".join(self.chat_history[-6:]) + " " + query
            if self.chat_history
            else query
        )

        query_vec = self._embed_query(full_query)

        results = self.collection.query(
            query_embeddings=[query_vec],
            n_results=min(self.k, self.collection.count()),
            include=["documents", "metadatas"],
        )

        dense_docs = [
            {"text": doc, "metadata": meta}
            for doc, meta in zip(
                results["documents"][0],
                results["metadatas"][0],
            )
        ]

        return self.reranker.rerank(full_query, dense_docs)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DOCUMENT PROCESSING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_text_from_file(uploaded_file) -> Document | None:
    name = uploaded_file.name.lower()
    try:
        if name.endswith(".pdf"):
            pdf  = fitz.open(stream=uploaded_file.read(), filetype="pdf")
            pages = []
            for page_num, page in enumerate(pdf, start=1):
                text = page.get_text()
                # Attempt table extraction if text is sparse
                if len(text.strip()) < 100:
                    tabs = page.find_tables()
                    if tabs.tables:
                        text = "\n".join(
                            tab.to_pandas().to_string(index=False)
                            for tab in tabs.tables
                        )
                pages.append(f"[Page {page_num}]\n{text}")
            content = "\n\n".join(pages)
        elif name.endswith(".txt"):
            content = uploaded_file.read().decode("utf-8")
        elif name.endswith(".docx"):
            doc     = docx_lib.Document(uploaded_file)
            content = "\n".join(p.text for p in doc.paragraphs)
        elif name.endswith(".csv"):
            df      = pd.read_csv(uploaded_file)
            content = df.to_string(index=False)
        elif name.endswith(".xlsx"):
            # Read all sheets
            xl      = pd.ExcelFile(uploaded_file)
            parts   = []
            for sheet in xl.sheet_names:
                df = xl.parse(sheet)
                parts.append(f"[Sheet: {sheet}]\n{df.to_string(index=False)}")
            content = "\n\n".join(parts)
        else:
            return None
    except Exception as e:
        st.sidebar.error(f"Error reading {uploaded_file.name}: {e}")
        return None

    return Document(
        page_content=content,
        metadata={
            "source":      uploaded_file.name,
            "source_type": name.rsplit(".", 1)[-1],
        },
    )


def clean_text(text: str) -> str:
    text = re.sub(r"\.{2,}", ".", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    text = re.sub(r"[ \t]+", " ", text)
    text = "\n".join(line.strip() for line in text.splitlines())
    text = "\n".join(line for line in text.splitlines() if line.strip())
    return text


def clean_documents(docs: list[Document]) -> list[Document]:
    return [
        Document(page_content=clean_text(d.page_content), metadata=d.metadata)
        for d in docs
    ]


def split_documents(docs: list[Document]) -> list[Document]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CFG["CHUNK_SIZE"],
        chunk_overlap=CFG["CHUNK_OVERLAP"],
        length_function=len,
        separators=["\n\n", "\n", ".", " ", ""],
        add_start_index=True,
    )
    return splitter.split_documents(docs)


def embed_and_index(chunks: list[Document], collection, embed_model):
    """Encode documents WITHOUT instruction prefix (Qwen3 asymmetric)."""
    texts  = [c.page_content for c in chunks]
    metas  = [c.metadata     for c in chunks]
    ids    = [str(uuid.uuid4()) for _ in chunks]

    # Batch encode for speed
    vecs = embed_model.encode(
        texts,
        normalize_embeddings=True,
        batch_size=64,
        show_progress_bar=True,
    ).tolist()

    # ChromaDB supports batch upsert up to 41666 items
    batch_size = 5000
    for i in range(0, len(texts), batch_size):
        collection.add(
            documents=texts[i: i + batch_size],
            embeddings=vecs[i:  i + batch_size],
            metadatas=metas[i:  i + batch_size],
            ids=ids[i:          i + batch_size],
        )


def get_indexed_sources(collection) -> set[str]:
    try:
        n = min(collection.count(), 50000)
        if n == 0:
            return set()
        result = collection.get(limit=n, include=["metadatas"])
        return {m.get("source", "") for m in result["metadatas"] if m.get("source")}
    except Exception:
        return set()


def extract_sources(docs: list[dict]) -> list[str]:
    seen, out = set(), []
    for doc in docs:
        src = doc.get("metadata", {}).get("source", "")
        if src and src not in seen:
            seen.add(src)
            out.append(src)
    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PROMPT BUILDER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT = """Vous Ãªtes un assistant RAG expert en systÃ¨mes bancaires, architecture de donnÃ©es et produits financiers.
Vous rÃ©pondez EXCLUSIVEMENT Ã  partir des documents fournis dans le contexte.
Vous citez toujours vos sources au format : [Source: <fichier>, Page <n>].
Si l'information est absente des documents, dites-le clairement."""


def build_prompt(query: str, context: list[dict], chat_history: list[str]) -> str:
    history_block = "\n".join(chat_history[-6:]) if chat_history else "Aucun historique."

    context_block = "\n\n".join(
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
        f"[Source: {c['metadata'].get('source', 'inconnu')}, "
        f"Page {c['metadata'].get('page', c['metadata'].get('start_index', '?'))}]\n"
        f"{c['text']}"
        for c in context
    )

    return (
        f"<|im_start|>system\n{SYSTEM_PROMPT}<|im_end|>\n"
        f"<|im_start|>user\n"
        f"[HISTORIQUE DE CONVERSATION]\n{history_block}\n\n"
        f"[CONTEXTE DOCUMENTAIRE]\n{context_block}\n\n"
        f"[QUESTION]\n{query}\n\n"
        f"[INSTRUCTIONS DE RÃ‰PONSE]\n"
        f"1. RÃ©pondez uniquement Ã  partir du contexte fourni.\n"
        f"2. Structure : a) RÃ©ponse dÃ©taillÃ©e  b) SynthÃ¨se  c) Sources utilisÃ©es\n"
        f"3. Citez chaque fait avec [Source: <fichier>, Page <n>].\n"
        f"4. Pour les tableaux ou donnÃ©es techniques, expliquez leur signification.\n"
        f"5. Si l'information manque, indiquez-le clairement.\n"
        f"<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GENERATION  (local vLLM)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_response(
    query: str,
    context: list[dict],
    chat_history: list[str],
    llm: LLM,
) -> str:
    prompt  = build_prompt(query, context, chat_history)
    params  = SamplingParams(
        temperature=CFG["TEMPERATURE"],
        top_p=CFG["TOP_P"],
        max_tokens=CFG["MAX_NEW_TOKENS"],
        stop=["<|im_end|>", "<|endoftext|>"],
    )
    outputs = llm.generate([prompt], params)
    return outputs[0].outputs[0].text.strip()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EVALUATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def evaluate_response(
    query: str,
    answer: str,
    docs: list[dict],
) -> dict:
    q_tokens = set(query.lower().split())
    a_tokens = set(answer.lower().split())

    # Document relevance
    relevant = sum(
        1 for doc in docs
        if (
            len(q_tokens & set(
                (doc.get("text") or "").lower().split()
            )) / len(q_tokens)
        ) > 0.2
        if q_tokens
    )
    doc_relevance    = round((relevant / len(docs)) * 10, 2) if docs else 0

    # Source diversity
    sources          = {doc.get("metadata", {}).get("source", "") for doc in docs}
    source_diversity = round(min(len(sources) / 3, 1.0) * 10, 2)

    # Answerâ€“query alignment
    alignment = round(
        (len(q_tokens & a_tokens) / len(q_tokens) * 10) if q_tokens else 0, 2
    )

    # Hallucination risk proxy
    cites_source = any(
        kw in answer.lower()
        for kw in ["source:", "page", "provient", "extrait", "selon le document"]
    )

    weights = {"doc_relevance": 0.4, "source_diversity": 0.1, "alignment": 0.3}
    scores  = {
        "doc_relevance":    doc_relevance,
        "source_diversity": source_diversity,
        "alignment":        alignment,
    }
    overall = round(sum(scores[k] * weights[k] for k in weights), 2)

    return {
        "document_relevance":    doc_relevance,
        "source_diversity":      source_diversity,
        "answer_query_alignment": alignment,
        "hallucination_risk":    "Faible" if cites_source else "Moyen",
        "overall_score":         overall,
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FEEDBACK
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_feedback(chat_id: str, msg_index: int, feedback: str):
    row = pd.DataFrame([{
        "chat_id":       chat_id,
        "message_index": msg_index,
        "feedback":      feedback,
        "timestamp":     str(datetime.datetime.now()),
    }])
    mode   = "a" if os.path.exists(CFG["FEEDBACK_CSV"]) else "w"
    header = mode == "w"
    row.to_csv(CFG["FEEDBACK_CSV"], mode=mode, header=header, index=False)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CHAT SESSION PERSISTENCE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_sessions() -> dict:
    if os.path.exists(CFG["CHAT_STORAGE"]):
        with open(CFG["CHAT_STORAGE"], "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_sessions(sessions: dict):
    with open(CFG["CHAT_STORAGE"], "w", encoding="utf-8") as f:
        json.dump(sessions, f, indent=2, ensure_ascii=False)


def get_history_strings(messages: list[dict], max_turns: int = 6) -> list[str]:
    return [
        f"{m['role'].capitalize()}: {m['content']}"
        for m in messages[-max_turns:]
    ]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN RAG PIPELINE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rag_pipeline(
    query: str,
    collection,
    embed_model,
    llm: LLM,
    chat_history: list[str],
    temp_chunks: list[str] | None = None,
) -> dict:
    retriever = BankRetriever(
        collection=collection,
        embed_model=embed_model,
        chat_history=chat_history,
        k=CFG["DENSE_K"],
        rerank_k=CFG["RERANK_K"],
    )
    context = retriever.get_relevant_documents(query)

    # Inject temporarily uploaded chunks at top of context
    if temp_chunks:
        temp_docs = [
            {"text": chunk, "metadata": {"source": "DOCUMENT_TEMPORAIRE", "page": "?"}}
            for chunk in temp_chunks
        ]
        context = temp_docs + context

    answer     = generate_response(query, context, chat_history, llm)
    evaluation = evaluate_response(query, answer, context)
    sources    = extract_sources(context)

    return {
        "answer":     answer,
        "context":    context,
        "evaluation": evaluation,
        "sources":    sources,
        "timestamp":  datetime.datetime.now(),
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STREAMLIT UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    st.set_page_config(
        page_title="Chatbot BNP â€“ Documentation Bancaire",
        page_icon="ðŸ¦",
        layout="wide",
    )

    # â”€â”€ Load resources â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    embed_model = load_embedding_model()
    llm         = load_llm()
    collection  = load_collection()

    # â”€â”€ Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    col_logo, col_title = st.columns([1, 6])
    with col_logo:
        if os.path.exists(CFG["LOGO_PATH"]):
            st.image(Image.open(CFG["LOGO_PATH"]), width=120)
    with col_title:
        st.title("ðŸ¦ Chatbot BNP â€“ Documentation Bancaire")
        st.caption(
            f"ModÃ¨le : Qwen2.5-72B-Instruct Â· "
            f"Embedding : Qwen3-Embedding Â· "
            f"Documents indexÃ©s : {collection.count():,}"
        )

    # â”€â”€ Session state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if "sessions" not in st.session_state:
        st.session_state.sessions = load_sessions()
    if "current_id" not in st.session_state:
        cid = str(uuid.uuid4())
        st.session_state.current_id = cid
        st.session_state.sessions[cid] = {"title": "Nouvelle conversation", "messages": []}

    # â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.sidebar:
        st.markdown("## ðŸ’¬ Conversations")
        if st.button("âž• Nouvelle conversation", use_container_width=True):
            cid = str(uuid.uuid4())
            st.session_state.current_id = cid
            st.session_state.sessions[cid] = {
                "title": "Nouvelle conversation", "messages": []
            }
            save_sessions(st.session_state.sessions)

        st.markdown("---")
        ids = list(st.session_state.sessions.keys())
        if ids:
            selected = st.radio(
                "Historique",
                ids,
                format_func=lambda k: st.session_state.sessions[k]["title"][:40],
                index=(
                    ids.index(st.session_state.current_id)
                    if st.session_state.current_id in ids
                    else 0
                ),
                label_visibility="collapsed",
            )
            st.session_state.current_id = selected

        st.markdown("---")
        st.markdown("## ðŸ“Ž Ajouter un document")
        uploaded = st.file_uploader(
            "PDF, DOCX, TXT, CSV, XLSX",
            type=["pdf", "txt", "docx", "csv", "xlsx"],
            label_visibility="collapsed",
        )

        # ChromaDB stats
        st.markdown("---")
        st.markdown("### ðŸ“Š Base de connaissances")
        st.metric("Chunks indexÃ©s", f"{collection.count():,}")

    # â”€â”€ Handle uploaded file â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    temp_chunks = None
    if uploaded:
        existing = get_indexed_sources(collection)
        if uploaded.name in existing:
            st.sidebar.warning(f"âš ï¸ **{uploaded.name}** est dÃ©jÃ  indexÃ©.")
        else:
            with st.sidebar:
                with st.spinner(f"Indexation de {uploaded.name}â€¦"):
                    raw     = extract_text_from_file(uploaded)
                    if raw is None:
                        st.error("âŒ Format non supportÃ©.")
                    else:
                        cleaned = clean_documents([raw])
                        chunks  = split_documents(cleaned)
                        embed_and_index(chunks, collection, embed_model)
                        temp_chunks = [c.page_content for c in chunks]
                        st.success(f"âœ… {len(chunks)} chunks indexÃ©s depuis **{uploaded.name}**")

    # â”€â”€ Chat area â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    session  = st.session_state.sessions[st.session_state.current_id]
    messages = session["messages"]

    for i, msg in enumerate(messages):
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"], unsafe_allow_html=True)

            if msg["role"] == "assistant":
                # Feedback buttons
                f_col1, f_col2, f_col3 = st.columns([1, 1, 10])
                with f_col1:
                    if st.button("ðŸ‘", key=f"up_{i}", help="Bonne rÃ©ponse"):
                        save_feedback(st.session_state.current_id, i, "positive")
                        st.toast("Merci pour votre retour !", icon="âœ…")
                with f_col2:
                    if st.button("ðŸ‘Ž", key=f"dn_{i}", help="Mauvaise rÃ©ponse"):
                        save_feedback(st.session_state.current_id, i, "negative")
                        st.toast("Retour enregistrÃ©.", icon="ðŸ“")

                # Metrics expander
                if "evaluation" in msg:
                    with st.expander("ðŸ“Š MÃ©triques RAG"):
                        ev = msg["evaluation"]
                        m1, m2, m3, m4 = st.columns(4)
                        m1.metric("Pertinence docs",  f"{ev['document_relevance']}/10")
                        m2.metric("DiversitÃ© sources", f"{ev['source_diversity']}/10")
                        m3.metric("Alignement",        f"{ev['answer_query_alignment']}/10")
                        m4.metric("Score global",      f"{ev['overall_score']}/10")
                        st.markdown(
                            f"**Risque hallucination :** {ev['hallucination_risk']}"
                        )

                if "sources" in msg and msg["sources"]:
                    with st.expander("ðŸ“‚ Sources utilisÃ©es"):
                        for src in msg["sources"]:
                            st.markdown(f"- `{src}`")

    # â”€â”€ Input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    user_input = st.chat_input("Posez votre question sur la documentation bancaireâ€¦")

    if user_input:
        ts = datetime.datetime.now().isoformat()

        # Auto-title the conversation
        if session["title"] == "Nouvelle conversation":
            session["title"] = user_input[:45] + ("â€¦" if len(user_input) > 45 else "")

        # Save & display user message
        messages.append({"role": "user", "content": user_input, "timestamp": ts})
        save_sessions(st.session_state.sessions)

        with st.chat_message("user"):
            st.markdown(user_input)

        # Generate & display assistant response
        with st.chat_message("assistant"):
            with st.spinner("Recherche et gÃ©nÃ©ration en coursâ€¦"):
                history  = get_history_strings(messages)
                response = rag_pipeline(
                    query=user_input,
                    collection=collection,
                    embed_model=embed_model,
                    llm=llm,
                    chat_history=history,
                    temp_chunks=temp_chunks,
                )

            answer = response["answer"]
            st.markdown(answer, unsafe_allow_html=True)

            # Inline metrics
            ev = response["evaluation"]
            m1, m2, m3, m4 = st.columns(4)
            m1.metric("Pertinence docs",   f"{ev['document_relevance']}/10")
            m2.metric("DiversitÃ© sources", f"{ev['source_diversity']}/10")
            m3.metric("Alignement",        f"{ev['answer_query_alignment']}/10")
            m4.metric("Score global",      f"{ev['overall_score']}/10")

            if response["sources"]:
                with st.expander("ðŸ“‚ Sources utilisÃ©es"):
                    for src in response["sources"]:
                        st.markdown(f"- `{src}`")

        # Persist assistant message with metadata
        messages.append({
            "role":       "assistant",
            "content":    answer,
            "timestamp":  ts,
            "evaluation": response["evaluation"],
            "sources":    response["sources"],
        })
        save_sessions(st.session_state.sessions)


if __name__ == "__main__":
    main()







Bonjour [Nom du manager],

Comme convenu, voici le dÃ©tail des heures supplÃ©mentaires que jâ€™ai effectuÃ©es afin dâ€™optimiser le modÃ¨le.

Jâ€™ai Ã©galement procÃ©dÃ© Ã  lâ€™envoi des fichiers tard dans la soirÃ©e afin de respecter les dÃ©lais et dâ€™assurer la continuitÃ© du travail.

Vous pouvez les comptabiliser le mois prochain selon ce qui vous convient.

Nâ€™hÃ©sitez pas Ã  me faire savoir si vous avez besoin dâ€™informations complÃ©mentaires.

Cordialement,
Nihad




Nous avons eu un problÃ¨me GPU et de connexion pour lâ€™upload, ce qui a retardÃ© lâ€™envoi.
Nous vous envoyons lâ€™output dans lâ€™heure.


# Clear pip cache
pip cache purge

# Clear HuggingFace cache if models are duplicated
du -sh ~/.cache/huggingface/
# Only delete if you have copies elsewhere
rm -rf ~/.cache/huggingface/hub/tmp*

# Clear temp files
rm -rf /tmp/*

# Clear torch cache
rm -rf ~/.cache/torch/



import shutil

folder_path = "/path/to/your/folder"

shutil.rmtree(folder_path)
print("Folder deleted successfully")




"Nombre de pages trouvÃ©es": doc.get("total_pages_found", 0)
}


row = {
    "filename": filename,
    "total_pages_found": sum(1 for p in pages.values() if not p["missing"])
}


for filename, pages in per_file.items():
    actual_pages = sum(1 for p in pages.values() if not p["missing"])
    final_output = {
        "filename": filename,
        "total_pages_found": actual_pages,
        ...
    }





# Step 4: Save JSON per PDF
for filename, pages in results_map.items():
    final_output = [
        {"file_name": filename, "page": str(idx + 1), "text": pages[idx]}
        for idx in sorted(pages)
    ]
    json_path = os.path.join(JSON_SAVE_DIR, f"{filename}.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)
    print(f"âœ“ saved {json_path}")

print(f"\nâœ… Done! {datetime.now()}")







final_output = [
    {"file_name": file_name, "page": str(i + 1), "text": page_text}
    for i, page_text in enumerate(all_text)
]


import os
import json
import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils import get_column_letter

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STRUCTURED_FOLDER = "/mnt/Dossier_agence/structured_V2_JANVIER_2026/"
OUTPUT_EXCEL      = "/mnt/Dossier_agence/extraction_results.xlsx"

PAGE_TYPES = [
    "ordre_de_virement",
    "annexe_1",
    "annexe_2",
    "bulletin_de_paie",
    "gestion_des_blocages",
]

PAGE_LABELS = {
    "ordre_de_virement":    "Ordre de Virement",
    "annexe_1":             "Annexe 1",
    "annexe_2":             "Annexe 2",
    "bulletin_de_paie":     "Bulletin de Paie",
    "gestion_des_blocages": "Gestion des Blocages",
}

# â”€â”€ Update these fields to match your exact schema field names â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PAGE_FIELDS = {
    "ordre_de_virement": [
        "Monnaie", "Montant en chiffres", "Montant en lettres",
        "Nom/PrÃ©nom", "Adresse complÃ¨te", "NumÃ©ro de compte",
        "SiÃ¨ge", "Racine", "Ordinal", "Devise", "ClÃ©", "Adresse SWIFT",
    ],
    "annexe_1": [
        "Nom/PrÃ©nom", "Date de naissance", "Matricule",
        "Grade", "Fonction", "Structure",
    ],
    "annexe_2": [
        "Nom/PrÃ©nom", "RIB", "Banque", "Montant", "Motif",
    ],
    "bulletin_de_paie": [
        "Nom/PrÃ©nom", "Mois", "Salaire de base",
        "Net Ã  payer", "IndemnitÃ©s", "Retenues", "Matricule",
    ],
    "gestion_des_blocages": [
        "Nom/PrÃ©nom", "Type de blocage", "Montant bloquÃ©",
        "Date dÃ©but", "Date fin", "Motif",
        "SiÃ¨ge", "Racine", "Ordinal", "Devise",
    ],
}

# â”€â”€ Step 1: Load all structured JSONs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all_rows = []

for filename in sorted(os.listdir(STRUCTURED_FOLDER)):
    if not filename.endswith(".json"):
        continue

    path = os.path.join(STRUCTURED_FOLDER, filename)
    with open(path, "r", encoding="utf-8") as f:
        doc = json.load(f)

    # Normalize root: handle list or dict
    if isinstance(doc, list):
        doc = doc[0] if doc else {}
    if not isinstance(doc, dict):
        print(f"âš  Skipping {filename}: unexpected root type {type(doc)}")
        continue

    row = {"Fichier": doc.get("filename", filename)}

    # Safely get pages dict
    pages = doc.get("pages", {})
    if not isinstance(pages, dict):
        pages = {}

    for page_type in PAGE_TYPES:
        label  = PAGE_LABELS[page_type]
        fields = PAGE_FIELDS[page_type]

        page_data = pages.get(page_type)  # None if key missing entirely

        # Determine presence and extract data dict
        if page_data is None or not isinstance(page_data, dict):
            is_present = False
            data = {}
        else:
            is_present = not page_data.get("missing", True)
            raw_data   = page_data.get("data", {})
            data       = raw_data if isinstance(raw_data, dict) else {}

        # Presence column
        row[f"{label} (prÃ©sent)"] = "Oui" if is_present else "Non"

        # Field columns
        for field in fields:
            value = data.get(field, None)
            # Normalize null strings to Python None
            if isinstance(value, str) and value.strip().lower() in ("null", "none", ""):
                value = None
            row[f"{label} | {field}"] = value

    all_rows.append(row)

print(f"âœ“ Loaded {len(all_rows)} files")

# â”€â”€ Step 2: Build DataFrame and write base Excel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df = pd.DataFrame(all_rows)
df.to_excel(OUTPUT_EXCEL, index=False, engine="openpyxl")

# â”€â”€ Step 3: Apply formatting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
wb = load_workbook(OUTPUT_EXCEL)
ws = wb.active
ws.title = "Extraction"

# Color scheme: (dark header color, light data color)
PAGE_COLORS = {
    "Ordre de Virement":    ("1A5276", "D6E4F0"),
    "Annexe 1":             ("1E8449", "D5F5E3"),
    "Annexe 2":             ("B7950B", "FEF9E7"),
    "Bulletin de Paie":     ("922B21", "FDEDEC"),
    "Gestion des Blocages": ("6C3483", "F5EEF8"),
}

PRESENCE_GREEN = "C6EFCE"
PRESENCE_RED   = "FFC7CE"

thin   = Side(style="thin", color="CCCCCC")
border = Border(left=thin, right=thin, top=thin, bottom=thin)

def get_label(col_name):
    for label in PAGE_COLORS:
        if col_name.startswith(label):
            return label
    return None

# Header row formatting
ws.row_dimensions[1].height = 38
for col_idx, col_name in enumerate(df.columns, start=1):
    cell  = ws.cell(row=1, column=col_idx)
    label = get_label(col_name)
    dark  = PAGE_COLORS[label][0] if label else "2C3E50"
    cell.fill      = PatternFill("solid", start_color=dark, end_color=dark)
    cell.font      = Font(bold=True, color="FFFFFF", name="Arial", size=10)
    cell.alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)
    cell.border    = border

# Data rows formatting
for row_idx in range(2, len(df) + 2):
    for col_idx, col_name in enumerate(df.columns, start=1):
        cell  = ws.cell(row=row_idx, column=col_idx)
        label = get_label(col_name)
        cell.font      = Font(name="Arial", size=9)
        cell.alignment = Alignment(horizontal="left", vertical="center")
        cell.border    = border

        if "(prÃ©sent)" in col_name:
            if cell.value == "Oui":
                cell.fill = PatternFill("solid", start_color=PRESENCE_GREEN, end_color=PRESENCE_GREEN)
                cell.font = Font(name="Arial", size=9, bold=True, color="276221")
            else:
                cell.fill = PatternFill("solid", start_color=PRESENCE_RED, end_color=PRESENCE_RED)
                cell.font = Font(name="Arial", size=9, bold=True, color="9C0006")
        elif label:
            light = PAGE_COLORS[label][1]
            cell.fill = PatternFill("solid", start_color=light, end_color=light)

        # Replace None with empty string for display
        if cell.value is None:
            cell.value = ""

# Column widths
for col_idx, col_name in enumerate(df.columns, start=1):
    col_letter = get_column_letter(col_idx)
    if col_idx == 1:
        ws.column_dimensions[col_letter].width = 35
    elif "(prÃ©sent)" in col_name:
        ws.column_dimensions[col_letter].width = 18
    else:
        ws.column_dimensions[col_letter].width = 28

# Freeze header row and filename column
ws.freeze_panes = "B2"

# â”€â”€ Step 4: Summary sheet â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ws2 = wb.create_sheet("RÃ©sumÃ©")
headers = ["Page", "Total fichiers", "PrÃ©sent", "Manquant", "% PrÃ©sent"]

for ci, h in enumerate(headers, 1):
    c = ws2.cell(row=1, column=ci, value=h)
    c.fill      = PatternFill("solid", start_color="2C3E50", end_color="2C3E50")
    c.font      = Font(bold=True, color="FFFFFF", name="Arial", size=10)
    c.alignment = Alignment(horizontal="center")
    c.border    = border

total = len(df)
for ri, (page_type, label) in enumerate(PAGE_LABELS.items(), start=2):
    presence_col = f"{label} (prÃ©sent)"
    present = int((df[presence_col] == "Oui").sum()) if presence_col in df.columns else 0
    missing = total - present
    pct     = f"{round(present / total * 100, 1)}%" if total > 0 else "0%"

    for ci, val in enumerate([label, total, present, missing, pct], 1):
        c = ws2.cell(row=ri, column=ci, value=val)
        c.font      = Font(name="Arial", size=10)
        c.alignment = Alignment(horizontal="center")
        c.border    = border
        if ci == 3:
            c.fill = PatternFill("solid", start_color=PRESENCE_GREEN, end_color=PRESENCE_GREEN)
        elif ci == 4:
            c.fill = PatternFill("solid", start_color=PRESENCE_RED, end_color=PRESENCE_RED)

for col in ["A", "B", "C", "D", "E"]:
    ws2.column_dimensions[col].width = 22

ws2.freeze_panes = "A2"

wb.save(OUTPUT_EXCEL)
print(f"âœ… Excel saved â†’ {OUTPUT_EXCEL}")
print(f"   {len(df)} rows | {len(df.columns)} columns")




page_data = pages.get(page_type, None) if isinstance(pages, dict) else None

for field in get_schema(page_type).keys():
    row[f"{label} | {field}"] = data.get(field, None) if data else None


pages = doc[0].get("pages", {}) if isinstance(doc, list) else doc.get("pages", {})


import os
import json
import re
import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils import get_column_letter

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STRUCTURED_FOLDER = "/mnt/Dossier_agence/structured_V2_JANVIER_2026/"
OUTPUT_EXCEL      = "/mnt/Dossier_agence/extraction_results.xlsx"

# Page types in order
PAGE_TYPES = [
    "ordre_de_virement",
    "annexe_1",
    "annexe_2",
    "bulletin_de_paie",
    "gestion_des_blocages",
]

PAGE_LABELS = {
    "ordre_de_virement":  "Ordre de Virement",
    "annexe_1":           "Annexe 1",
    "annexe_2":           "Annexe 2",
    "bulletin_de_paie":   "Bulletin de Paie",
    "gestion_des_blocages": "Gestion des Blocages",
}

# â”€â”€ Load all structured JSONs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all_rows = []

for filename in sorted(os.listdir(STRUCTURED_FOLDER)):
    if not filename.endswith(".json"):
        continue

    path = os.path.join(STRUCTURED_FOLDER, filename)
    with open(path, "r", encoding="utf-8") as f:
        doc = json.load(f)

    pages = doc.get("pages", {})
    row = {"Fichier": doc.get("filename", filename)}

    for page_type in PAGE_TYPES:
        label = PAGE_LABELS[page_type]
        page_data = pages.get(page_type, None)

        # Presence column: Yes / No
        if page_data is None or page_data.get("missing", True):
            row[f"{label} (prÃ©sent)"] = "Non"
            # All fields null
            for field in get_schema(page_type).keys():
                row[f"{label} | {field}"] = None
        else:
            row[f"{label} (prÃ©sent)"] = "Oui"
            data = page_data.get("data", {})
            for field in get_schema(page_type).keys():
                row[f"{label} | {field}"] = data.get(field, None)

    all_rows.append(row)

# â”€â”€ Build DataFrame â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df = pd.DataFrame(all_rows)

# â”€â”€ Write to Excel with formatting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df.to_excel(OUTPUT_EXCEL, index=False, engine="openpyxl")

wb = load_workbook(OUTPUT_EXCEL)
ws = wb.active
ws.title = "Extraction"

# Color palette per page type
PAGE_COLORS = {
    "Ordre de Virement":    "D6E4F0",   # blue
    "Annexe 1":             "D5F5E3",   # green
    "Annexe 2":             "FEF9E7",   # yellow
    "Bulletin de Paie":     "FDEDEC",   # red
    "Gestion des Blocages": "F5EEF8",   # purple
}

PRESENCE_GREEN = "C6EFCE"
PRESENCE_RED   = "FFC7CE"

# Style helpers
def header_fill(hex_color):
    return PatternFill("solid", start_color=hex_color, end_color=hex_color)

def cell_fill(hex_color):
    return PatternFill("solid", start_color=hex_color, end_color=hex_color)

thin = Side(style="thin", color="AAAAAA")
border = Border(left=thin, right=thin, top=thin, bottom=thin)

# â”€â”€ Format header row â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for col_idx, col_name in enumerate(df.columns, start=1):
    cell = ws.cell(row=1, column=col_idx)
    cell.font      = Font(bold=True, color="FFFFFF", name="Arial", size=10)
    cell.alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)
    cell.border    = border

    # Determine which page this column belongs to
    matched_color = "2C3E50"   # dark default (for "Fichier" column)
    for label, color in PAGE_COLORS.items():
        if col_name.startswith(label):
            # Darken the header vs data rows
            matched_color = "1A5276" if "bleu" not in label else "1D8348"
            # Use a darker shade for header
            dark_colors = {
                "Ordre de Virement":    "1A5276",
                "Annexe 1":             "1E8449",
                "Annexe 2":             "B7950B",
                "Bulletin de Paie":     "922B21",
                "Gestion des Blocages": "6C3483",
            }
            matched_color = dark_colors.get(label, "2C3E50")
            break

    cell.fill = PatternFill("solid", start_color=matched_color, end_color=matched_color)

# â”€â”€ Format data rows â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for row_idx in range(2, len(df) + 2):
    for col_idx, col_name in enumerate(df.columns, start=1):
        cell = ws.cell(row=row_idx, column=col_idx)
        cell.font      = Font(name="Arial", size=9)
        cell.alignment = Alignment(horizontal="left", vertical="center", wrap_text=False)
        cell.border    = border

        # Color presence columns
        if "(prÃ©sent)" in col_name:
            val = cell.value
            if val == "Oui":
                cell.fill = cell_fill(PRESENCE_GREEN)
                cell.font = Font(name="Arial", size=9, bold=True, color="276221")
            elif val == "Non":
                cell.fill = cell_fill(PRESENCE_RED)
                cell.font = Font(name="Arial", size=9, bold=True, color="9C0006")
        else:
            # Light background per page section
            for label, color in PAGE_COLORS.items():
                if col_name.startswith(label):
                    cell.fill = cell_fill(color)
                    break

        # Null â†’ empty string display
        if cell.value is None:
            cell.value = ""

# â”€â”€ Column widths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ws.column_dimensions["A"].width = 35   # Fichier

for col_idx, col_name in enumerate(df.columns, start=1):
    col_letter = get_column_letter(col_idx)
    if "(prÃ©sent)" in col_name:
        ws.column_dimensions[col_letter].width = 18
    elif col_idx == 1:
        pass  # already set
    else:
        ws.column_dimensions[col_letter].width = 28

# â”€â”€ Freeze first row + first column â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ws.freeze_panes = "B2"

# â”€â”€ Add a summary sheet â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ws_summary = wb.create_sheet("RÃ©sumÃ©")
ws_summary["A1"] = "Page"
ws_summary["B1"] = "PrÃ©sent"
ws_summary["C1"] = "Manquant"
ws_summary["D1"] = "% PrÃ©sent"

for cell in ws_summary[1]:
    cell.font = Font(bold=True, name="Arial", size=10)
    cell.fill = PatternFill("solid", start_color="2C3E50", end_color="2C3E50")
    cell.font = Font(bold=True, color="FFFFFF", name="Arial", size=10)

total_files = len(df)
for i, (page_type, label) in enumerate(PAGE_LABELS.items(), start=2):
    presence_col = f"{label} (prÃ©sent)"
    if presence_col in df.columns:
        present_count = (df[presence_col] == "Oui").sum()
        missing_count = total_files - present_count
        ws_summary.cell(row=i, column=1).value = label
        ws_summary.cell(row=i, column=2).value = present_count
        ws_summary.cell(row=i, column=3).value = missing_count
        ws_summary.cell(row=i, column=4).value = f"={get_column_letter(2)}{i}/{total_files}"

for col in ["A", "B", "C", "D"]:
    ws_summary.column_dimensions[col].width = 25

wb.save(OUTPUT_EXCEL)
print(f"âœ… Excel saved â†’ {OUTPUT_EXCEL}")
print(f"   Rows: {len(df)} | Columns: {len(df.columns)}")






# â”€â”€ Schemas per page type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_schema(document_type: str) -> dict:
    schemas = {
        "ordre_de_virement": {
            "Monnaie": None,
            "Montant en chiffres": None,
            "Montant en lettres": None,
            "Nom/PrÃ©nom": None,
            "Adresse complÃ¨te": None,
            "NumÃ©ro de compte": None,
            "SiÃ¨ge": None,        # 5 digits
            "Racine": None,       # 8 digits
            "Ordinal": None,      # 3 digits
            "Devise": None,       # letters
            "ClÃ©": None,          # 2 digits
            "Adresse SWIFT": None,# letters only
        },
        "annexe_1": {
            "Nom/PrÃ©nom": None,
            "Date de naissance": None,
            "Matricule": None,
            "Grade": None,
            "Fonction": None,
            "Structure": None,
        },
        "annexe_2": {
            "Nom/PrÃ©nom": None,
            "RIB": None,
            "Banque": None,
            "Montant": None,
            "Motif": None,
        },
        "bulletin_de_paie": {
            "Nom/PrÃ©nom": None,
            "Mois": None,
            "Salaire de base": None,
            "Net Ã  payer": None,
            "IndemnitÃ©s": None,
            "Retenues": None,
            "Matricule": None,
        },
        "gestion_des_blocages": {
            "Nom/PrÃ©nom": None,
            "Type de blocage": None,
            "Montant bloquÃ©": None,
            "Date dÃ©but": None,
            "Date fin": None,
            "Motif": None,
        },
    }
    if document_type not in schemas:
        raise ValueError(f"Unknown document type: {document_type}")
    return schemas[document_type]


# Page index â†’ document type mapping
PAGE_TYPES = {
    0: "ordre_de_virement",
    1: "annexe_1",
    2: "annexe_2",
    3: "bulletin_de_paie",
    4: "gestion_des_blocages",
}

# â”€â”€ Prompt builder (per page) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_prompt_for_page(page_text: str, document_type: str) -> str:
    schema = get_schema(document_type)

    hints = ""
    if document_type == "ordre_de_virement":
        hints = """Field format hints:
- SiÃ¨ge: exactly 5 digits
- Racine: exactly 8 digits
- Ordinal: exactly 3 digits
- Devise: letters only
- ClÃ©: exactly 2 digits
- Adresse SWIFT: letters only"""

    return f"""<|im_start|>system
/no think
You are an expert document information extraction system.
The document language may be French, Arabic, or both.
Document type: {document_type.replace('_', ' ').upper()}
Return ONLY valid JSON matching the schema exactly.
{hints}
Do not explain anything. If a field is missing, use null.
<|im_end|>
<|im_start|>user
Schema:
{json.dumps(schema, ensure_ascii=False, indent=2)}

OCR TEXT:
{page_text}
<|im_end|>
<|im_start|>assistant
"""

# â”€â”€ JSON parser (robust) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_json_from_output(text: str) -> dict:
    # Try proper JSON first
    try:
        start = text.index("{")
        end   = text.rindex("}") + 1
        return json.loads(text[start:end])
    except Exception:
        pass

    # Fallback: line-by-line key:value
    data = {}
    for line in text.splitlines():
        line = line.strip().strip(",")
        if ":" in line:
            try:
                key, value = line.split(":", 1)
                key   = key.strip().strip('"')
                value = value.strip().strip('"')
                if key:
                    data[key] = value if value.lower() != "null" else None
            except Exception:
                continue
    return data

# â”€â”€ Load JSON files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

INPUT_FOLDER            = "/mnt/Dossier_agence/json_V2_JANVIER_2026/"
OUTPUT_STRUCTURED_FOLDER = "/mnt/Dossier_agence/structured_V2_JANVIER_2026/"
BATCH_SIZE              = 12
MAX_NEW_TOKENS          = 512   # per page extraction needs less tokens

os.makedirs(OUTPUT_STRUCTURED_FOLDER, exist_ok=True)

all_files = [f for f in os.listdir(INPUT_FOLDER) if f.endswith(".json")]
print(f"Found {len(all_files)} JSON files")

# Load all files: filename â†’ list of page texts
loaded = []
for filename in all_files:
    file_path = os.path.join(INPUT_FOLDER, filename)
    with open(file_path, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)
    loaded.append((filename, ocr_pages))

# â”€â”€ Core batch extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_batch_raw(prompts: list, filenames_meta: list) -> list:
    """
    prompts       : list of prompt strings
    filenames_meta: list of (filename, page_index, document_type)
    returns       : list of (filename, page_index, document_type, extracted_dict)
    """
    inputs = tokenizer(
        prompts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=3072,
    ).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            num_beams=1,
            use_cache=True,
            do_sample=False,
            temperature=None,
            top_p=None,
            pad_token_id=tokenizer.pad_token_id,
        )

    input_len = inputs["input_ids"].shape[1]
    results = []
    for i, (filename, page_idx, doc_type) in enumerate(filenames_meta):
        generated_ids = outputs[i][input_len:]
        raw = tokenizer.decode(generated_ids, skip_special_tokens=True)
        print(f"\nâ”€â”€ [{filename}] page {page_idx+1} ({doc_type}) â”€â”€\n{raw}\n")
        extracted = parse_json_from_output(raw)
        results.append((filename, page_idx, doc_type, extracted))
    return results

# â”€â”€ Main pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

all_rows = []

# Build a flat list of all (filename, page_idx, doc_type, page_text) tasks
all_tasks = []
for filename, ocr_pages in loaded:
    # ocr_pages is a list of dicts with "text" key
    # Normalize: could be list of dicts or a single dict
    if isinstance(ocr_pages, dict):
        ocr_pages = [ocr_pages]

    num_pages = len(ocr_pages)

    for page_idx, doc_type in PAGE_TYPES.items():
        if page_idx < num_pages:
            # Page exists
            page = ocr_pages[page_idx]
            page_text = page.get("text", "") if isinstance(page, dict) else str(page)
            all_tasks.append((filename, page_idx, doc_type, page_text, False))
        else:
            # Page is MISSING
            all_tasks.append((filename, page_idx, doc_type, None, True))

print(f"Total page slots: {len(all_tasks)}")
missing = sum(1 for t in all_tasks if t[4])
print(f"Missing pages:    {missing}")
print(f"Pages to process: {len(all_tasks) - missing}")

# Separate missing (no GPU needed) from real tasks
real_tasks    = [t for t in all_tasks if not t[4]]
missing_tasks = [t for t in all_tasks if t[4]]

# Process real tasks in batches
n_batches = (len(real_tasks) + BATCH_SIZE - 1) // BATCH_SIZE
extraction_results = {}  # (filename, page_idx) â†’ extracted_dict

for i in range(n_batches):
    batch = real_tasks[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]
    prompts       = [build_prompt_for_page(t[3], t[2]) for t in batch]
    filenames_meta = [(t[0], t[1], t[2]) for t in batch]

    print(f"\n[Batch {i+1}/{n_batches}] {len(batch)} pages | {datetime.now()}")
    batch_results = extract_batch_raw(prompts, filenames_meta)

    for filename, page_idx, doc_type, extracted in batch_results:
        extraction_results[(filename, page_idx)] = (doc_type, extracted)

# Add missing pages with null values
for filename, page_idx, doc_type, _, _ in missing_tasks:
    null_schema = {k: None for k in get_schema(doc_type)}
    extraction_results[(filename, page_idx)] = (doc_type, null_schema)

# â”€â”€ Assemble and save one JSON per PDF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Group by filename
from collections import defaultdict
per_file = defaultdict(dict)
for (filename, page_idx), (doc_type, extracted) in extraction_results.items():
    per_file[filename][page_idx] = {
        "page_type": doc_type,
        "page_number": page_idx + 1,
        "missing": extracted == {k: None for k in get_schema(doc_type)},
        "data": extracted,
    }

for filename, pages in per_file.items():
    final_output = {
        "filename": filename,
        "total_pages_found": sum(1 for p in pages.values() if not p["missing"]),
        "pages": {PAGE_TYPES[idx]: pages[idx] for idx in sorted(pages)},
    }

    output_path = os.path.join(OUTPUT_STRUCTURED_FOLDER, f"structured_{filename}")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)
    print(f"âœ“ Saved â†’ {output_path}")

    # Collect flat row for Excel
    row = {"filename": filename}
    for idx, page_data in pages.items():
        prefix = PAGE_TYPES[idx]
        for k, v in page_data["data"].items():
            row[f"{prefix}__{k}"] = v
        if page_data["missing"]:
            row[f"{prefix}__MISSING"] = True
    all_rows.append(row)

print(f"\nâœ… Done! [{datetime.now()}]")
Output structure per PDF
{
  "filename": "dossier_001.json",
  "total_pages_found": 3,
  "pages": {
    "ordre_de_virement": {
      "page_type": "ordre_de_virement",
      "page_number": 1,
      "missing": false,
      "data": { "Monnaie": "DZD", "Montant en chiffres": "50000", ... }
    },
    "annexe_1": { "missing": false, "data": { ... } },
    "annexe_2": { "missing": false, "data": { ... } },
    "bulletin_de_paie": {
      "missing": true,
      "data": { "Nom/PrÃ©nom": null, "Mois": null, ... }
    },
    "gestion_des_blocages": {
      "missing": true,
      "data": { "Nom/PrÃ©nom": null, ... }
    }
  }
}






filename = os.path.splitext(os.path.basename(pdf_path))[0] if os.path.basename(pdf_path).startswith("First_") and pdf_path.lower().endswith(".pdf") else None



import os
import json
import torch
import fitz
import glob
from PIL import Image
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
from transformers import AutoProcessor, AutoModelForVision2Seq

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_PATH     = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_FOLDER     = "/mnt/pdfs"
JSON_SAVE_DIR  = "/mnt/json_output"
MAX_NEW_TOKENS = 1024
BATCH_SIZE     = 16        # tune based on your VRAM
DPI            = 150       # 150 is fine for text, 200 wastes time
NUM_CPU_WORKERS = 8        # for parallel PDFâ†’image conversion

os.makedirs(JSON_SAVE_DIR, exist_ok=True)

# â”€â”€ Load model ONCE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("Loading model...")
processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
model_image = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,
    trust_remote_code=True,
    low_cpu_mem_usage=True,
).eval().to("cuda")
print("Model ready.")


# â”€â”€ PDF â†’ images (in memory, no disk write) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def pdf_to_images_memory(pdf_path, dpi=DPI):
    """Convert PDF pages to PIL images in RAM â€” no disk I/O."""
    doc = fitz.open(pdf_path)
    images = []
    for page in doc:
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        # Pad to square
        max_side = max(img.width, img.height)
        canvas = Image.new("RGB", (max_side, max_side), "white")
        canvas.paste(img, (0, 0))
        images.append(canvas)
    doc.close()
    return images


def convert_one_pdf(pdf_path):
    """Worker function for multiprocessing."""
    filename = os.path.splitext(os.path.basename(pdf_path))[0]
    images   = pdf_to_images_memory(pdf_path)
    return filename, images


# â”€â”€ Prompt builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_image_prompt(image: Image.Image) -> str:
    messages = [{
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text",
             "text": "Extract all visible text from this French document page. Return plain text only."},
        ],
    }]
    return processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )


# â”€â”€ Batch GPU inference â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_text_batch(images: list) -> list:
    """Run a batch of PIL images through the VLM in one GPU call."""
    prompts = [build_image_prompt(img) for img in images]

    inputs = processor(
        text=prompts,
        images=images,
        padding=True,
        return_tensors="pt",
    ).to("cuda")

    with torch.inference_mode():          # faster than no_grad
        outputs = model_image.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            temperature=None,
            top_p=None,
            use_cache=True,
        )

    # Decode only newly generated tokens
    new_ids = outputs[:, inputs["input_ids"].shape[1]:]
    results = processor.batch_decode(new_ids, skip_special_tokens=True)
    return [r.strip() for r in results]


# â”€â”€ Main pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_all_pdfs(pdf_folder, batch_size=BATCH_SIZE):
    pdf_files = glob.glob(os.path.join(pdf_folder, "*.pdf"))
    print(f"Found {len(pdf_files)} PDFs")

    # â”€â”€ Step 1: Convert ALL PDFs to images in parallel (CPU workers) â”€â”€
    print(f"\n[{datetime.now()}] Converting PDFs to images using {NUM_CPU_WORKERS} workers...")
    pdf_image_map = {}   # filename â†’ [PIL images]

    with ProcessPoolExecutor(max_workers=NUM_CPU_WORKERS) as executor:
        futures = {executor.submit(convert_one_pdf, p): p for p in pdf_files}
        for future in as_completed(futures):
            filename, images = future.result()
            pdf_image_map[filename] = images
            print(f"  âœ“ {filename}: {len(images)} page(s)")

    print(f"[{datetime.now()}] All PDFs converted.\n")

    # â”€â”€ Step 2: Flatten into one big list, track which page â†’ which PDF â”€â”€
    all_tasks = []   # list of (filename, page_index, PIL_image)
    for filename, images in pdf_image_map.items():
        for idx, img in enumerate(images):
            all_tasks.append((filename, idx, img))

    print(f"Total pages to process: {len(all_tasks)}")

    # â”€â”€ Step 3: GPU batching over ALL pages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    results_map = {}   # filename â†’ {page_idx: text}

    n_batches = (len(all_tasks) + batch_size - 1) // batch_size
    for i in range(n_batches):
        batch = all_tasks[i * batch_size:(i + 1) * batch_size]
        images_batch = [t[2] for t in batch]

        print(f"[Batch {i+1}/{n_batches}] {len(batch)} pages | {datetime.now()}")
        texts = extract_text_batch(images_batch)

        for (filename, page_idx, _), text in zip(batch, texts):
            results_map.setdefault(filename, {})[page_idx] = text

    # â”€â”€ Step 4: Save JSON per PDF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for filename, pages in results_map.items():
        full_text = "\n".join(pages[idx] for idx in sorted(pages))
        final_output = [{
            "file_name": filename,
            "page": "1",
            "text": full_text,
        }]
        json_path = os.path.join(JSON_SAVE_DIR, f"{filename}.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(final_output, f, indent=4, ensure_ascii=False)
        print(f"  saved â†’ {json_path}")

    print(f"\nâœ… Done! [{datetime.








full_text = "\n".join(
    p["text"] for p in ocr_pages if isinstance(p, dict) and p.get("text")
)



import time

start = time.time()
prompts = [build_prompt(item[1]) for item in batch]
print("Prompt build:", time.time() - start)

start = time.time()
inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=3072).to(model.device)
print("Tokenization:", time.time() - start)

start = time.time()
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=300)
print("Generation:", time.time() - start)




import torch
torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats()




import os
import glob
import json

pdf_files = glob.glob(f"{pdf_folder}/*.pdf")
print(f"Found {len(pdf_files)} PDF files")

for pdf_path in pdf_files:
    file_name = os.path.splitext(os.path.basename(pdf_path))[0]
    print(f"\nProcessing PDF: {file_name}")

    # Convert PDF to images
    images = pdf_to_images(pdf_path, save_dir, dpi=100)
    print(f"{len(images)} pages found. Processing in batches of {BATCH_SIZE}")

    all_text = []  # collect all page texts

    n_batches = (len(images) + BATCH_SIZE - 1) // BATCH_SIZE

    for i in range(n_batches):
        batch_images = images[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]

        print(f"[Batch {i+1}/{n_batches}] Pages {i*BATCH_SIZE+1}-{i*BATCH_SIZE+len(batch_images)}")

        batch_texts = extract_text_batch(batch_images)  # returns list of strings per page

        # Merge text from batch into all_text
        all_text.extend(batch_texts)

    # Merge all pages into ONE text string
    full_text = "\n".join(all_text)

    # Create ONE JSON root object
    final_output = {
        "file_name": file_name,
        "text": full_text
    }

    # Save JSON
    json_path = os.path.join(JSON_SAVE_DIR, f"{file_name}.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)

    print(f"JSON saved: {json_path}")


import os
import glob
import json

pdf_files = glob.glob(f"{pdf_folder}/*.pdf")
print(f"Found {len(pdf_files)} PDF files")

for pdf_path in pdf_files:
    file_name = os.path.splitext(os.path.basename(pdf_path))[0]
    print(f"\nProcessing PDF: {file_name}")

    images = pdf_to_images(pdf_path, save_dir, dpi=100)
    print(f"{len(images)} pages found. Processing in batches of {BATCH_SIZE}")

    pdf_text = []

    n_batches = (len(images) + BATCH_SIZE - 1) // BATCH_SIZE

    for i in range(n_batches):
        batch_images = images[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]
        batch_start_page = i * BATCH_SIZE

        print(f"[Batch {i+1}/{n_batches}] Pages {batch_start_page+1} to {batch_start_page+len(batch_images)}")

        batch_texts = extract_text_batch(batch_images)

        for j, page_text in enumerate(batch_texts):
            page_num = batch_start_page + j + 1

            pdf_text.append({
                "page": page_num,
                "text": page_text
            })

    json_path = os.path.join(JSON_SAVE_DIR, f"{file_name}.json")

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(pdf_text, f, indent=4, ensure_ascii=False)

    print(f"JSON saved: {json_path}")





import subprocess
subprocess.run(["apt-get", "install", "-y", "poppler-utils"])
Or directly:



!conda install -c conda-forge poppler -y



import os
import glob
import json
import torch
from PIL import Image
from pdf2image import convert_from_path
from transformers import AutoProcessor, AutoModelForVision2Seq

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_PATH       = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
pdf_folder       = "./pdfs"
save_dir         = "./images"
JSON_SAVE_DIR    = "./json_output"
MAX_NEW_TOKENS   = 1024
BATCH_SIZE       = 4   # â† images are heavier than text, start at 4, try 6 if no OOM

os.makedirs(save_dir, exist_ok=True)
os.makedirs(JSON_SAVE_DIR, exist_ok=True)

# â”€â”€ Load model & processor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
processor = AutoProcessor.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
)

model_image = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,
    attn_implementation="flash_attention_2",
    trust_remote_code=True,
    low_cpu_mem_usage=True,
)
model_image.eval()
print("Model loaded âœ“")

# â”€â”€ PDF to images (your exact function) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def pdf_to_images(pdf_path: str, save_dir: str, dpi: int = 100):
    images = convert_from_path(pdf_path, dpi=dpi)
    saved = []
    for i, img in enumerate(images):
        img_path = os.path.join(save_dir, f"{os.path.splitext(os.path.basename(pdf_path))[0]}_page{i+1}.png")
        img.save(img_path)
        saved.append(img)
    return saved


# â”€â”€ Build prompt for one image (your exact prompt) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_image_prompt(image: Image.Image) -> dict:
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {
                    "type": "text",
                    "text": "Extract all visible text from this French document page. Return plain text only.",
                },
            ],
        }
    ]
    prompt = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    return prompt


# â”€â”€ Batch extraction (replaces your single extract_text) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_text_batch(images: list) -> list:
    """
    images = list of PIL images
    returns list of extracted text strings
    All images processed in ONE GPU call instead of one-by-one.
    """
    prompts = [build_image_prompt(img) for img in images]

    # Tokenize all images + prompts together
    inputs = processor(
        text=prompts,
        images=images,
        padding=True,
        return_tensors="pt",
    )

    # Move all tensors to GPU
    inputs = inputs.to("cuda")

    with torch.no_grad():
        outputs = model_image.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            temperature=None,
            top_p=None,
            use_cache=True,
        )

    # Decode only newly generated tokens â€” your exact logic
    generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
    results = processor.batch_decode(
        generated_ids,
        skip_special_tokens=True,
    )

    return [r.strip() for r in results]


# â”€â”€ Process all PDFs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pdf_files = glob.glob(f"{pdf_folder}/*.pdf")
print(f"Found {len(pdf_files)} PDF files")

for pdf_path in pdf_files:
    file_name = os.path.splitext(os.path.basename(pdf_path))[0]
    print(f"\nProcessing PDF: {file_name}")

    # Convert PDF to images â€” your exact logic
    images = pdf_to_images(pdf_path, save_dir, dpi=100)
    print(f"  {len(images)} pages found â€” processing in batches of {BATCH_SIZE}")

    pdf_text = []

    # Process pages in batches instead of one by one
    n_batches = (len(images) + BATCH_SIZE - 1) // BATCH_SIZE

    for i in range(n_batches):
        batch_images = images[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]
        batch_start_page = i * BATCH_SIZE

        print(f"  [Batch {i+1}/{n_batches}] pages {batch_start_page+1}â€“{batch_start_page+len(batch_images)}")

        # Extract text from all images in batch â€” ONE GPU call
        batch_texts = extract_text_batch(batch_images)

        for j, page_text in enumerate(batch_texts):
            page_num = batch_start_page + j + 1
            pdf_text.append({
                "page": page_num,
                "text": page_text,
            })

    # Save ONE JSON for THIS PDF â€” your exact logic
    json_path = os.path.join(JSON_SAVE_DIR, f"{file_name}.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(pdf_text, f, indent=4, ensure_ascii=False)

    print(f"  JSON saved: {json_path}")

print("\nâœ… All PDFs processed!")




import os
import json
import torch
from datetime import datetime
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model_name = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
INPUT_FOLDER = "./input_jsons"
OUTPUT_STRUCTURED_FOLDER = "./output_structured"
BATCH_SIZE = 8  # â† lower to 4 if OOM, raise to 12 if GPU has room

os.makedirs(OUTPUT_STRUCTURED_FOLDER, exist_ok=True)

# â”€â”€ Load model & tokenizer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True,
    padding_side="left",  # REQUIRED for batching
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    trust_remote_code=True,
    low_cpu_mem_usage=True,
)
model.eval()

# â”€â”€ Schema â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_schema(document_type: str) -> dict:
    if document_type == "bank":
        return {
            "Monnaie": None,
            "Montant en chiffres": None,
            "Montant en lettres": None,
            "Agence": None,
            "Nom/PrÃ©nom": None,
            "Nature du paiement": None,
            "Adresse complÃ¨te": None,
            "NumÃ©ro de compte": None,
            "Date de valeur": None,
            "SiÃ¨ge": None,
            "Racine": None,
            "Ordinal": None,
            "Devise": None,
            "CL": None,
            "Adresse SWIFT": None,
            "Autre (Transfert en FUBO)": None,
        }
    else:
        raise ValueError(f"Unknown document type: {document_type}")


# â”€â”€ Parse JSON from model output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_json_from_output(text: str) -> dict:
    data = {}
    lines = text.splitlines()
    for line in lines:
        line = line.strip()
        if ":" in line:
            try:
                key, value = line.split(":", 1)
                key = key.strip().strip('"')
                value = value.strip().strip('"').strip(",")
                if key:
                    data[key] = value
            except:
                continue
    return data


# â”€â”€ Build prompt (your exact prompt) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_prompt(ocr_pages: list) -> str:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))
    schema = get_schema(document_type="bank")

    return f"""<|im_start|>system
/no_think
You are an expert document information extraction system.
The document language may be French, Arabic, or both.
Return ONLY valid JSON matching the schema exactly.
just to let you know in the ordre de virement the SiÃ¨ge contain 5 number and the Racine contain 6 and the ordinal 3 numbers and the devise letters and the cle 2 numbers,
and Adresse SWIFT contain just letters
Do not explain anything. If a field is missing, use null.
<|im_end|>
<|im_start|>user
Schema:
{json.dumps(schema, ensure_ascii=False, indent=2)}

OCR TEXT:
{full_text}
<|im_end|>
<|im_start|>assistant
"""


# â”€â”€ Batch extraction (replaces your single extract_structured_data) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_structured_data_batch(batch: list) -> list:
    """
    batch = list of (filename, ocr_pages)
    returns list of (filename, extracted_dict)
    All files in the batch are processed in ONE GPU call instead of one-by-one.
    """
    filenames = [item[0] for item in batch]
    prompts   = [build_prompt(item[1]) for item in batch]

    # Tokenize all prompts together with padding
    inputs = tokenizer(
        prompts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=3072,
    ).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=220,
            num_beams=1,
            use_cache=True,
            do_sample=False,
            temperature=None,
            top_p=None,
            pad_token_id=tokenizer.pad_token_id,
        )

    # Decode only newly generated tokens â€” your exact logic
    input_len = inputs["input_ids"].shape[1]
    results = []
    for i, filename in enumerate(filenames):
        generated_ids = outputs[i][input_len:]
        result = tokenizer.decode(generated_ids, skip_special_tokens=True)
        print(f"\n--- Raw model output [{filename}] ---\n{result}\n---\n")
        extracted = parse_json_from_output(result)
        results.append((filename, extracted))

    return results


# â”€â”€ Load all JSON files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all_files = [f for f in os.listdir(INPUT_FOLDER) if f.endswith(".json")]
print(f"Found {len(all_files)} JSON files â€” processing in batches of {BATCH_SIZE}")

loaded = []
for filename in all_files:
    file_path = os.path.join(INPUT_FOLDER, filename)
    with open(file_path, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)
    loaded.append((filename, ocr_pages))

# â”€â”€ Process in batches â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all_rows = []
n_batches = (len(loaded) + BATCH_SIZE - 1) // BATCH_SIZE

for i in range(n_batches):
    batch = loaded[i * BATCH_SIZE : (i + 1) * BATCH_SIZE]
    print(f"\n[Batch {i + 1}/{n_batches}] Processing: {[b[0] for b in batch]}")

    batch_results = extract_structured_data_batch(batch)

    for filename, extracted_values in batch_results:
        print(json.dumps(extracted_values, indent=2, ensure_ascii=False))

        # Create output file path â€” your exact logic
        output_file_path = os.path.join(
            OUTPUT_STRUCTURED_FOLDER,
            f"structured_{filename}"
        )

        # Save JSON inside folder â€” your exact logic
        with open(output_file_path, "w", encoding="utf-8") as f:
            json.dump(extracted_values, f, indent=4, ensure_ascii=False)

        print(datetime.now())
        print(f"\n Done! Structured data saved to {output_file_path}")

        # Collect for Excel
        row = {"filename": filename}
        row.update(extracted_values)
        all_rows.append(row)

# â”€â”€ Export to Excel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df = pd.DataFrame(all_rows)
excel_path = os.path.join(OUTPUT_STRUCTURED_FOLDER, "I25.xlsx")
df.to_excel(excel_path, index=False)
print(f"\nâœ… All done! Excel saved to: {excel_path}")







Bonjour [Nom du manager],

Veuillez trouver ci-joint lâ€™output final du modÃ¨le OCR concernant le Lot 2 â€“ DÃ©cembre.

Vous trouverez Ã©galement ci-dessous le lien vers le fichier de sortie complet :
[Lien vers lâ€™output]

Nâ€™hÃ©sitez pas Ã  me faire part de vos retours ou de toute remarque Ã©ventuelle.

Cordialement,
Nihad


import os
import json
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM

# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model_name = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
INPUT_FOLDER = "./input_jsons"
JSON_SAVE_DIR = "./output_structured"
os.makedirs(JSON_SAVE_DIR, exist_ok=True)

# â”€â”€ Load model & tokenizer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    attn_implementation="flash_attention_2",
    low_cpu_mem_usage=True,
)

# â”€â”€ Schema per document type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_schema(document_type: str) -> dict:
    if document_type == "bank":
        return {
            "Monnaie": None,
            "Montant en chiffres": None,
            "Montant en lettres": None,
            "Agence": None,
            "Nom/PrÃ©nom": None,
            "Nature du paiement": None,
            "Adresse complÃ¨te": None,
            "NumÃ©ro de compte": None,
            "Date de valeur": None,
            "SiÃ¨ge": None,
            "Racine": None,
            "Ordinal": None,
            "Devise": None,
            "CL": None,
            "Adresse SWIFT": None,
            "Autre (Transfert en FUBO)": None,
        }
    else:
        raise ValueError(f"Unknown document type: {document_type}")


# â”€â”€ Parse JSON from model output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_json_from_output(text: str) -> dict:
    data = {}
    lines = text.splitlines()
    for line in lines:
        line = line.strip()
        if ":" in line:
            try:
                key, value = line.split(":", 1)
                key = key.strip().strip('"')
                value = value.strip().strip('"').strip(",")
                if key:
                    data[key] = value
            except:
                continue
    return data


# â”€â”€ Main extraction function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))
    schema = get_schema(document_type="bank")

    prompt = f"""<|im_start|>system
You are an expert document information extraction system.
The document language may be French, Arabic, or both.
Return ONLY valid JSON matching the schema exactly.
just to let you know in the ordre de virement the Siege contain 5 number and the Racine contain 6 and the ordinal 3 numbers and the devise letters and the cle 2 numbers,
and Adresse SWIFT contain just letters
Do not explain anything. If a field is missing, use null.
<|im_end|>
<|im_start|>user
{json.dumps(schema, ensure_ascii=False, indent=2)}

Schema:

OCR TEXT:
{full_text}
<|im_end|>
<|im_start|>assistant
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=220,
            num_beams=1,
            use_cache=True,
            do_sample=False,
            temperature=None,
            top_p=None,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Decode only newly generated tokens
    generated_ids = outputs[0][inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_ids, skip_special_tokens=True)

    print(f"\n--- Raw model output ---\n{result}\n")

    return parse_json_from_output(result)


# â”€â”€ Process all JSON files in folder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all_rows = []

for filename in os.listdir(INPUT_FOLDER):
    if filename.endswith(".json"):
        file_path = os.path.join(INPUT_FOLDER, filename)

        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        extracted_values = extract_structured_data(ocr_pages)

        print(json.dumps(extracted_values, indent=2, ensure_ascii=False))

        # Save individual structured JSON
        output_file_path = os.path.join(JSON_SAVE_DIR, f"structured_{filename}")
        with open(output_file_path, "w", encoding="utf-8") as f:
            json.dump(extracted_values, f, indent=4, ensure_ascii=False)

        print(f"\n Done! Structured data saved to {output_file_path}")

        # Collect row for Excel
        row = {"filename": filename}
        row.update(extracted_values)
        all_rows.append(row)

# â”€â”€ Export to Excel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df = pd.DataFrame(all_rows)
excel_path = os.path.join(JSON_SAVE_DIR, "I25.xlsx")
df.to_excel(excel_path, index=False)
print(f"\nâœ… All done! Excel saved to: {excel_path}")





# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model_name   = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
INPUT_FOLDER = "input_jsons"
OUTPUT_FOLDER = "output_structured"
BATCH_SIZE   = 8   # process 8 files at once â€” tune up/down based on VRAM
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

# â”€â”€ Load model (once) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.padding_side = "left"  # required for batch generation

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    attn_implementation="flash_attention_2",
    low_cpu_mem_usage=True,
    device_map="auto"
)
model.eval()
print("Model loaded âœ“")

# â”€â”€ Schema â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCHEMA = {
    "Monnaie": None, "Montant en chiffres": None, "Montant en lettres": None,
    "Agence": None, "Nom/PrÃ©nom": None, "Nature du paiement": None,
    "Adresse complÃ¨te": None, "NumÃ©ro de compte": None, "Date de valeur": None,
    "SiÃ¨ge": None, "Racine": None, "Ordinal": None, "Devise": None,
    "CL": None, "Adresse SWIFT": None, "Autre (Transfert en FUBO)": None
}

SCHEMA_STR = json.dumps(SCHEMA, ensure_ascii=False, indent=2)

SYSTEM_PROMPT = """/no_think
You are an expert document extraction system. Extract fields from French/Arabic bank documents.
Rules:
- SiÃ¨ge = 5 digits, Racine = 6 digits, Ordinal = 3 digits, Devise = letters only, CL = 2 digits, Adresse SWIFT = letters only
- Return ONLY a valid JSON object matching the schema. No explanation, no markdown, no extra text.
- Use null for missing fields."""

# â”€â”€ Build prompt for one file â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_prompt(ocr_pages: list) -> str:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user",   "content": f"Schema:\n{SCHEMA_STR}\n\nOCR TEXT:\n{full_text}"}
    ]
    return tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )

# â”€â”€ Parse model output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_output(text: str) -> dict:
    # Try clean JSON parse first
    try:
        start = text.find("{")
        end   = text.rfind("}") + 1
        if start != -1 and end > start:
            return json.loads(text[start:end])
    except json.JSONDecodeError:
        pass

    # Fallback: line-by-line key:value parsing
    data = {}
    for line in text.splitlines():
        line = line.strip().strip(",")
        if ":" in line:
            try:
                key, value = line.split(":", 1)
                key   = key.strip().strip('"')
                value = value.strip().strip('"')
                if key:
                    data[key] = None if value.lower() in ("null", "none", "") else value
            except:
                continue
    return data

# â”€â”€ Batch inference â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_batch(batch: list[tuple[str, list]]) -> list[tuple[str, dict]]:
    """batch = list of (filename, ocr_pages)"""
    filenames = [item[0] for item in batch]
    prompts   = [build_prompt(item[1]) for item in batch]

    # Tokenize all prompts together (padded)
    inputs = tokenizer(
        prompts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=3072
    ).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=300,      # extraction doesn't need more
            num_beams=1,             # greedy â€” fastest
            do_sample=False,
            temperature=None,
            top_p=None,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
        )

    results = []
    input_len = inputs["input_ids"].shape[1]
    for i, filename in enumerate(filenames):
        generated = outputs[i][input_len:]
        text = tokenizer.decode(generated, skip_special_tokens=True)
        parsed = parse_output(text)
        results.append((filename, parsed))
        print(f"  âœ“ {filename}")

    return results

# â”€â”€ Main loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_json_file(filepath: str):
    with open(filepath, "r", encoding="utf-8") as f:
        return json.load(f)

# Collect all files
all_files = [f for f in os.listdir(INPUT_FOLDER) if f.endswith(".json")]
print(f"Found {len(all_files)} JSON files. Processing in batches of {BATCH_SIZE}...")

# Pre-load all JSONs in parallel (I/O bound â€” use threads)
filepaths = [(f, os.path.join(INPUT_FOLDER, f)) for f in all_files]
with ThreadPoolExecutor(max_workers=8) as executor:
    loaded = list(executor.map(lambda x: (x[0], load_json_file(x[1])), filepaths))

print(f"All files loaded âœ“")

# Process in batches
all_rows = []
for i in range(0, len(loaded), BATCH_SIZE):
    batch = loaded[i:i+BATCH_SIZE]
    print(f"\nBatch {i//BATCH_SIZE + 1}/{(len(loaded)-1)//BATCH_SIZE + 1} â€” files {i+1}â€“{min(i+BATCH_SIZE, len(loaded))}")
    
    batch_results = process_batch(batch)
    
    for filename, extracted in batch_results:
        # Save individual JSON
        out_path = os.path.join(OUTPUT_FOLDER, f"structured_{filename}")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(extracted, f, indent=4, ensure_ascii=False)
        
        # Collect for Excel
        row = {"filename": filename}
        row.update(extracted)
        all_rows.append(row)

# â”€â”€ Export to Excel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df = pd.DataFrame(all_rows)
excel_path = os.path.join(OUTPUT_FOLDER, "all_extractions.xlsx")
df.to_excel(excel_path, index=False)
print(f"\nâœ… Done! {len(all_rows)} files processed.")
print(f"ðŸ“Š Excel saved to: {excel_path




attn_implementation="flash_attention_2"



Bonjour,

Comme convenu avec vous, je vous informe des heures supplÃ©mentaires effectuÃ©es :

- Jeudi : de 19h Ã  23h
- Vendredi : de 9h30 Ã  13h00
- Samedi : de 10h00 Ã  22h30

Je reste Ã  votre disposition si vous avez besoin de plus dâ€™informations.

Cordialement,
Nihad Senhadji




import os
import json

for filename in os.listdir(INPUT_FOLDER):

    file_path = os.path.join(INPUT_FOLDER, filename)

    with open(file_path, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Processing: {filename}")

    extracted_values = extract_structured_data(ocr_pages)

    if not extracted_values:
        print("âš ï¸ Empty extraction for:", filename)
        continue

    output_file_path = os.path.join(
        OUTPUT_STRUCTURED_FOLDER,
        f"structured_{filename}"
    )

    with open(output_file_path, "w", encoding="utf-8") as f:
        json.dump(extracted_values, f, indent=4, ensure_ascii=False)

    print("âœ… Done! Saved to:", output_file_path)



import os; print(len([f for f in os.listdir("your/folder/path") if f.lower().endswith(".pdf")]))



import os
from PyPDF2 import PdfReader, PdfWriter

# Input folder containing PDFs
input_folder = "input_pdfs"

# Output folder for first pages
output_folder = "first_pages"
os.makedirs(output_folder, exist_ok=True)

# Loop through all files in folder
for filename in os.listdir(input_folder):
    if filename.lower().endswith(".pdf"):
        input_path = os.path.join(input_folder, filename)
        
        # Read PDF
        reader = PdfReader(input_path)
        
        # Skip empty PDFs
        if len(reader.pages) == 0:
            continue
        
        # Create writer and add only first page
        writer = PdfWriter()
        writer.add_page(reader.pages[0])
        
        # Save new PDF
        output_path = os.path.join(output_folder, f"first_{filename}")
        with open(output_path, "wb") as output_pdf:
            writer.write(output_pdf)

print("Done! First pages extracted.")




import os
import pandas as pd

# Folder containing your "JSON" files
json_folder = "structured_json_folder"
output_file = "structured_data.xlsx"

# Columns in desired order
columns_order = [
    "Monnaie",
    "Montant en chiffres",
    "Montant en lettres",
    "Nom/PrÃ©nom",
    "Adresse complÃ¨te",
    "NumÃ©ro de compte",
    "Adresse SWIFT",
    "source_file"  # NEW column for filename
]

all_data = []

for filename in os.listdir(json_folder):
    if filename.startswith("structure_") and filename.endswith(".json"):
        path = os.path.join(json_folder, filename)
        with open(path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        row = {col: "" for col in columns_order}  # initialize empty row
        row["source_file"] = filename  # add filename

        for line in lines:
            line = line.strip()
            if not line:
                continue

            for col in columns_order[:-1]:  # skip 'source_file'
                if line.startswith(col):
                    value = line[len(col):].strip()
                    value = value.strip('"')
                    if col == "Montant en chiffres":
                        value = value.replace(",", ".")
                    row[col] = value

        all_data.append(row)

# Convert to DataFrame
df = pd.DataFrame(all_data, columns=columns_order)

# Save to Excel
df.to_excel(output_file, index=False, engine="openpyxl")

print("âœ… Excel file created successfully with filenames!")




import os
import pandas as pd

# Folder containing your "JSON" files
json_folder = "structured_json_folder"
output_file = "structured_data.xlsx"

# Columns in desired order
columns_order = [
    "Monnaie",
    "Montant en chiffres",
    "Montant en lettres",
    "Nom/PrÃ©nom",
    "Adresse complÃ¨te",
    "NumÃ©ro de compte",
    "Adresse SWIFT"
]

all_data = []

for filename in os.listdir(json_folder):
    if filename.startswith("structure_") and filename.endswith(".json"):
        path = os.path.join(json_folder, filename)
        with open(path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        row = {col: "" for col in columns_order}  # initialize empty row

        for line in lines:
            line = line.strip()
            # Skip empty lines
            if not line:
                continue

            for col in columns_order:
                # If line starts with the column name
                if line.startswith(col):
                    # Remove the column name from the line
                    value = line[len(col):].strip()
                    # Remove surrounding quotes if any
                    value = value.strip('"')
                    # Fix comma decimal to dot
                    if col == "Montant en chiffres":
                        value = value.replace(",", ".")
                    row[col] = value

        all_data.append(row)

# Convert to DataFrame
df = pd.DataFrame(all_data, columns=columns_order)

# Save to Excel
df.to_excel(output_file, index=False, engine="openpyxl")

print("âœ… Excel file created successfully!")




import os
import json
import re
import pandas as pd

json_folder = "structured_json_folder"
output_file = "structured_data.xlsx"

# Desired columns and order
columns_order = [
    "Monnaie",
    "Montant en chiffres",
    "Montant en lettres",
    "Nom/PrÃ©nom",
    "Adresse complÃ¨te",
    "NumÃ©ro de compte",
    "Adresse SWIFT"
]

all_data = []

def extract_json(text):
    """
    Extract valid JSON object from messy text.
    """
    match = re.search(r'\{.*\}', text, re.DOTALL)
    if match:
        json_text = match.group()
        # Fix broken quotes
        json_text = json_text.replace('\\"', '"').replace('""', '"')
        try:
            return json.loads(json_text)
        except:
            return None
    return None

for filename in os.listdir(json_folder):
    if filename.startswith("structure_") and filename.endswith(".json"):
        path = os.path.join(json_folder, filename)
        with open(path, "r", encoding="utf-8") as f:
            raw = f.read()

        data = extract_json(raw)
        if data:
            # Keep only desired columns
            row = {col: data.get(col, "") for col in columns_order}
            all_data.append(row)

# Convert to DataFrame
df = pd.DataFrame(all_data, columns=columns_order)

# Save to Excel
df.to_excel(output_file, index=False, engine="openpyxl")

print("âœ… Excel file created successfully!")










import os
import json
import pandas as pd
from openpyxl import Workbook

# Path to your JSON folder
json_folder = "json_folder"

# List to store all JSON data
all_data = []

# Loop through files
for filename in os.listdir(json_folder):
    if filename.startswith("structure_") and filename.endswith(".json"):
        file_path = os.path.join(json_folder, filename)
        
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            all_data.append(data)

# Convert to DataFrame
df = pd.DataFrame(all_data)

# Save to Excel using openpyxl
output_file = "output.xlsx"
df.to_excel(output_file, index=False, engine="openpyxl")

print("Excel file created successfully!")





import glob
import os
import json
import pandas as pd
from datetime import datetime

STRUCTURED_FOLDER = "Dossier_agence/structured_json"
OUTPUT_EXCEL = "Dossier_agence/Final_output.xlsx"

print("Start:", datetime.now())

json_files = glob.glob(os.path.join(STRUCTURED_FOLDER, "*.json"))
print(f"Found {len(json_files)} structured JSON files.")

all_rows = []

for json_path in json_files:

    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    file_name = os.path.basename(json_path).replace("structured_", "").replace(".json", "")

    # If single dictionary
    if isinstance(data, dict):
        data["File Name"] = file_name
        all_rows.append(data)

    # If list of dictionaries
    elif isinstance(data, list):
        for item in data:
            if isinstance(item, dict):
                item["File Name"] = file_name
                all_rows.append(item)

df = pd.DataFrame(all_rows)

df.to_excel(OUTPUT_EXCEL, index=False)

print(f"Excel file created at: {OUTPUT_EXCEL}")
print("Finished:", datetime.now())





import glob
import os
import json
import pandas as pd
from datetime import datetime

print("Start:", datetime.now())

json_files = glob.glob(os.path.join(STRUCTURED_FOLDER, "*.json"))
print(f"Found {len(json_files)} structured JSON files.")

all_rows = []

for json_path in json_files:

    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    file_name = os.path.basename(json_path).replace("structured_", "").replace(".json", "")

    # âœ… If JSON is a dictionary
    if isinstance(data, dict):
        data["File Name"] = file_name
        all_rows.append(data)

    # âœ… If JSON is a list of dictionaries
    elif isinstance(data, list):
        for item in data:
            if isinstance(item, dict):
                item["File Name"] = file_name
                all_rows.append(item)

df = pd.DataFrame(all_rows)

df.to_excel(OUTPUT_EXCEL, index=False)

print(f"Excel file created at: {OUTPUT_EXCEL}")
print("Finished:", datetime.now())




.def parse_json_from_output(text: str) -> dict:
    data = {}

    lines = text.splitlines()

    for line in lines:
        line = line.strip()

        if ":" in line:
            try:
                key, value = line.split(":", 1)

                key = key.strip().strip('"')
                value = value.strip().strip('"').strip(",")

                if key:
                    data[key] = value

            except:
                continue

    return data




import json
import re

def parse_json_from_output(text: str) -> dict:

    text = text.strip()

    # Wrap with braces if missing
    if not text.startswith("{"):
        text = "{" + text

    if not text.endswith("}"):
        text = text.rstrip(",") + "}"

    # Remove trailing commas before }
    text = re.sub(r",\s*}", "}", text)

    try:
        return json.loads(text)
    except json.JSONDecodeError as e:
        print("==== BROKEN JSON ====")
        print(text)
        print("=====================")
        raise e







import json
import re

def parse_json_from_output(text: str) -> dict:

    # Remove assistant prefix if exists
    if "<|im_start|>assistant" in text:
        text = text.split("<|im_start|>assistant")[-1]

    text = text.strip()

    # Try to extract JSON block
    json_match = re.search(r"\{.*\}", text, re.DOTALL)
    if json_match:
        return json.loads(json_match.group(0))

    # ðŸ”¥ If no braces but looks like JSON fields â†’ wrap it
    if ":" in text and "{" not in text:
        cleaned = "{" + text.strip().rstrip(",") + "}"
        return json.loads(cleaned)

    raise ValueError(f"No valid JSON found in model output:\n{text[:500]}")









import os
import json

os.makedirs(OUTPUT_STRUCTURED_FOLDER, exist_ok=True)

all_rows = []

for filename in os.listdir(INPUT_FOLDER):

    if filename.endswith(".json"):

        file_path = os.path.join(INPUT_FOLDER, filename)

        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        extracted_values = extract_structured_data(ocr_pages)

        print(json.dumps(extracted_values, indent=2, ensure_ascii=False))

        # âœ… Create output file path (important fix)
        output_file_path = os.path.join(
            OUTPUT_STRUCTURED_FOLDER,
            f"structured_{filename}"
        )

        # âœ… Save JSON inside folder (not the folder itself)
        with open(output_file_path, "w", encoding="utf-8") as f:
            json.dump(extracted_values, f, indent=4, ensure_ascii=False)

        print(f"Done! Structured data saved to {output_file_path}")






structured_list = []

for item in extracted_values:
    parts = item.split("\n")
    data_dict = {}
    i = 0
    while i < len(parts):
        key = parts[i].strip()
        if key in fields:
            # The next line is the value
            if i + 1 < len(parts):
                value = parts[i + 1].strip()
                data_dict[key] = value
                i += 2
            else:
                i += 1
        else:
            i += 1
    structured_list.append(data_dict)

# Save as JSON file
with open("structured_output.json", "w", encoding="utf-8") as f:
    json.dump(structured_list, f, indent=2, ensure_ascii=False)

print("JSON file saved successfully!")





def map_values_to_fields(raw_text: str):
    values = [v.strip() if v.strip() != "" else None for v in raw_text.split("I")]
    structured_dict = {field: values[i] if i < len(values) else None for i, field in enumerate(BANK_FIELDS)}
    return structured_dict









import os
import json
import re

INPUT_FOLDER = "/mnt/Dossier_agence/json_first_page"
OUTPUT_STRUCTURED_FOLDER = "/mnt/Dossier_agence/structured_json"
os.makedirs(OUTPUT_STRUCTURED_FOLDER, exist_ok=True)

def parse_extracted_text(text):
    """
    Convert messy extracted text into a structured dictionary.
    """
    structured_dict = {}

    # Fix common JSON-like parts
    text = text.replace('\n', ' ').replace('"', '')
    
    # Extract key-value pairs like 'Key: Value'
    pattern = r'(\b[A-Za-zÃ©Ã¨Ã§Ã Ã‰ÃˆÃ‡Ã€ ]+)\s*[:]\s*([^:]+?)(?=\s+[A-Za-zÃ©Ã¨Ã§Ã Ã‰ÃˆÃ‡Ã€ ]+[:]|$)'
    matches = re.findall(pattern, text)
    
    for key, value in matches:
        structured_dict[key.strip()] = value.strip()

    return structured_dict

for filename in os.listdir(INPUT_FOLDER):
    if filename.endswith(".json"):
        file_path = os.path.join(INPUT_FOLDER, filename)
        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        # Extracted text from your tokenizer function
        extracted_text = extract_structured_data(ocr_pages)

        if not extracted_text:
            print(f"No data extracted from {filename}")
            continue

        # Parse messy text to structured dictionary
        structured_dict = parse_extracted_text(extracted_text)

        # Save as JSON
        structured_file_path = os.path.join(
            OUTPUT_STRUCTURED_FOLDER, f"{os.path.splitext(filename)[0]}_structured.json"
        )
        with open(structured_file_path, "w", encoding="utf-8") as sf:
            json.dump(structured_dict, sf, ensure_ascii=False, indent=4)

print("All structured JSON files saved!")












structured_dict = {}
        # This regex splits lines like 'Key: Value'
        for line in extracted_text.split("\n"):
            if ":" in line:
                key, value = line.split(":", 1)
                structured_dict[key.strip()] = value.strip()

        # Save as JSON
        structured_file_path = os.path.join(
            OUTPUT_STRUCTURED_FOLDER, f"{os.path.splitext(filename)[0]}_structured.json"
        )
        with open(structured_file_path, "w", encoding="utf-8") as sf:
            json.dump(structured_dict, sf, ensure_ascii=False, indent=4)




file
        structured_file_path = os.path.join(
            OUTPUT_STRUCTURED_FOLDER, f"{os.path.splitext(filename)[0]}_structured.json"
        )
        with open(structured_file_path, "w", encoding="utf-8") as sf:
            json.dump(extracted_values, sf, ensure_ascii=False, indent=4)




{
  "Monnaie": "DZD",
  "Montant en chiffres": "318,060.13",
  "Montant en lettres": "trois cent dix-huit mille soixante virgule treize Dinars",
  "Agence": "MOSTAGANEM",
  "Nom/PrÃ©nom": "ALI SAYED HASMAT",
  "Adresse complÃ¨te": "ARATI, PATSAWRA, PASCHIM BARDHAMAN, DURGAPUR, WEST BENGAL PIN - 713363",
  "NumÃ©ro de compte": "50100692108329",
  "Date de valeur": "05/02/2026",
  "bank code": "073",
  "SiÃ¨ge": "027",
  "Racine": "007",
  "Ordinal": "310",
  "Devise": "DZD",
  "ClÃ©": "110526",
  "Adresse SWIFT": "HDFC INBB",
  "Autre (Transfert en EURO)": null
}





import json
import os
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

INPUT_FOLDER = "/mnt/Dossier_agence/json_first_page"
OUTPUT_STRUCTURED_FOLDER = "/mnt/Dossier_agence/structured_json"

BANK_FIELDS = [
    "Monnaie", "Montant en chiffres", "Montant en lettres", "Agence",
    "Nom/PrÃ©nom", "Adresse complÃ¨te", "NumÃ©ro de compte", "Date de valeur",
    "bank code", "SiÃ¨ge", "Racine", "Ordinal", "Devise", "ClÃ©",
    "Adresse SWIFT", "Autre (Transfert en EURO)",
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOAD MODEL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("âœ“ Model loaded\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FIXED PROMPT (DON'T SHOW FIELD NAMES)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def build_prompt(text: str) -> str:
    """Build prompt WITHOUT showing field names to avoid confusion"""
    
    prompt = f"""Extract bank transfer information from this document.

Return values in this EXACT order, separated by "|":
1. Currency (Monnaie)
2. Amount in numbers
3. Amount in words
4. Agency name
5. Beneficiary name
6. Complete address
7. Account number
8. Value date
9. Bank code (3 digits)
10. SiÃ¨ge (5 digits)
11. Racine (6 digits)
12. Ordinal (3 digits)
13. Devise (letters)
14. ClÃ© (2 digits)
15. SWIFT address
16. Other (Transfer in EURO)

Rules:
- Return ONLY the values separated by "|"
- Use "null" if a field is missing
- No field names, no explanations

Example format:
DZD | 318060.13 | trois cent dix-huit mille... | MOSTAGANEM | ALI SAYED HASMAT | ARATI... | 50100692 | 05/02/2026 | 073 | 10110 | 526001 | 261 | DZD | 52 | HDFC INBB | null

Document text:
{text[:700]}
"""
    return prompt.strip()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXTRACTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def extract_structured_data(ocr_pages: list) -> dict:
    """Extract from OCR pages"""
    # Get text
    if isinstance(ocr_pages, dict):
        full_text = ocr_pages.get("text", "")
    else:
        full_text = "\n".join(p.get("text", "") for p in ocr_pages if isinstance(p, dict) and p.get("text"))
    
    if not full_text.strip():
        raise ValueError("No text found")
    
    # Build prompt (NO field names shown)
    prompt = build_prompt(full_text)
    
    # Generate
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1500).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    # Decode
    result = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    # Parse
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    structured = {}
    for i, field in enumerate(BANK_FIELDS):
        if i < len(values):
            val = values[i]
            structured[field] = None if val.lower() == "null" or val == "" else val
        else:
            structured[field] = None
    
    return structured

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BATCH PROCESS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
os.makedirs(OUTPUT_STRUCTURED_FOLDER, exist_ok=True)

print(f"Processing files from: {INPUT_FOLDER}")
print(f"Start: {datetime.now()}\n")

success = 0
errors = 0

for filename in os.listdir(INPUT_FOLDER):
    if filename.endswith(".json"):
        file_path = os.path.join(INPUT_FOLDER, filename)
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                ocr_pages = json.load(f)
            
            print(f"Processing: {filename}...", end=" ")
            
            # Extract
            extracted = extract_structured_data(ocr_pages)
            
            # Save
            output_file = filename.replace(".json", "_structured.json")
            output_path = os.path.join(OUTPUT_STRUCTURED_FOLDER, output_file)
            
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(extracted, f, indent=4, ensure_ascii=False)
            
            print("âœ“")
            success += 1
            
        except Exception as e:
            print(f"âœ— Error: {e}")
            errors += 1

print(f"\nFinished: {datetime.now()}")
print(f"Success: {success} | Errors: {errors}")





extracted_values = extract_structured_data(ocr_pages)

# Clean split and strip
if isinstance(extracted_values, str):
    values = [v.strip() for v in extracted_values.split("|")]
else:
    values = [v.strip() for v in extracted_values]

# Safety: fix length mismatch
if len(values) != len(BANK_FIELDS):
    values += ["null"] * (len(BANK_FIELDS) - len(values))





def build_prompt(text: str) -> str:
    order = " | ".join(BANK_FIELDS)

    prompt = f"""
You are an expert document information extraction system.

Extract the following fields in EXACT order separated by "|":

{order}

Rules for bank documents:
- bank code = 3 numbers
- SiÃ¨ge = 5 numbers
- Racine = 6 numbers
- Ordinal = 3 numbers
- Devise = letters
- ClÃ© = 2 numbers

Return ONLY values separated by "|".
If a value is missing, return null.
No explanations, no headers.

OCR:
{text}
"""
    return prompt.strip()








INPUT_FOLDER = "/mnt/json_folder"
OUTPUT_STRUCTURED_FOLDER = "/mnt/structured_output"
OUTPUT_EXCEL = "/mnt/final_output.xlsx"

os.makedirs(OUTPUT_STRUCTURED_FOLDER, exist_ok=True)

all_rows = []

for filename in os.listdir(INPUT_FOLDER):

    if filename.endswith(".json"):

        file_path = os.path.join(INPUT_FOLDER, filename)

        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        extracted_values = extract_structured_data(ocr_pages)

        row_dict = dict(zip(BANK_FIELDS, extracted_values))
        row_dict["source_file"] = filename

        # âœ… Save individual structured JSON
        output_json_path = os.path.join(
            OUTPUT_STRUCTURED_FOLDER,
            filename.replace(".json", "_structured.json")
        )

        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(row_dict, f, indent=2, ensure_ascii=False)

        all_rows.append(row_dict)

# âœ… Save global Excel
df = pd.DataFrame(all_rows)
df.to_excel(OUTPUT_EXCEL, index=False)

print("âœ… All files processed.")















INPUT_FOLDER = "/mnt/json_folder"
OUTPUT_STRUCTURED_FOLDER = "/mnt/structured_output"
OUTPUT_EXCEL = "/mnt/final_output.xlsx"

os.makedirs(OUTPUT_STRUCTURED_FOLDER, exist_ok=True)

all_rows = []

for filename in os.listdir(INPUT_FOLDER):

    if filename.endswith(".json"):

        file_path = os.path.join(INPUT_FOLDER, filename)

        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        extracted_values = extract_structured_data(ocr_pages)

        row_dict = dict(zip(BANK_FIELDS, extracted_values))
        row_dict["source_file"] = filename

        # âœ… Save individual structured JSON
        output_json_path = os.path.join(
            OUTPUT_STRUCTURED_FOLDER,
            filename.replace(".json", "_structured.json")
        )

        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(row_dict, f, indent=2, ensure_ascii=False)

        all_rows.append(row_dict)

# âœ… Save global Excel
df = pd.DataFrame(all_rows)
df.to_excel(OUTPUT_EXCEL, index=False)

print("âœ… All files processed.")














def extract_structured_data(ocr_pages: list) -> list:
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))

    prompt = build_prompt(full_text)

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=800)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            use_cache=True,
            num_beams=1,
        )

    structured = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()

    # Convert to list
    values = structured.split("|")

    # Ensure correct length
    if len(values) != len(BANK_FIELDS):
        values = values + ["null"] * (len(BANK_FIELDS) - len(values))

    return values





INPUT_FOLDER = "/mnt/json_folder"
OUTPUT_EXCEL = "/mnt/final_output.xlsx"

all_rows = []

for filename in os.listdir(INPUT_FOLDER):

    if filename.endswith(".json"):

        file_path = os.path.join(INPUT_FOLDER, filename)

        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        extracted_values = extract_structured_data(ocr_pages)

        # Create row dictionary
        row_dict = dict(zip(BANK_FIELDS, extracted_values))

        # Optional: keep file name
        row_dict["source_file"] = filename

        all_rows.append(row_dict)

# Convert to DataFrame
df = pd.DataFrame(all_rows)

# Save Excel
df.to_excel(OUTPUT_EXCEL, index=False)

print("âœ… All files processed successfully.")
print(f"Excel saved at: {OUTPUT_EXCEL}")












INPUT_FOLDER = "/mnt/json_folder"
OUTPUT_EXCEL = "/mnt/final_output.xlsx"

all_rows = []

for filename in os.listdir(INPUT_FOLDER):

    if filename.endswith(".json"):

        file_path = os.path.join(INPUT_FOLDER, filename)

        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        extracted_values = extract_structured_data(ocr_pages)

        # Create row dictionary
        row_dict = dict(zip(BANK_FIELDS, extracted_values))

        # Optional: keep file name
        row_dict["source_file"] = filename

        all_rows.append(row_dict)

# Convert to DataFrame
df = pd.DataFrame(all_rows)

# Save Excel
df.to_excel(OUTPUT_EXCEL, index=False)

print("âœ… All files processed successfully.")
print(f"Excel saved at: {OUTPUT_EXCEL}")





INPUT_FOLDER = "/mnt/json_folder"
OUTPUT_EXCEL = "/mnt/final_output.xlsx"

all_rows = []

for filename in os.listdir(INPUT_FOLDER):

    if filename.endswith(".json"):

        file_path = os.path.join(INPUT_FOLDER, filename)

        with open(file_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)

        print(f"Processing: {filename}")

        extracted_values = extract_structured_data(ocr_pages)

        # Create row dictionary
        row_dict = dict(zip(BANK_FIELDS, extracted_values))

        # Optional: keep file name
        row_dict["source_file"] = filename

        all_rows.append(row_dict)

# Convert to DataFrame
df = pd.DataFrame(all_rows)

# Save Excel
df.to_excel(OUTPUT_EXCEL, index=False)

print("âœ… All files processed successfully.")
print(f"Excel saved at: {OUTPUT_EXCEL}")




import zipfile
import os

# Path to your zip file
zip_path = "your_folder.zip"

# Where you want to extract the files
extract_path = "extracted_files"

# Create extraction folder if it doesn't exist
os.makedirs(extract_path, exist_ok=True)

# Unzip
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Extraction completed.")




import json
import glob
import os
import torch
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

INPUT_FOLDER = "/mnt/Dossier_agence/json_first_page/"
OUTPUT_FOLDER = "/mnt/Dossier_agence/structured_json/"

# Bank document fields (in extraction order)
BANK_FIELDS = [
    "Monnaie",
    "Montant en chiffres",
    "Montant en lettres",
    "Agence",
    "Nom/PrÃ©nom",
    "Adresse complÃ¨te",
    "NumÃ©ro de compte",
    "Date de valeur",
    "bank code",
    "SiÃ¨ge",
    "Racine",
    "Ordinal",
    "Devise",
    "ClÃ©",
    "Adresse SWIFT",
    "Autre (Transfert en EURO)",
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOAD MODEL (ONCE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("=" * 80)
print("LOADING MODEL")
print("=" * 80)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("âœ“ Model loaded successfully\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXTRACTION FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_prompt(text: str) -> str:
    """
    Build extraction prompt for bank documents
    
    Args:
        text: OCR text from document
        
    Returns:
        Formatted prompt string
    """
    fields_order = " | ".join(BANK_FIELDS)
    
    prompt = f"""You are an expert document information extraction system.
Extract the following fields in EXACT order separated by "|":

Important field format rules:
- bank code: 3 numbers
- SiÃ¨ge: 5 numbers
- Racine: 6 numbers
- Ordinal: 3 numbers
- Devise: letters
- ClÃ©: 2 numbers

{fields_order}

Return ONLY values separated by "|".
If a value is missing, return null.
No explanations, no field names.

OCR:
{text[:1000]}
"""
    return prompt.strip()


def extract_from_ocr(ocr_pages: list) -> dict:
    """
    Extract structured data from OCR pages
    
    Args:
        ocr_pages: List of page dicts with "text" key
        
    Returns:
        Dictionary with extracted fields
    """
    # Step 1: Merge all page text
    if isinstance(ocr_pages, dict):
        # Handle single page (not in array)
        full_text = ocr_pages.get("text", "")
    elif isinstance(ocr_pages, list):
        # Handle multiple pages
        full_text = "\n".join(
            p.get("text", "") for p in ocr_pages 
            if isinstance(p, dict) and p.get("text")
        )
    else:
        raise ValueError(f"Unexpected OCR format: {type(ocr_pages)}")
    
    if not full_text.strip():
        raise ValueError("No text found in OCR pages")
    
    # Step 2: Build prompt
    prompt = build_prompt(full_text)
    
    # Step 3: Tokenize
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=1500
    ).to(DEVICE)
    
    # Step 4: Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    # Step 5: Decode
    generated_text = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    # Step 6: Parse pipe-separated values
    values = [v.strip() for v in generated_text.split("|")]
    
    # Step 7: Build result dictionary
    result = {}
    for i, field_name in enumerate(BANK_FIELDS):
        if i < len(values):
            val = values[i]
            result[field_name] = None if val.lower() == "null" or val == "" else val
        else:
            result[field_name] = None
    
    return result

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BATCH PROCESSING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Create output folder
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

# Find all JSON files
json_files = sorted(glob.glob(f"{INPUT_FOLDER}/*.json"))

print("=" * 80)
print(f"BATCH PROCESSING: {len(json_files)} FILES")
print("=" * 80)
print(f"Input:  {INPUT_FOLDER}")
print(f"Output: {OUTPUT_FOLDER}")
print(f"Start:  {datetime.now()}\n")

success_count = 0
error_count = 0
results = []

for idx, json_path in enumerate(json_files, 1):
    file_name = os.path.basename(json_path).replace(".json", "")
    
    print(f"[{idx}/{len(json_files)}] {file_name}...")
    
    try:
        # Load JSON
        with open(json_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)
        
        # Debug info
        print(f"  â”œâ”€ Type: {type(ocr_pages)}")
        if isinstance(ocr_pages, list):
            print(f"  â”œâ”€ Pages: {len(ocr_pages)}")
            if len(ocr_pages) > 0:
                first_text = ocr_pages[0].get("text", "")[:80]
                print(f"  â”œâ”€ Preview: {first_text}...")
        elif isinstance(ocr_pages, dict):
            first_text = ocr_pages.get("text", "")[:80]
            print(f"  â”œâ”€ Preview: {first_text}...")
        
        # Extract
        structured_dict = extract_from_ocr(ocr_pages)
        
        # Save
        output_path = os.path.join(OUTPUT_FOLDER, f"{file_name}_structured.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(structured_dict, f, indent=4, ensure_ascii=False)
        
        print(f"  â””â”€ âœ“ SUCCESS\n")
        success_count += 1
        results.append({"file": file_name, "status": "success"})
        
    except Exception as e:
        print(f"  â””â”€ âœ— ERROR: {str(e)}\n")
        error_count += 1
        results.append({"file": file_name, "status": "error", "error": str(e)})

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SUMMARY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("=" * 80)
print("BATCH PROCESSING COMPLETE")
print("=" * 80)
print(f"Finished: {datetime.now()}")
print(f"Success:  {success_count}")
print(f"Errors:   {error_count}")
print(f"Total:    {len(json_files)}\n")

if error_count > 0:
    print("Files with errors:")
    for r in results:
        if r["status"] == "error":
            print(f"  - {r['file']}: {r.get('error', 'Unknown error')}")

print("\nâœ“ Results saved to:", OUTPUT_FOLDER)










import json
import glob
import os
import torch
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

INPUT_FOLDER = "/mnt/Dossier_agence/json_first_page/"
OUTPUT_STRUCTURED = "/mnt/Dossier_agence/structured_json/"

# Bank fields
BANK_FIELDS = [
    "Monnaie", "Montant en chiffres", "Montant en lettres", "Agence",
    "Nom/PrÃ©nom", "Adresse complÃ¨te", "NumÃ©ro de compte", "Date de valeur",
    "bank code", "SiÃ¨ge", "Racine", "Ordinal", "Devise", "ClÃ©",
    "Adresse SWIFT", "Autre (Transfert en EURO)",
]

# --------------------------
# Load model once
# --------------------------
print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("âœ“ Model loaded.\n")

# --------------------------
# Prompt builder
# --------------------------
def build_prompt(text: str) -> str:
    """Build extraction prompt for FIRST PAGE ONLY"""
    order = " | ".join(BANK_FIELDS)
    
    prompt = f"""You are an expert document information extraction system.
Extract the following fields in EXACT order separated by "|":

{order}

Important rules for bank documents:
- bank code: 3 numbers
- SiÃ¨ge: 5 numbers
- Racine: 6 numbers
- Ordinal: 3 numbers
- Devise: letters
- ClÃ©: 2 numbers

Return ONLY values separated by "|".
If a value is missing, return null.
No explanations, no field names.

OCR:
{text[:800]}
"""
    return prompt.strip()

# --------------------------
# Extract from SINGLE PAGE (first page only)
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    """Extract from first page OCR JSON"""
    
    # Merge all text from OCR pages (usually just 1 page in your case)
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
    
    if not full_text.strip():
        raise ValueError("No text found in OCR pages")
    
    # Build prompt
    prompt = build_prompt(full_text)
    
    # Tokenize & generate
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    # Decode output
    result = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    # Parse pipe-separated values
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    structured = {}
    for i, field in enumerate(BANK_FIELDS):
        if i < len(values):
            val = values[i]
            structured[field] = None if val.lower() == "null" or val == "" else val
        else:
            structured[field] = None
    
    return structured

# --------------------------
# Batch process folder
# --------------------------
os.makedirs(OUTPUT_STRUCTURED, exist_ok=True)

json_files = glob.glob(f"{INPUT_FOLDER}/*.json")
print(f"Found {len(json_files)} first-page JSON files.")
print(f"Start: {datetime.now()}\n")

success_count = 0
error_count = 0

for json_path in json_files:
    file_name = os.path.basename(json_path).replace("_page1.json", "").replace(".json", "")
    
    try:
        # Load OCR JSON
        with open(json_path, "r", encoding="utf-8") as f:
            ocr_pages = json.load(f)
        
        # Extract
        structured_dict = extract_structured_data(ocr_pages)
        
        # Save
        output_path = os.path.join(OUTPUT_STRUCTURED, f"{file_name}_structured.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(structured_dict, f, indent=4, ensure_ascii=False)
        
        print(f"âœ“ Processed: {file_name}")
        success_count += 1
        
    except Exception as e:
        print(f"âœ— Error processing {file_name}: {e}")
        error_count += 1

print(f"\nFinished: {datetime.now()}")
print(f"Success: {success_count} | Errors: {error_count}")













import os
import glob
import json
from datetime import datetime
import torch
import pandas as pd

# --- LLM / Tokenizer ---
# Assume you have your model & tokenizer already loaded:
# tokenizer = ...
# model = ...

# --- Folders ---
INPUT_FOLDER = "/mnt/Dossier_agence/json_first_page/"
STRUCTURED_FOLDER = "/mnt/Dossier_agence/structured_json/"
os.makedirs(STRUCTURED_FOLDER, exist_ok=True)

OUTPUT_EXCEL = "/mnt/Dossier_agence/final_output.xlsx"

# --- Bank Fields ---
BANK_FIELDS = [
    "Montant en chiffres",
    "Montant en lettres",
    "Nom/PrÃ©nom",
    "Adresse complÃ¨te",
    "NumÃ©ro de compte",
    "SiÃ¨ge",
    "Racine",
    "Ordinal",
    "Devise",
    "ClÃ©",
    "Adresse SWIFT",
    "Autre (Transfert en EURO)",
    "Nom",
    "RÃ©fÃ©rences du paiement et observations du donneur d'ordre"
]

# --- Build Prompt ---
def build_prompt(text: str) -> str:
    order = " | ".join(BANK_FIELDS)
    prompt = f"""
You are an expert document information extraction system.

Extract the following fields in EXACT order separated by "|":

{order}

Rules:
- SiÃ¨ge = 5 digits
- Racine = 6 digits
- Ordinal = 3 digits
- ClÃ© = 2 digits
- Devise = currency letters (DZD, EUR, etc.)

Return ONLY values separated by "|".
If a value is missing, return null.
No explanations.
No field names.

OCR:
{text}
"""
    return prompt.strip()


# --- Extract Structured Data ---
def extract_structured_data(ocr_pages: list, file_name: str) -> dict:

    # Merge OCR text
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
    
    if not full_text.strip():
        print(f"âš ï¸ Empty OCR text for {file_name}")
        return None

    # Build prompt
    prompt = build_prompt(full_text)

    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200)

    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=300,
            do_sample=False,
            num_beams=1,
            use_cache=True
        )

    # Decode output
    structured_output = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()

    # Debugging: print first 100 chars if output is empty or weird
    if not structured_output or structured_output.replace("|", "").strip() == "":
        print(f"âš ï¸ Empty or invalid output for {file_name}")
        print("RAW OUTPUT:", structured_output[:200])

    # Split into fields
    values = [v.strip() for v in structured_output.split("|")]

    if len(values) != len(BANK_FIELDS):
        print(f"âš ï¸ Field mismatch for {file_name} ({len(values)} fields)")
        print("RAW OUTPUT:", structured_output[:200])
        # Fill missing with null
        while len(values) < len(BANK_FIELDS):
            values.append("null")
        values = values[:len(BANK_FIELDS)]

    structured_dict = dict(zip(BANK_FIELDS, values))
    return structured_dict


# --- Process All JSON Files ---
json_files = glob.glob(os.path.join(INPUT_FOLDER, "*.json"))
print(f"Found {len(json_files)} JSON files")
print("Start:", datetime.now())

all_rows = []

for json_path in json_files:
    file_name = os.path.basename(json_path).replace(".json", "")
    with open(json_path, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    try:
        structured_dict = extract_structured_data(ocr_pages, file_name)
        if structured_dict is None:
            continue

        # Save structured JSON per PDF
        output_path = os.path.join(STRUCTURED_FOLDER, f"{file_name}_structured.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(structured_dict, f, indent=4, ensure_ascii=False)

        # Keep for Excel
        row = structured_dict.copy()
        row["PDF_Name"] = file_name
        all_rows.append(row)

        print(f"Processed: {file_name}")

    except Exception as e:
        print(f"âŒ Error processing {file_name}: {e}")

print("Finished processing JSONs:", datetime.now())

# --- Merge All Structured JSON into Excel ---
if all_rows:
    df = pd.DataFrame(all_rows)
    df.to_excel(OUTPUT_EXCEL, index=False)
    print(f"âœ… Excel created: {OUTPUT_EXCEL}")
else:
    print("âš ï¸ No data extracted for Excel")











import os
import glob
import json
import pandas as pd
from datetime import datetime

STRUCTURED_FOLDER = "/mnt/Dossier_agence/structured_json/"
OUTPUT_EXCEL = "/mnt/Dossier_agence/final_output.xlsx"

print("Start:", datetime.now())

json_files = glob.glob(f"{STRUCTURED_FOLDER}/*.json")

print(f"Found {len(json_files)} structured JSON files.")

all_rows = []

for json_path in json_files:
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # Add filename column
    file_name = os.path.basename(json_path).replace("_structured.json", "")
    data["PDF_Name"] = file_name

    all_rows.append(data)

# Convert to DataFrame
df = pd.DataFrame(all_rows)

# Save to Excel
df.to_excel(OUTPUT_EXCEL, index=False)

print(f"Excel file created at: {OUTPUT_EXCEL}")
print("Finished:", datetime.now())





def build_prompt(text: str) -> str:

    order = " | ".join(BANK_FIELDS)

    prompt = f"""
You are an expert document information extraction system.

Extract the following fields in EXACT order separated by "|":

{order}

Rules:
- Bank code (SiÃ¨ge) = 5 digits
- Racine = 6 digits
- Ordinal = 3 digits
- ClÃ© = 2 digits
- Devise = currency letters (e.g., DZD, EUR)

Return ONLY values separated by "|".
If a value is missing, return null.
No explanations.
No field names.

OCR:
{text}
"""

    return prompt.strip()




def extract_structured_data(ocr_pages: list) -> str:

    # Merge OCR text
    full_text = "\n".join(
        p.get("text", "") for p in ocr_pages if p.get("text")
    )

    if not full_text.strip():
        raise ValueError("Empty OCR text.")

    # Build prompt
    prompt = build_prompt(full_text)

    # Tokenize
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=800
    )

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            num_beams=1,
            use_cache=True
        )

    # Decode ONLY new tokens
    structured = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()

    return structured


import os
import glob
import json
from datetime import datetime

INPUT_FOLDER = "/mnt/Dossier_agence/json_first_page/"
OUTPUT_STRUCTURED = "/mnt/Dossier_agence/structured_json/"

os.makedirs(OUTPUT_STRUCTURED, exist_ok=True)

json_files = glob.glob(f"{INPUT_FOLDER}/*.json")

print(f"Found {len(json_files)} first-page JSON files.")
print("Start:", datetime.now())

for json_path in json_files:

    file_name = os.path.basename(json_path).replace("_page1.json", "")

    with open(json_path, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    try:
        structured_output = extract_structured_data(ocr_pages)

        # Convert "|" output to dictionary
        values = structured_output.split("|")
        structured_dict = dict(zip(BANK_FIELDS, [v.strip() for v in values]))

        output_path = os.path.join(
            OUTPUT_STRUCTURED,
            f"{file_name}_structured.json"
        )

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(structured_dict, f, indent=4, ensure_ascii=False)

        print(f"Processed: {file_name}")

    except Exception as e:
        print(f"Error processing {file_name}: {e}")

print("Finished:", datetime.now())







import os
import glob
import json

JSON_FOLDER = "/mnt/Dossier_agence/json/"
OUTPUT_FOLDER = "/mnt/Dossier_agence/json_first_page/"

os.makedirs(OUTPUT_FOLDER, exist_ok=True)

json_files = glob.glob(f"{JSON_FOLDER}/*.json")

print(f"Found {len(json_files)} JSON files.")

for json_path in json_files:
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # Keep only first page
    if isinstance(data, list) and len(data) > 0:
        first_page = [data[0]]
    else:
        first_page = []

    # Create new file name
    file_name = os.path.basename(json_path)
    new_path = os.path.join(OUTPUT_FOLDER, file_name.replace(".json", "_page1.json"))

    with open(new_path, "w", encoding="utf-8") as f:
        json.dump(first_page, f, indent=4, ensure_ascii=False)

    print(f"Created: {os.path.basename(new_path)}")

print("Done âœ…")





import json
from datetime import datetime

print(datetime.now())

all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}...")

    page_text = extract_text(img)

    all_text.append({
        "page": i+1,
        "text": page_text
    })

    print(f"Page {i+1} done. Preview: {page_text[:80]}...\n")

# Save ONE JSON for this PDF
with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print(f"\nDone! Text saved to {OUTPUT_JSON}")
print(datetime.now())




import os
import glob
import json
from datetime import datetime
from PIL import Image, ImageDraw, ImageFont
import fitz  # PyMuPDF

# ---------- CONFIG ----------
PDF_FOLDER = "/mnt/Dossier_agence/"
IMAGE_SAVE_DIR = "/mnt/Dossier_agence/pages/"
JSON_SAVE_DIR = "/mnt/Dossier_agence/json/"
DPI = 100

os.makedirs(IMAGE_SAVE_DIR, exist_ok=True)
os.makedirs(JSON_SAVE_DIR, exist_ok=True)

# ---------- YOUR OCR FUNCTION ----------
def extract_text(img):
    from pytesseract import image_to_string
    return image_to_string(img, lang="fra")  # change if needed


# ---------- PDF TO IMAGES ----------
def pdf_to_images(pdf_path, save_dir=IMAGE_SAVE_DIR, dpi=DPI):
    doc = fitz.open(pdf_path)
    images = []

    file_name = os.path.splitext(os.path.basename(pdf_path))[0]

    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        # Pad to square
        max_side = max(img.width, img.height)
        img_square = Image.new("RGB", (max_side, max_side), "white")
        img_square.paste(img, (0, 0))

        # Draw filename on image
        draw = ImageDraw.Draw(img_square)
        try:
            font = ImageFont.truetype("arial.ttf", 20)
        except:
            font = ImageFont.load_default()

        text = f"{file_name} - Page {i+1}"
        draw.text((10, 10), text, fill="black", font=font)

        img_path = os.path.join(save_dir, f"{file_name}_page_{i+1}.png")
        img_square.save(img_path)

        images.append(img_square)

    return images


# ---------- MAIN PIPELINE ----------
pdf_files = glob.glob(f"{PDF_FOLDER}/*.pdf")

print(f"Found {len(pdf_files)} PDFs.")
print("Start:", datetime.now())

for pdf_path in pdf_files:
    file_name = os.path.splitext(os.path.basename(pdf_path))[0]
    print(f"\nProcessing PDF: {file_name}")

    images = pdf_to_images(pdf_path)

    pdf_text = []

    for i, img in enumerate(images):
        print(f"  Extracting page {i+1}/{len(images)}...")
        page_text = extract_text(img)

        pdf_text.append({
            "page": i+1,
            "text": page_text
        })

    # Save ONE JSON for THIS PDF
    json_path = os.path.join(JSON_SAVE_DIR, f"{file_name}.json")

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(pdf_text, f, indent=4, ensure_ascii=False)

    print(f"  JSON saved: {json_path}")

print("\nFinished:", datetime.now())













from datetime import datetime
import json

print(datetime.now())

all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}...")
    
    page_text = extract_text(img)
    all_text.append({"page": i+1, "text": page_text})
    
    print(f"Page {i+1} done. Preview: {page_text[:80]}...\n")

with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print(f"\nDone! Text saved to {OUTPUT_JSON}")
print(datetime.now())















4import glob
from datetime import datetime

pdf_folder = "/mnt/Dossier_agence/"
save_dir = "/mnt/Dossier_agence/"

# Get all PDF files in the folder
pdf_files = glob.glob(f"{pdf_folder}/*.pdf")

print(f"Found {len(pdf_files)} PDFs.")

start_time = datetime.now()
print("Start:", start_time)

for pdf_path in pdf_files:
    images = pdf_to_images(pdf_path, save_dir, dpi=100)

end_time = datetime.now()
print("End:", end_time)








import os
from PIL import Image, ImageDraw, ImageFont
import fitz  # PyMuPDF

def pdf_to_images(pdf_path, save_dir="/mnt/pdf_pages", dpi=200):
    os.makedirs(save_dir, exist_ok=True)
    
    doc = fitz.open(pdf_path)
    images = []
    
    # Get the PDF filename without extension
    file_name = os.path.splitext(os.path.basename(pdf_path))[0]
    
    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Pad to square without resizing smaller
        max_side = max(img.width, img.height)
        img_square = Image.new("RGB", (max_side, max_side), "white")
        img_square.paste(img, (0, 0))
        
        # Draw filename on the image
        draw = ImageDraw.Draw(img_square)
        try:
            font = ImageFont.truetype("arial.ttf", 20)  # Change font/size as needed
        except:
            font = ImageFont.load_default()
        text = f"{file_name} - Page {i+1}"
        draw.text((10, 10), text, fill="black", font=font)
        
        # Save image
        img_path = os.path.join(save_dir, f"{file_name}_page_{i+1}.png")
        img_square.save(img_path)
        images.append(img_square)
    
    return images








import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("âœ“ Model loaded.\n")

# --------------------------
# Field Schemas
# --------------------------
PAGE_SCHEMAS = {
    "ordre_de_virement": [
        "Monnaie", "Montant en chiffres", "Montant en lettres", "Agence",
        "Nom/PrÃ©nom", "Adresse complÃ¨te", "NumÃ©ro de compte", "Date de valeur",
        "bank code", "SiÃ¨ge", "Racine", "Ordinal", "Devise", "ClÃ©",
        "Adresse SWIFT", "Autre (Transfert en EURO)",
    ],
    "annexe_1": ["Nom", "Date de signature"],
    "annexe_2": ["Mois", "Part transfÃ©rable", "Nom et prÃ©nom"],
    "domiciliation_import": ["Nom"],
    "bulletin_de_paie": ["Nom"],
    "gestion_de_blocages": [],
}

ID_FIELDS = [
    "Ø§Ù„Ù„Ù‚Ø¨", "Ø§Ù„Ø§Ø³Ù…", "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯", "Ø§Ù„Ø¬Ù†Ø³",
    "Ø±Ù‚Ù… Ø§Ù„ÙˆØ·Ù†ÙŠ", "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØµØ¯Ø§Ø±", "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡",
    "Ø³Ù„Ø·Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø±", "Nom", "PrÃ©nom"
]

# --------------------------
# Prompt Builder (YOUR CUSTOM VERSION)
# --------------------------
def build_prompt(text: str, fields: list, is_bank: bool = False) -> str:
    """Build CPU-friendly minimal prompt"""
    order = " | ".join(fields)
    
    # Base prompt
    prompt = f"""You are an expert document information extraction system.
Extract the following fields in EXACT order separated by "|":

{order}
"""
    
    # Add bank-specific rules
    if is_bank:
        prompt += """
Note for bank documents:
- bank code: 3 numbers
- SiÃ¨ge: 5 numbers
- Racine: 6 numbers
- Ordinal: 3 numbers
- Devise: letters
- ClÃ©: 2 numbers

"""
    
    # Instructions
    prompt += f"""Return ONLY values separated by "|".
If a value is missing, return null.
No explanations, no field names.

OCR:
{text}
"""
    
    return prompt.strip()

# --------------------------
# Helper: Detect page type
# --------------------------
def detect_page_type(text: str) -> str:
    text_lower = text.lower()
    if "ordre de virement" in text_lower or "virement a l'etranger" in text_lower:
        return "ordre_de_virement"
    elif "annexe 1" in text_lower or "annexe i" in text_lower:
        return "annexe_1"
    elif "annexe 2" in text_lower or "annexe ii" in text_lower:
        return "annexe_2"
    elif "domiciliation" in text_lower and "import" in text_lower:
        return "domiciliation_import"
    elif "bulletin de paie" in text_lower or "bulletin de salaire" in text_lower:
        return "bulletin_de_paie"
    elif "gestion de blocages" in text_lower or "blocage" in text_lower:
        return "gestion_de_blocages"
    return "unknown"

# --------------------------
# Helper: LLM extraction (UPDATED TO USE build_prompt)
# --------------------------
def llm_extract(text: str, fields: list, is_bank: bool = False) -> dict:
    """Call LLM to extract fields from text - ALWAYS returns dict"""
    if not fields:
        return {}
    
    # Use your custom prompt builder
    prompt = build_prompt(text[:800], fields, is_bank)
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs, max_new_tokens=180, do_sample=False,
            use_cache=True, num_beams=1, pad_token_id=tokenizer.eos_token_id
        )
    
    result = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()
    
    # Parse pipe-separated values
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    extracted = {}
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            extracted[field] = None if val.lower() == "null" or val == "" else val
        else:
            extracted[field] = None
    
    return extracted

# --------------------------
# UNIFIED EXTRACTION FUNCTION
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    """
    Single function to extract from both ID cards and multi-page bank documents
    
    Input: ocr_pages = [{"page": 1, "text": "..."}, ...]
    Output: structured dict with all extracted fields
    """
    if not ocr_pages:
        raise ValueError("No OCR pages provided")
    
    # Detect document type from first page
    first_page_text = ocr_pages[0].get("text", "").lower()
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ID CARD PATH
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    if any(k in first_page_text for k in ["Ø¨Ø·Ø§Ù‚Ø©", "cni", "carte", "identitÃ©"]):
        print("Document type: ID Card\n")
        
        full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
        
        extracted = llm_extract(full_text, ID_FIELDS, is_bank=False)
        extracted["document_type"] = "Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙˆØ·Ù†ÙŠØ©"
        
        print("âœ“ ID card extraction complete\n")
        return extracted
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # BANK DOCUMENT PATH
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    elif any(k in first_page_text for k in ["ordre de virement", "virement", "banque", "swift"]):
        print(f"Document type: Bank Transfer ({len(ocr_pages)} pages)\n")
        
        all_data = {"document_type": "ORDRE DE VIREMENT A L'ETRANGER"}
        
        # Process each page
        for i, page in enumerate(ocr_pages):
            page_text = page.get("text", "")
            if not page_text:
                continue
            
            print(f"Page {i+1}:", end=" ")
            page_type = detect_page_type(page_text)
            
            if page_type == "unknown":
                print("âš  Unknown page type, skipping")
                continue
            
            if page_type == "gestion_de_blocages":
                print(f"{page_type} (skipped)")
                continue
            
            # Get fields for this page type
            fields = PAGE_SCHEMAS.get(page_type, [])
            if not fields:
                print(f"{page_type} (no fields defined)")
                continue
            
            # Extract (pass is_bank=True for bank-specific rules)
            is_bank_page = (page_type == "ordre_de_virement")
            page_data = llm_extract(page_text, fields, is_bank=is_bank_page)
            
            # Verify it's a dict
            if not isinstance(page_data, dict):
                print(f"âš  Error: expected dict, got {type(page_data)}")
                continue
            
            # Merge into main dict (don't overwrite existing values)
            for key, value in page_data.items():
                if key not in all_data or all_data[key] is None:
                    all_data[key] = value
            
            print(f"{page_type} â†’ extracted {len(page_data)} fields")
        
        print("\nâœ“ Bank document extraction complete\n")
        return all_data
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # UNKNOWN DOCUMENT TYPE
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    else:
        raise ValueError("Unknown document type - could not detect ID card or bank document")

# --------------------------
# Run
# --------------------------
if __name__ == "__main__":
    # Load OCR output
    with open("extracted.json", "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)
    
    # Extract
    structured = extract_structured_data(ocr_pages)
    
    # Save JSON
    with open("structured.json", "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)
    
    print(json.dumps(structured, indent=2, ensure_ascii=False))















import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("âœ“ Model loaded.\n")

# --------------------------
# Field Schemas
# --------------------------
PAGE_SCHEMAS = {
    "ordre_de_virement": [
        "Monnaie", "Montant en chiffres", "Montant en lettres", "Agence",
        "Nom/PrÃ©nom", "Adresse complÃ¨te", "NumÃ©ro de compte", "Date de valeur",
        "bank code", "SiÃ¨ge", "Racine", "Ordinal", "Devise", "ClÃ©",
        "Adresse SWIFT", "Autre (Transfert en EURO)",
    ],
    "annexe_1": ["Nom", "Date de signature"],
    "annexe_2": ["Mois", "Part transfÃ©rable", "Nom et prÃ©nom"],
    "domiciliation_import": ["Nom"],
    "bulletin_de_paie": ["Nom"],
    "gestion_de_blocages": [],
}

ID_FIELDS = [
    "Ø§Ù„Ù„Ù‚Ø¨", "Ø§Ù„Ø§Ø³Ù…", "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯", "Ø§Ù„Ø¬Ù†Ø³",
    "Ø±Ù‚Ù… Ø§Ù„ÙˆØ·Ù†ÙŠ", "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØµØ¯Ø§Ø±", "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡",
    "Ø³Ù„Ø·Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø±", "Nom", "PrÃ©nom"
]

# --------------------------
# Helper: Detect page type
# --------------------------
def detect_page_type(text: str) -> str:
    text_lower = text.lower()
    if "ordre de virement" in text_lower or "virement a l'etranger" in text_lower:
        return "ordre_de_virement"
    elif "annexe 1" in text_lower or "annexe i" in text_lower:
        return "annexe_1"
    elif "annexe 2" in text_lower or "annexe ii" in text_lower:
        return "annexe_2"
    elif "domiciliation" in text_lower and "import" in text_lower:
        return "domiciliation_import"
    elif "bulletin de paie" in text_lower or "bulletin de salaire" in text_lower:
        return "bulletin_de_paie"
    elif "gestion de blocages" in text_lower or "blocage" in text_lower:
        return "gestion_de_blocages"
    return "unknown"

# --------------------------
# Helper: LLM extraction
# --------------------------
def llm_extract(text: str, fields: list) -> dict:
    """Call LLM to extract fields from text"""
    if not fields:
        return {}
    
    messages = [
        {"role": "system", "content": "Extract document fields. Output ONLY pipe-separated values, no field names."},
        {"role": "user", "content": f"""Extract these fields in order:
{' | '.join(fields)}

Rules:
- Return ONLY values like: value1 | value2 | value3 | ...
- Use "null" for missing fields
- No explanations, no field names

Document:
{text[:800]}"""}
    ]
    
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs, max_new_tokens=180, do_sample=False,
            use_cache=True, num_beams=1, pad_token_id=tokenizer.eos_token_id
        )
    
    result = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    extracted = {}
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            extracted[field] = None if val.lower() == "null" or val == "" else val
        else:
            extracted[field] = None
    
    return extracted

# --------------------------
# UNIFIED EXTRACTION FUNCTION
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    """
    Single function to extract from both ID cards and multi-page bank documents
    
    Input: ocr_pages = [{"page": 1, "text": "..."}, ...]
    Output: structured dict with all extracted fields
    """
    if not ocr_pages:
        raise ValueError("No OCR pages provided")
    
    # Detect document type from first page
    first_page_text = ocr_pages[0].get("text", "").lower()
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ID CARD PATH (single page or simple multi-page)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    if any(k in first_page_text for k in ["Ø¨Ø·Ø§Ù‚Ø©", "cni", "carte", "identitÃ©"]):
        print("Document type: ID Card\n")
        
        # Merge all pages text
        full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
        
        # Extract
        extracted = llm_extract(full_text, ID_FIELDS)
        extracted["document_type"] = "Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙˆØ·Ù†ÙŠØ©"
        
        print("âœ“ ID card extraction complete\n")
        return extracted
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # BANK DOCUMENT PATH (multi-page with page-specific fields)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    elif any(k in first_page_text for k in ["ordre de virement", "virement", "banque", "swift"]):
        print(f"Document type: Bank Transfer ({len(ocr_pages)} pages)\n")
        
        all_data = {"document_type": "ORDRE DE VIREMENT A L'ETRANGER"}
        
        # Process each page
        for i, page in enumerate(ocr_pages):
            page_text = page.get("text", "")
            if not page_text:
                continue
            
            print(f"Page {i+1}:", end=" ")
            page_type = detect_page_type(page_text)
            
            if page_type == "unknown":
                print("âš  Unknown page type, skipping")
                continue
            
            if page_type == "gestion_de_blocages":
                print(f"{page_type} (skipped)")
                continue
            
            # Get fields for this page type
            fields = PAGE_SCHEMAS.get(page_type, [])
            if not fields:
                print(f"{page_type} (no fields defined)")
                continue
            
            # Extract
            page_data = llm_extract(page_text, fields)
            
            # Merge into main dict (don't overwrite existing values)
            for key, value in page_data.items():
                if key not in all_data or all_data[key] is None:
                    all_data[key] = value
            
            print(f"{page_type} â†’ extracted {len(page_data)} fields")
        
        print("\nâœ“ Bank document extraction complete\n")
        return all_data
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # UNKNOWN DOCUMENT TYPE
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    else:
        raise ValueError("Unknown document type - could not detect ID card or bank document")

# --------------------------
# Run
# --------------------------
if __name__ == "__main__":
    # Load OCR output
    with open("extracted.json", "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)
    
    # Extract (single function handles everything)
    structured = extract_structured_data(ocr_pages)
    
    # Save JSON
    with open("structured.json", "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)
    
    print(json.dumps(structured, indent=2, ensure_ascii=False))















import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("âœ“ Model loaded.\n")

# --------------------------
# Page-specific field schemas
# --------------------------
PAGE_SCHEMAS = {
    "ordre_de_virement": [
        "Monnaie",
        "Montant en chiffres",
        "Montant en lettres",
        "Agence",
        "Nom/PrÃ©nom",
        "Adresse complÃ¨te",
        "NumÃ©ro de compte",
        "Date de valeur",
        "bank code",
        "SiÃ¨ge",
        "Racine",
        "Ordinal",
        "Devise",
        "ClÃ©",
        "Adresse SWIFT",
        "Autre (Transfert en EURO)",
    ],
    "annexe_1": [
        "Nom",
        "Date de signature",
    ],
    "annexe_2": [
        "Mois",
        "Part transfÃ©rable",
        "Nom et prÃ©nom",
    ],
    "domiciliation_import": [
        "Nom",
        # Add other fields you need from this page
    ],
    "bulletin_de_paie": [
        "Nom",
        # Add other fields you need from this page
    ],
    "gestion_de_blocages": [
        # No extraction needed
    ],
}

ID_FIELDS = [
    "Ø§Ù„Ù„Ù‚Ø¨", "Ø§Ù„Ø§Ø³Ù…", "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯", "Ø§Ù„Ø¬Ù†Ø³",
    "Ø±Ù‚Ù… Ø§Ù„ÙˆØ·Ù†ÙŠ", "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØµØ¯Ø§Ø±", "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡",
    "Ø³Ù„Ø·Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø±", "Nom", "PrÃ©nom"
]

# --------------------------
# Detect page type by content
# --------------------------
def detect_page_type(text: str) -> str:
    """Detect what type of page this is based on text content"""
    text_lower = text.lower()
    
    # Check for specific page identifiers
    if "ordre de virement" in text_lower or "virement a l'etranger" in text_lower:
        return "ordre_de_virement"
    elif "annexe 1" in text_lower or "annexe i" in text_lower:
        return "annexe_1"
    elif "annexe 2" in text_lower or "annexe ii" in text_lower:
        return "annexe_2"
    elif "domiciliation" in text_lower and "import" in text_lower:
        return "domiciliation_import"
    elif "bulletin de paie" in text_lower or "bulletin de salaire" in text_lower:
        return "bulletin_de_paie"
    elif "gestion de blocages" in text_lower or "blocage" in text_lower:
        return "gestion_de_blocages"
    
    return "unknown"

# --------------------------
# Detect document type (bank vs ID)
# --------------------------
def detect_document_type(ocr_pages: list) -> str:
    """Detect if this is a bank document or ID card"""
    # Check first page
    first_page_text = ocr_pages[0].get("text", "").lower() if ocr_pages else ""
    
    if any(k in first_page_text for k in ["Ø¨Ø·Ø§Ù‚Ø©", "cni", "carte", "identitÃ©"]):
        return "id_card"
    elif any(k in first_page_text for k in ["ordre de virement", "virement", "banque", "swift"]):
        return "bank"
    
    return "unknown"

# --------------------------
# Extract from a single page
# --------------------------
def extract_page(page_text: str, page_type: str) -> dict:
    """Extract fields from a single page"""
    if page_type == "gestion_de_blocages" or page_type == "unknown":
        return {}
    
    fields = PAGE_SCHEMAS.get(page_type, [])
    if not fields:
        return {}
    
    # Build prompt
    messages = [
        {
            "role": "system",
            "content": "Extract document fields. Output ONLY pipe-separated values, no field names."
        },
        {
            "role": "user",
            "content": f"""Extract these fields in order:
{' | '.join(fields)}

Rules:
- Return ONLY values like: value1 | value2 | value3 | ...
- Use "null" for missing fields
- No explanations, no field names

Document:
{page_text[:800]}"""
        }
    ]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    print(f"  Page type: {page_type}")
    print(f"  Raw output: {result[:100]}...")
    
    # Parse pipe-separated values
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    page_data = {}
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            page_data[field] = None if val.lower() == "null" or val == "" else val
        else:
            page_data[field] = None
    
    return page_data

# --------------------------
# Extract ID card
# --------------------------
def extract_id_card(ocr_pages: list) -> dict:
    """Extract ID card (single page or multi-page)"""
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
    
    messages = [
        {
            "role": "system",
            "content": "Extract document fields. Output ONLY pipe-separated values, no field names."
        },
        {
            "role": "user",
            "content": f"""Extract these fields in order:
{' | '.join(ID_FIELDS)}

Rules:
- Return ONLY values like: value1 | value2 | value3 | ...
- Use "null" for missing fields
- No explanations, no field names

Document:
{full_text[:800]}"""
        }
    ]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    values = [v.strip() for v in result.split("|")]
    
    structured = {"document_type": "Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙˆØ·Ù†ÙŠØ©"}
    for i, field in enumerate(ID_FIELDS):
        if i < len(values):
            val = values[i]
            structured[field] = None if val.lower() == "null" or val == "" else val
        else:
            structured[field] = None
    
    return structured

# --------------------------
# Extract multi-page bank document
# --------------------------
def extract_bank_document(ocr_pages: list) -> dict:
    """Extract bank document with page-specific fields"""
    all_data = {"document_type": "ORDRE DE VIREMENT A L'ETRANGER"}
    
    print(f"Processing {len(ocr_pages)} pages...\n")
    
    for i, page in enumerate(ocr_pages):
        page_text = page.get("text", "")
        if not page_text:
            continue
        
        print(f"Page {i+1}:")
        page_type = detect_page_type(page_text)
        
        if page_type == "unknown":
            print(f"  âš  Could not detect page type, skipping\n")
            continue
        
        page_data = extract_page(page_text, page_type)
        
        # Merge into main dict (avoid overwriting)
        for key, value in page_data.items():
            if key not in all_data or all_data[key] is None:
                all_data[key] = value
        
        print(f"  âœ“ Extracted {len(page_data)} fields\n")
    
    return all_data

# --------------------------
# Main extraction function
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    """Main entry point: detect document type and extract accordingly"""
    doc_type = detect_document_type(ocr_pages)
    
    if doc_type == "id_card":
        print("Document type: ID Card\n")
        return extract_id_card(ocr_pages)
    elif doc_type == "bank":
        print("Document type: Bank Transfer (Multi-page)\n")
        return extract_bank_document(ocr_pages)
    else:
        raise ValueError("Unknown document type")

# --------------------------
# Run
# --------------------------
if __name__ == "__main__":
    # Load OCR output
    with open("extracted.json", "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)
    
    # Extract
    structured = extract_structured_data(ocr_pages)
    
    # Save JSON
    with open("structured.json", "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)
    
    print("\nâœ“ Extraction complete!\n")
    print(json.dumps(structured, indent=2, ensure_ascii=False))


















from concurrent.futures import ProcessPoolExecutor
import os
import json

INPUT_FOLDER = "/mnt/ocr_outputs"

def process_file(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    structured = extract_structured_data(ocr_pages)
    return {
        "file": os.path.basename(file_path),
        "extracted": structured
    }

if __name__ == "__main__":

    json_files = [
        os.path.join(INPUT_FOLDER, f)
        for f in os.listdir(INPUT_FOLDER)
        if f.endswith(".json")
    ]

    with ProcessPoolExecutor() as executor:
        results = list(executor.map(process_file, json_files))

    print(json.dumps(results, indent=2, ensure_ascii=False))



















# Split the string into lines
lines = structured.split("\n")

# Create dictionary
data_dict = {}
for line in lines:
    if ":" in line:
        key, value = line.split(":", 1)  # split at first colon
        data_dict[key.strip()] = value.strip()

print(data_dict)
import pandas as pd

# If you have multiple pages, you can wrap them in a list
df = pd.DataFrame([data_dict])  # wrap in list for single row

# Save to Excel
df.to_excel("payment.xlsx", index=False, encoding="utf-8")








import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("âœ“ Model loaded.\n")

# --------------------------
# Schemas
# --------------------------
BANK_FIELDS = [
    "Monnaie",
    "Montant en chiffres",
    "Montant en lettres",
    "Agence",
    "Nom/PrÃ©nom",
    "Nature du paiement",
    "Adresse complÃ¨te",
    "NumÃ©ro de compte",
    "Date de valeur",
    "SiÃ¨ge",
    "Racine",
    "Ordinal",
    "Devise",
    "ClÃ©",
    "Adresse SWIFT",
    "Autre (Transfert en EURO)",
]

ID_FIELDS = [
    "Ø§Ù„Ù„Ù‚Ø¨", "Ø§Ù„Ø§Ø³Ù…", "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯", "Ø§Ù„Ø¬Ù†Ø³",
    "Ø±Ù‚Ù… Ø§Ù„ÙˆØ·Ù†ÙŠ", "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØµØ¯Ø§Ø±", "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡",
    "Ø³Ù„Ø·Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø±", "Nom", "PrÃ©nom"
]

def detect_document_type(text: str) -> str:
    if any(k in text for k in ["ORDRE", "virement", "Banque", "SWIFT"]):
        return "bank"
    elif any(k in text for k in ["Ø¨Ø·Ø§Ù‚Ø©", "CNI", "Carte"]):
        return "id"
    return "unknown"

# --------------------------
# FIXED PROMPT (uses chat template properly)
# --------------------------
def build_prompt(text: str, doc_type: str) -> str:
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    
    # Qwen chat format
    messages = [
        {
            "role": "system",
            "content": "Extract document fields. Output ONLY pipe-separated values, no field names."
        },
        {
            "role": "user",
            "content": f"""Extract these fields in order:
{' | '.join(fields)}

Rules:
- Return ONLY values like: value1 | value2 | value3 | ...
- Use "null" for missing fields
- No explanations, no field names

Document:
{text[:800]}"""
        }
    ]
    
    return tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

# --------------------------
# Extract
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
    doc_type = detect_document_type(full_text)
    
    if doc_type == "unknown":
        raise ValueError("Unknown document type")
    
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    prompt = build_prompt(full_text, doc_type)
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    print(f"Raw output:\n{result}\n")
    
    # Parse pipe-separated values
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    doc_type_name = "ORDRE DE VIREMENT A L'ETRANGER" if doc_type == "bank" else "Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙˆØ·Ù†ÙŠØ©"
    structured = {"document_type": doc_type_name}
    
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            structured[field] = None if val.lower() == "null" or val == "" else val
        else:
            structured[field] = None
    
    return structured

# --------------------------
# Run
# --------------------------
if __name__ == "__main__":
    # Load your extracted.json
    with open("extracted.json", "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)
    
    structured = extract_structured_data(ocr_pages)
    
    # Save
    with open("structured.json", "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)
    
    print(json.dumps(structured, indent=2, ensure_ascii=False))












import zipfile
import os

folder_path = "dossier"
zip_path = "excel_files.zip"

with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for file in os.listdir(folder_path):
        if file.endswith(".xlsx"):
            zipf.write(os.path.join(folder_path, file),
                       arcname=file)

print("ZIP file created successfully!")







structured = {}

for i in range(len(fields)):
    if i < len(values):
        val = values[i].strip()
        structured[fields[i]] = None if val.lower() == "null" else val
    else:
        structured[fields[i]] = None







structured = {}

for i, field in enumerate(fields):
    if i < len(values):
        structured[field] = values[i]
    else:
        structured[field] = None




import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"  # CPU only
torch.set_num_threads(8)  # adjust to your CPU cores

# Load model
print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("âœ“ Model loaded.\n")


# --------------------------
# Document type detection
# --------------------------
def detect_document_type(text: str) -> str:
    """Detect document type from OCR text"""
    if any(k in text for k in ["ORDRE DE VIREMENT", "virement", "Banque"]):
        return "bank"
    elif any(k in text for k in ["Ø¨Ø·Ø§Ù‚Ø©", "CNI", "Carte"]):
        return "id"
    else:
        return "unknown"


# --------------------------
# Field Schemas
# --------------------------
BANK_FIELDS = [
    "Normale",
    "Montant en chiffres",
    "Montant en lettres",
    "Agence",
    "Nom/PrÃ©nom",
    "Nature du paiement",
    "Adresse complÃ¨te",
    "NumÃ©ro de compte",
    "Date de valeur",
    "SiÃ¨ge",
    "Racine",
    "Ordinal",
    "Devise",
    "ClÃ©",
    "Adresse SWIFT",
    "Autre (Transfert en EURO)",
]

ID_FIELDS = [
    "Ø§Ù„Ù„Ù‚Ø¨",
    "Ø§Ù„Ø§Ø³Ù…",
    "Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯",
    "Ø§Ù„Ø¬Ù†Ø³",
    "Ø±Ù‚Ù… Ø§Ù„ÙˆØ·Ù†ÙŠ",
    "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØµØ¯Ø§Ø±",
    "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡",
    "Ø³Ù„Ø·Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø±",
    "PrÃ©nom",
]


# --------------------------
# Prompt Builder
# --------------------------
def build_prompt(text: str, doc_type: str) -> str:
    """Build CPU-friendly minimal prompt"""
    if doc_type == "bank":
        order = " | ".join(BANK_FIELDS)
    else:
        order = " | ".join(ID_FIELDS)

    prompt = f"""
You are an expert document information extraction system.
Extract the following fields in EXACT order separated by "|":

{order}

Return ONLY values separated by "|".
If a value is missing, return null.
No explanations.

OCR:
{text}
"""
    return prompt.strip()


# --------------------------
# Extraction Function
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    """
    Input: ocr_pages = [{"page": 1, "text": "..."}, ...]
    Output: structured dict with all fields
    """
    # Merge OCR text
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))

    # Detect document type
    doc_type = detect_document_type(full_text)
    if doc_type == "unknown":
        raise ValueError("Unknown document type.")

    # Build prompt
    prompt = build_prompt(full_text, doc_type)

    # Tokenize & generate
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=800).to(DEVICE)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=120,  # CPU-friendly
            do_sample=False,
            use_cache=True,
            num_beams=1,
        )

    # Decode output
    generated = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()

    # Split pipe-separated values
    values = [v.strip() if v.strip().lower() != "null" else None for v in generated.split("|")]

    # Map to fields
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    structured = dict(zip(fields, values))

    # Add document type
    structured["document_type"] = doc_type

    return structured


# --------------------------
# Example Usage
# --------------------------
if __name__ == "__main__":
    # Example OCR input
    ocr_pages = [
        {"page": 1, "text": "ORDRE DE VIREMENT A L'ETRANGER Date: 05/02/2026 Beneficiaire: John Doe Amount: 1000 EUR Account: FR761234567890 Swift: ABCDFRPP Bank: BNP Paribas Agency: Algiers Nature: Salary"},
    ]

    structured = extract_structured_data(ocr_pages)
    print(json.dumps(structured, indent=2, ensure_ascii=False))

















import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

# â”€â”€ Load model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("Loading model...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if device == "cuda" else torch.float32

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=dtype,
    device_map="auto" if device == "cuda" else None,
    trust_remote_code=True,
)
if device == "cpu":
    model = model.to("cpu")
    torch.set_num_threads(8)

model.eval()
print(f"âœ“ Loaded on {device} in {time()-t0:.1f}s\n")

# â”€â”€ Schema per document type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_schema(document_type: str) -> dict:
    if document_type == "bank":
        return {
            "document_type": "ORDRE DE VIREMENT A L'ETRANGER",
            "Monnaie": None,
            "Montant en chiffres": None,
            "Montant en lettres": None,
            "Agence": None,
            "Nom/PrÃ©nom": None,
            "Nature du paiement": None,
            "Adresse complÃ¨te": None,
            "NumÃ©ro de compte": None,
            "Date de valeur": None,
            "SiÃ¨ge": None,      # 5 digits
            "Racine": None,     # 6 digits
            "Ordinal": None,    # 3 digits
            "Devise": None,     # letters
            "ClÃ©": None,        # 2 digits
            "Adresse SWIFT": None,
            "Autre (Transfert en EURO)": None,
        }
    elif document_type == "id_card":
        return {
            "document_type": "Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙˆØ·Ù†ÙŠØ©",
            "Ø§Ù„Ù„Ù‚Ø¨": None,
            "Ø§Ù„Ø§Ø³Ù…": None,
            "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯": None,
            "Ø§Ù„Ø¬Ù†Ø³": None,
            "Ø±Ù‚Ù… Ø§Ù„ÙˆØ·Ù†ÙŠ": None,
            "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØµØ¯Ø§Ø±": None,
            "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡": None,
            "Ø³Ù„Ø·Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø±": None,
            "Nom": None,
            "PrÃ©nom": None,
        }
    else:
        raise ValueError(f"Unknown document type: {document_type}")

# â”€â”€ Detect document type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["Ø¨Ø·Ø§Ù‚Ø©", "Ø§Ù„ØªØ¹Ø±ÙŠÙ", "Ø§Ù„ÙˆØ·Ù†ÙŠØ©", "carte nationale", "cni", "identitÃ©"]):
        return "id_card"
    return "unknown"

# â”€â”€ Parse JSON from output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_json_from_output(text: str) -> dict:
    if "<|im_start|>assistant" in text:
        text = text.split("<|im_start|>assistant")[-1]
    
    # Try ```json block
    json_match = re.search(r'```json\s*(\{.*?)```', text, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(1))
        except json.JSONDecodeError:
            pass
    
    # Try incomplete ```json block
    json_match = re.search(r'```json\s*(\{.*)', text, re.DOTALL)
    if json_match:
        json_str = json_match.group(1).strip()
        if json_str.count('{') > json_str.count('}'):
            json_str += '}'
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass
    
    # Fallback: find { ... }
    start = text.find('{')
    end = text.rfind('}') + 1
    if start != -1 and end > start:
        json_str = text[start:end]
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            if json_str.count('{') > json_str.count('}'):
                json_str += '}'
            return json.loads(json_str)
    
    raise ValueError(f"No valid JSON found in model output:\n{text[:500]}")

# â”€â”€ Extract structured data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))[:1000]  # truncate
    document_type = detect_document_type(full_text)
    schema = get_schema(document_type)
    
    # Extract only field names (keys), not the full schema with None values
    field_names = [k for k in schema.keys() if k != "document_type"]
    
    # OPTIMIZED PROMPT - Much shorter
    prompt = f"""<|im_start|>system
Extract fields to JSON. Return only values, no field names repeated.
<|im_end|>
<|im_start|>user
Fields: {', '.join(field_names)}

Text:
{full_text}

Return JSON with these fields. Use null if missing.
<|im_end|>
<|im_start|>assistant
```json
{{
  "document_type": "{schema['document_type']}"
"""

    print(f"Document type: {document_type}")
    print(f"Prompt tokens: ~{len(tokenizer.encode(prompt))}\n")
    
    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1500).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=250,  # reduced from 350-500
            do_sample=False,
            num_beams=1,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    gen_time = time() - t0
    
    print(f"â± Generation: {gen_time:.2f}s")
    print(f"Raw output:\n{result[:200]}...\n")
    
    # Complete the JSON if needed
    complete_json = '{\n  "document_type": "' + schema['document_type'] + '"\n' + result
    
    return parse_json_from_output(complete_json)

# â”€â”€ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages\n")

    t_start = time()
    structured = extract_structured_data(ocr_pages)
    
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"\nâœ“ Done in {time()-t_start:.1f}s!\n")
    print(json.dumps(structured, indent=2, ensure_ascii=False))












import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

# â”€â”€ Load model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("Loading model...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if device == "cuda" else torch.float32

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=dtype,
    device_map="auto" if device == "cuda" else None,
    trust_remote_code=True,
)
if device == "cpu":
    model = model.to("cpu")
    torch.set_num_threads(8)

model.eval()
print(f"âœ“ Loaded on {device} in {time()-t0:.1f}s\n")

# â”€â”€ Field order (important!) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BANK_FIELDS = [
    "Monnaie",
    "Montant en chiffres", 
    "Montant en lettres",
    "Agence",
    "Nom/PrÃ©nom",
    "Nature du paiement",
    "Adresse complÃ¨te",
    "NumÃ©ro de compte",
    "Date de valeur",
    "SiÃ¨ge",
    "Racine",
    "Ordinal",
    "Devise",
    "ClÃ©",
    "Adresse SWIFT",
    "Autre (Transfert en EURO)"
]

ID_FIELDS = [
    "Ø§Ù„Ù„Ù‚Ø¨",
    "Ø§Ù„Ø§Ø³Ù…",
    "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯",
    "Ø§Ù„Ø¬Ù†Ø³",
    "Ø±Ù‚Ù… Ø§Ù„ÙˆØ·Ù†ÙŠ",
    "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØµØ¯Ø§Ø±",
    "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡",
    "Ø³Ù„Ø·Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø±",
    "Nom",
    "PrÃ©nom"
]

# â”€â”€ Detect document type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["Ø¨Ø·Ø§Ù‚Ø©", "Ø§Ù„ØªØ¹Ø±ÙŠÙ", "Ø§Ù„ÙˆØ·Ù†ÙŠØ©", "carte nationale", "cni", "identitÃ©"]):
        return "id_card"
    return "unknown"

# â”€â”€ Extract with values-only prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))[:1200]
    doc_type = detect_document_type(full_text)
    
    if doc_type == "unknown":
        raise ValueError("Could not detect document type")
    
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    field_list = "\n".join([f"{i+1}. {field}" for i, field in enumerate(fields)])
    
    if doc_type == "bank":
        example = '"020", "318060.13", "trois cent dix-huit mille soixante virgule treize Dinars", "MOSTAGANEM", "ALI SAYED HASSNAT", "virement de trÃ©sorerie", "HARATI, PATS, PASCHIM BARDHAMAN, DURGAPUR, WEST BENGAL PIN 713102", "50100692", null, "97000", "675432", "052", "070", "92", "ABCDINDR", null'
    else:
        example = '"Ø´ÙŠØ§Ù„", "Ù…Ø­Ù…Ø¯", "1956", "Ø°ÙƒØ±", "100581262011435400", "20180121", "20280121", null, "CHIAL", "M HAMED"'
    
    prompt = f"""<|im_start|>system
Extract document fields. Return ONLY values as a comma-separated list in quotes. No field names.
<|im_end|>
<|im_start|>user
Extract these fields IN ORDER:
{field_list}

Rules:
- Return ONLY values in quotes: "value1", "value2", "value3", ...
- If field is missing or empty, write null (no quotes)
- Do NOT include field names
- Do NOT explain anything

Example output format:
{example}

Document text:
{full_text}
<|im_end|>
<|im_start|>assistant
"""

    print(f"Document type: {doc_type}")
    
    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1800).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=False,
            num_beams=1,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    gen_time = time() - t0
    
    print(f"â± Generation: {gen_time:.2f}s")
    print(f"Raw output:\n{result}\n")
    
    # Parse comma-separated values
    result = result.strip()
    if result.startswith('"') or result.startswith("'"):
        # Use regex to split by commas outside quotes
        values = re.findall(r'"([^"]*)"|\bnull\b', result)
        values = [None if v == '' and 'null' in result[i:i+10] else v for i, v in enumerate(values)]
    else:
        # Fallback: simple split
        values = [v.strip().strip('"').strip("'") for v in result.split(",")]
        values = [None if v.lower() == "null" or v == "" else v for v in values]
    
    # Build result dict
    doc_type_name = "ORDRE DE VIREMENT A L'ETRANGER" if doc_type == "bank" else "Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙˆØ·Ù†ÙŠØ©"
    structured = {"document_type": doc_type_name}
    
    for i, field in enumerate(fields):
        structured[field] = values[i] if i < len(values) else None
    
    return structured

# â”€â”€ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages\n")

    t_start = time()
    structured = extract_structured_data(ocr_pages)
    
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"\nâœ“ Done in {time()-t_start:.1f}s!\n")
    print(json.dumps(structured, indent=2, ensure_ascii=False))








import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

# â”€â”€ Load model (optimized for CPU if needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("Loading model...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

# Check if GPU is available
device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if device == "cuda" else torch.float32

if device == "cpu":
    torch.set_num_threads(8)  # adjust to your CPU cores

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=dtype,
    device_map="auto" if device == "cuda" else None,
    trust_remote_code=True,
)
if device == "cpu":
    model = model.to("cpu")
    
model.config.use_cache = True
model.eval()
print(f"âœ“ Loaded on {device} in {time()-t0:.1f}s\n")

# â”€â”€ Field definitions (order matters!) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BANK_FIELDS = [
    "Monnaie",
    "Montant en chiffres",
    "Montant en lettres",
    "Agence",
    "Nom/PrÃ©nom",
    "Nature du paiement",
    "Adresse complÃ¨te",
    "NumÃ©ro de compte",
    "Date de valeur",
    "SiÃ¨ge",      # 5 digits
    "Racine",     # 6 digits
    "Ordinal",    # 3 digits
    "Devise",     # letters
    "ClÃ©",        # 2 digits
    "Adresse SWIFT",
    "Autre (Transfert en EURO)"
]

ID_FIELDS = [
    "Ø§Ù„Ù„Ù‚Ø¨",
    "Ø§Ù„Ø§Ø³Ù…",
    "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯",
    "Ø§Ù„Ø¬Ù†Ø³",
    "Ø±Ù‚Ù… Ø§Ù„ÙˆØ·Ù†ÙŠ",
    "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØµØ¯Ø§Ø±",
    "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡",
    "Ø³Ù„Ø·Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø±",
    "Nom",
    "PrÃ©nom"
]

# â”€â”€ Detect document type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["Ø¨Ø·Ø§Ù‚Ø©", "Ø§Ù„ØªØ¹Ø±ÙŠÙ", "Ø§Ù„ÙˆØ·Ù†ÙŠØ©", "carte nationale", "cni", "identitÃ©"]):
        return "id_card"
    return "unknown"

# â”€â”€ Build minimal prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_prompt(text: str, fields: list, doc_type: str) -> str:
    field_list = " | ".join(fields)
    
    if doc_type == "bank":
        notes = "Note: SiÃ¨ge=5 digits, Racine=6 digits, Ordinal=3 digits, Devise=letters, ClÃ©=2 digits."
    else:
        notes = ""
    
    prompt = f"""<|im_start|>system
Extract document fields.
<|im_end|>
<|im_start|>user
Extract these fields in EXACT order, separated by "|":

{field_list}

Return ONLY values separated by "|". If missing, write "null". No explanation.
{notes}

Text:
{text[:1000]}
<|im_end|>
<|im_start|>assistant
"""
    return prompt

# â”€â”€ Parse pipe-separated output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_output(text: str, fields: list, doc_type: str) -> dict:
    # Clean output
    text = text.strip()
    if "<|im_start|>assistant" in text:
        text = text.split("<|im_start|>assistant")[-1].strip()
    
    # Split by pipe
    values = [v.strip() for v in text.split("|")]
    
    # Build dict
    result = {"document_type": "ORDRE DE VIREMENT A L'ETRANGER" if doc_type == "bank" else "Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙˆØ·Ù†ÙŠØ©"}
    
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            result[field] = None if val.lower() == "null" or val == "" else val
        else:
            result[field] = None
    
    return result

# â”€â”€ Extract â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))
    doc_type = detect_document_type(full_text)
    
    if doc_type == "unknown":
        raise ValueError("Could not detect document type")
    
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    prompt = build_prompt(full_text, fields, doc_type)
    
    print(f"Document: {doc_type}")
    print(f"Prompt length: {len(tokenizer.encode(prompt))} tokens\n")
    
    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1536).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=120,  # â†“ from 350 â€” enough for pipe-separated
            do_sample=False,
            num_beams=1,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    gen_time = time() - t0
    
    print(f"â± Generation: {gen_time:.2f}s")
    print(f"Raw output: {result}\n")
    
    return parse_output(result, fields, doc_type)

# â”€â”€ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages\n")

    t_start = time()
    structured = extract_structured_data(ocr_pages)
    
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"âœ“ Done in {time()-t_start:.1f}s!\n")
    print(json.dumps(structured, indent=2, ensure_ascii=False))













import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# â”€â”€ Use 0.5B model (4x faster than 1.5B) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-0.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

print("Loading 0.5B model (fast)...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
)
model.eval()
print(f"âœ“ Loaded in {time()-t0:.1f}s\n")

def detect_document_type(text: str) -> str:
    return "id_card" if any(k in text for k in ["Ø¨Ø·Ø§Ù‚Ø©", "CNI", "Carte"]) else "bank"

SCHEMAS = {
    "bank": {"Montant": None, "Nom": None, "Compte": None},
    "id_card": {"Ø§Ù„Ù„Ù‚Ø¨": None, "Ø§Ù„Ø§Ø³Ù…": None, "Ø±Ù‚Ù… Ø§Ù„ÙˆØ·Ù†ÙŠ": None, "Nom": None, "PrÃ©nom": None}
}

def parse_json(text: str) -> dict:
    match = re.search(r'\{[^{}]*\}', text)
    return json.loads(match.group(0)) if match else {}

def extract_structured_data(ocr_pages: list) -> dict:
    text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))[:600]
    doc_type = detect_document_type(text)
    schema = SCHEMAS[doc_type]

    prompt = f"""Extract to JSON:
{json.dumps(schema, ensure_ascii=False)}

{text}

{{"""

    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt", max_length=800, truncation=True).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=150, do_sample=False, use_cache=True)
    
    result = "{" + tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    print(f"â± {time()-t0:.2f}s | {result[:100]}...")
    
    return parse_json(result)

if __name__ == "__main__":
    with open(INPUT_JSON, "r") as f:
        ocr_pages = json.load(f)

    structured = extract_structured_data(ocr_pages)
    
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"\nâœ“ Done!\n{json.dumps(structured, indent=2, ensure_ascii=False)}")






import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

# â”€â”€ Load model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("Loading tokenizer and model...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map="auto",
    trust_remote_code=True,
)
model.eval()
print(f"Model loaded in {time()-t0:.1f}s\n")

# â”€â”€ Detect document type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["Ø¨Ø·Ø§Ù‚Ø©", "Ø§Ù„ØªØ¹Ø±ÙŠÙ", "Ø§Ù„ÙˆØ·Ù†ÙŠØ©", "carte nationale", "cni", "identitÃ©"]):
        return "id_card"
    return "unknown"

# â”€â”€ Schema per document type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_schema(document_type: str) -> dict:
    if document_type == "bank":
        return {
            "document_type": "ORDRE DE VIREMENT A L'ETRANGER",
            "Monnaie": None,
            "Montant en chiffres": None,
            "Montant en lettres": None,
            "Agence": None,
            "Nom/PrÃ©nom": None,
            "Nature du paiement": None,
            "Adresse complÃ¨te": None,
            "NumÃ©ro de compte": None,
            "Date de valeur": None,
            "SiÃ¨ge": None,
            "Racine": None,
            "Ordinal": None,
            "Devise": None,
            "ClÃ©": None,
            "Adresse SWIFT": None,
            "Autre (Transfert en EURO)": None,
        }
    elif document_type == "id_card":
        return {
            "document_type": "Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙˆØ·Ù†ÙŠØ©",
            "Ø§Ù„Ù„Ù‚Ø¨": None,
            "Ø§Ù„Ø§Ø³Ù…": None,
            "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯": None,
            "Ø§Ù„Ø¬Ù†Ø³": None,
            "Ø±Ù‚Ù… Ø§Ù„ÙˆØ·Ù†ÙŠ": None,
            "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØµØ¯Ø§Ø±": None,
            "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡": None,
            "Ø³Ù„Ø·Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø±": None,
            "Nom": None,
            "PrÃ©nom": None,
        }
    else:
        raise ValueError(f"Unknown document type: {document_type}")

# â”€â”€ Clean and parse JSON from model output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_json_from_output(text: str) -> dict:
    if "<|im_start|>assistant" in text:
        text = text.split("<|im_start|>assistant")[-1]

    json_match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
    if json_match:
        return json.loads(json_match.group(1))

    start = text.find("{")
    end   = text.rfind("}") + 1
    if start != -1 and end > start:
        return json.loads(text[start:end])

    raise ValueError(f"No valid JSON found in model output:\n{text[:500]}")

# â”€â”€ Main extraction function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))
    document_type = detect_document_type(full_text)
    schema = get_schema(document_type)

    # Shorter, more direct prompt
    prompt = f"""<|im_start|>system
Extract document fields into JSON. Return only valid JSON, no explanation.
<|im_end|>
<|im_start|>user
Schema:
{json.dumps(schema, ensure_ascii=False, indent=2)}

Text:
{full_text[:2000]}
<|im_end|>
<|im_start|>assistant
"""

    print(f"Document type: {document_type}")
    print(f"Prompt length: {len(prompt)} chars\n")
    
    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    prep_time = time() - t0
    print(f"â± Tokenization: {prep_time:.2f}s")

    t0 = time()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=400,        # â†“ from 600
            do_sample=False,
            num_beams=1,               # explicit greedy decode
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    gen_time = time() - t0
    print(f"â± Generation: {gen_time:.2f}s")

    generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    print(f"â± Total: {prep_time + gen_time:.2f}s\n")
    print(f"â”€â”€ Raw output â”€â”€\n{result[:300]}...\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")

    return parse_json_from_output(result)

# â”€â”€ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    t_start = time()
    
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages from {INPUT_JSON}\n")

    structured = extract_structured_data(ocr_pages)

    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    t_total = time() - t_start
    print(f"âœ“ Done in {t_total:.1f}s! Structured data saved to {OUTPUT_JSON}\n")
    print(json.dumps(structured, indent=4, ensure_ascii=False))










import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"   # output from the OCR step
OUTPUT_JSON = "structured.json"

# â”€â”€ Load model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("Loading tokenizer and model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map="auto",
    trust_remote_code=True,
)
model.eval()
print("Model loaded.")

# â”€â”€ Detect document type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["Ø¨Ø·Ø§Ù‚Ø©", "Ø§Ù„ØªØ¹Ø±ÙŠÙ", "Ø§Ù„ÙˆØ·Ù†ÙŠØ©", "carte nationale", "cni", "identitÃ©"]):
        return "id_card"
    return "unknown"

# â”€â”€ Schema per document type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_schema(document_type: str) -> dict:
    if document_type == "bank":
        return {
            "document_type": "ORDRE DE VIREMENT A L'ETRANGER",
            "Monnaie": None,
            "Montant en chiffres": None,
            "Montant en lettres": None,
            "Agence": None,
            "Nom/PrÃ©nom": None,
            "Nature du paiement": None,
            "Adresse complÃ¨te": None,
            "NumÃ©ro de compte": None,
            "Date de valeur": None,
            "SiÃ¨ge": None,
            "Racine": None,
            "Ordinal": None,
            "Devise": None,
            "ClÃ©": None,
            "Adresse SWIFT": None,
            "Autre (Transfert en EURO)": None,
        }
    elif document_type == "id_card":
        return {
            "document_type": "Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙˆØ·Ù†ÙŠØ©",
            "Ø§Ù„Ù„Ù‚Ø¨": None,
            "Ø§Ù„Ø§Ø³Ù…": None,
            "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯": None,
            "Ø§Ù„Ø¬Ù†Ø³": None,
            "Ø±Ù‚Ù… Ø§Ù„ÙˆØ·Ù†ÙŠ": None,
            "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØµØ¯Ø§Ø±": None,
            "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡": None,
            "Ø³Ù„Ø·Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø±": None,
            "Nom": None,
            "PrÃ©nom": None,
        }
    else:
        raise ValueError(f"Unknown document type: {document_type}")

# â”€â”€ Clean and parse JSON from model output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_json_from_output(text: str) -> dict:
    # Strip the prompt echo if present (keep only after last assistant turn)
    if "<|im_start|>assistant" in text:
        text = text.split("<|im_start|>assistant")[-1]

    # Try to find a JSON block (handles ```json ... ``` or raw {...})
    json_match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
    if json_match:
        return json.loads(json_match.group(1))

    # Fallback: find first { ... } block
    start = text.find("{")
    end   = text.rfind("}") + 1
    if start != -1 and end > start:
        return json.loads(text[start:end])

    raise ValueError(f"No valid JSON found in model output:\n{text[:500]}")

# â”€â”€ Main extraction function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))
    document_type = detect_document_type(full_text)
    schema = get_schema(document_type)

    prompt = f"""<|im_start|>system
You are an expert document information extraction system.
The document language may be French, Arabic, or both.
The OCR text may contain noise or formatting errors â€” correct them carefully.
Return ONLY valid JSON matching the schema exactly.
Do not explain anything. If a field is missing, use null.
<|im_end|>
<|im_start|>user
Schema:
{json.dumps(schema, ensure_ascii=False, indent=2)}

OCR TEXT:
{full_text}
<|im_end|>
<|im_start|>assistant
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=600,  # increased slightly for 1.5B model
            do_sample=False,
            temperature=None,
            top_p=None,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Decode only newly generated tokens
    generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    print(f"\nâ”€â”€ Raw model output â”€â”€\n{result}\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")

    return parse_json_from_output(result)

# â”€â”€ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages from {INPUT_JSON}")

    structured = extract_structured_data(ocr_pages)

    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"\nâœ“ Done! Structured data saved to {OUTPUT_JSON}")
    print(json.dumps(structured, indent=4, ensure_ascii=False))







import torch
import json
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM

# =========================================
# 1ï¸âƒ£ LOAD MODEL
# =========================================

model_name = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-32B-Instruct/main"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# =========================================
# 2ï¸âƒ£ MERGE OCR PAGES
# =========================================

def merge_ocr_pages(ocr_output):
    full_text = ""
    for page in ocr_output:
        full_text += page["text"] + "\n"
    return full_text


# =========================================
# 3ï¸âƒ£ DOCUMENT TYPE DETECTION
# =========================================

def detect_document_type(text):

    text_lower = text.lower()

    if "virement" in text_lower and "etranger" in text_lower:
        return "bank"

    elif "Ø§Ù„ØªØ¹Ø±ÙŠÙ" in text and "Ø§Ù„ÙˆØ·Ù†ÙŠØ©" in text:
        return "id"

    else:
        return "unknown"


# =========================================
# 4ï¸âƒ£ STRUCTURED EXTRACTION
# =========================================

def extract_structured_data(ocr_output):

    full_text = merge_ocr_pages(ocr_output)

    document_type = detect_document_type(full_text)

    if document_type == "bank":
        schema = """
{
  "document_type": "foreign_transfer_order",
  "date": "",
  "amount": "",
  "currency": "",
  "beneficiary_name": "",
  "account_number": "",
  "swift_code": "",
  "bank_name": "",
  "agency": "",
  "nature_of_payment": ""
}
"""
    elif document_type == "id":
        schema = """
{
  "document_type": "algerian_id_card",
  "full_name": "",
  "date_of_birth": "",
  "place_of_birth": "",
  "id_number": "",
  "expiration_date": "",
  "nationality": "",
  "blood_group": ""
}
"""
    else:
        raise ValueError("âŒ Unknown document type")

    prompt = f"""
You are an expert document information extraction system.

The OCR text may contain noise or formatting errors.
Extract the correct structured data carefully.

Return ONLY valid JSON.
Do not explain anything.

If a field is missing, return null.

Schema:
{schema}

OCR TEXT:
{full_text}
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=500,
        temperature=0.1
    )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

    json_start = result.find("{")
    json_end = result.rfind("}") + 1
    clean_json = result[json_start:json_end]

    return json.loads(clean_json)


# =========================================
# 5ï¸âƒ£ EXECUTION
# =========================================

# Example OCR output
ocr_output = [
    {"page": 1, "text": "ORDRE DE VIREMENT A L'ETRANGER ..."},
    {"page": 2, "text": "Beneficiaire ..."}
]

structured_data = extract_structured_data(ocr_output)

df = pd.DataFrame([structured_data])

print(df)

df.to_excel("structured_output.xlsx", index=False)

print("âœ… Structured extraction completed")




















OCR text â†’ Structured JSON (via Qwen2.5) â†’ Pandas Table â†’ Excel export




import torch
import json
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from pdf2image import convert_from_path
import pytesseract

# =========================================
# 1ï¸âƒ£ LOAD MODEL
# =========================================

model_name = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-32B-Instruct/main"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# =========================================
# 2ï¸âƒ£ OCR FUNCTION
# =========================================

def extract_text_from_scanned_pdf(pdf_path, lang="eng"):
    pages = convert_from_path(pdf_path)
    full_text = ""

    for page in pages:
        text = pytesseract.image_to_string(page, lang=lang)
        full_text += text + "\n"

    return full_text


# =========================================
# 3ï¸âƒ£ AUTO DOCUMENT TYPE DETECTION
# =========================================

def detect_document_type(ocr_text):

    text_lower = ocr_text.lower()

    if "ordre de virement" in text_lower:
        return "bank"

    elif "Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙˆØ·Ù†ÙŠØ©" in ocr_text:
        return "id"

    else:
        return "unknown"


# =========================================
# 4ï¸âƒ£ STRUCTURED EXTRACTION
# =========================================

def extract_structured_data(ocr_text):

    document_type = detect_document_type(ocr_text)

    if document_type == "bank":
        schema = """
{
  "document_type": "foreign_transfer_order",
  "date": "",
  "amount": "",
  "currency": "",
  "beneficiary_name": "",
  "account_number": "",
  "swift_code": "",
  "bank_name": "",
  "agency": "",
  "nature_of_payment": ""
}
"""

    elif document_type == "id":
        schema = """
{
  "document_type": "algerian_id_card",
  "full_name": "",
  "date_of_birth": "",
  "place_of_birth": "",
  "id_number": "",
  "expiration_date": "",
  "nationality": "",
  "blood_group": ""
}
"""
    else:
        raise ValueError("âŒ Unknown document type")

    prompt = f"""
You are an expert information extraction system.

The OCR text may contain noise.
Extract the correct structured data.

Return ONLY valid JSON.
Do not explain anything.

If a field is missing, return null.

Schema:
{schema}

OCR TEXT:
{ocr_text}
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=500,
        temperature=0.1
    )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

    json_start = result.find("{")
    json_end = result.rfind("}") + 1
    clean_json = result[json_start:json_end]

    return json.loads(clean_json)


# =========================================
# 5ï¸âƒ£ MAIN
# =========================================

pdf_path = "your_scanned_document.pdf"

# OCR
ocr_text = extract_text_from_scanned_pdf(pdf_path, lang="fra")  # change if needed

# Auto extraction
structured_data = extract_structured_data(ocr_text)

# Convert to table
df = pd.DataFrame([structured_data])

print(df)

df.to_excel("structured_output.xlsx", index=False)

print("âœ… Auto detection + extraction completed")






from pypdf import PdfReader, PdfWriter

input_pdf = "input.pdf"        # your original PDF
output_pdf = "first_page.pdf"  # new PDF with only page 1

reader = PdfReader(input_pdf)
writer = PdfWriter()

# Add first page (index 0)
writer.add_page(reader.pages[0])

# Save as new PDF
with open(output_pdf, "wb") as f:
    writer.write(f)

print("First page saved as PDF successfully!")






print(type(image))
print(image.size)
image.save("/mnt/test_page.png")

def extract_text(image):

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": "Extract all visible text from this French document page. Return plain text only."}
            ],
        }
    ]

    # Build proper multimodal prompt
    prompt = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = processor(
        text=[prompt],        # MUST be list
        images=[image],       # MUST be list
        padding=True,
        return_tensors="pt"
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048
        )

    result = processor.batch_decode(
        output, skip_special_tokens=True
    )[0]

    return result









import os
import json
import torch
import fitz  # PyMuPDF
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq

# Paths
MODEL_PATH = "/domina/cdv/modelhub/Modelhub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"

# Load model and processor
processor = AutoProcessor.from_pretrained(
    MODEL_PATH, 
    trust_remote_code=True
)

model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,
    device_map="cpu",
    trust_remote_code=True
)

model.eval()

def pdf_to_images(pdf_path, save_dir="/mnt/pdf_pages", dpi=300):
    """Convert PDF pages to images"""
    os.makedirs(save_dir, exist_ok=True)
    
    doc = fitz.open(pdf_path)
    images = []
    
    for i, page in enumerate(doc):
        # Render page to pixmap
        pix = page.get_pixmap(dpi=dpi)
        
        # Convert to PIL Image
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Save image
        img_path = f"{save_dir}/page_{i+1}.png"
        img.save(img_path)
        print(f"Saved page {i+1}: {img_path} ({img.width}x{img.height})")
        
        images.append(img)
    
    doc.close()
    return images

def extract_text(image):
    """Extract text from image using Qwen2.5-VL"""
    
    # Qwen2.5-VL specific prompt format
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text", 
                    "text": "Extract ALL visible text from this document page. Preserve the structure and formatting. Return plain text only."
                }
            ],
        }
    ]
    
    # Apply chat template
    text = processor.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    # Process inputs
    inputs = processor(
        text=[text],
        images=[image],
        padding=True,
        return_tensors="pt"
    )
    
    # Generate
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=4096,
            temperature=0.1,
            do_sample=False
        )
    
    # Decode output
    generated_ids = [
        output_ids[len(input_ids):]
        for input_ids, output_ids in zip(inputs.input_ids, output_ids)
    ]
    
    result = processor.batch_decode(
        generated_ids, 
        skip_special_tokens=True, 
        clean_up_tokenization_spaces=False
    )[0]
    
    return result

# Main execution
def main():
    OUTPUT_IMG_DIR = "/mnt/pdf_pages"
    
    # Create output directory
    if not os.path.exists(OUTPUT_IMG_DIR):
        os.makedirs(OUTPUT_IMG_DIR)
    
    # Convert PDF to images
    print(f"Converting PDF: {PDF_PATH}")
    images = pdf_to_images(PDF_PATH, save_dir=OUTPUT_IMG_DIR, dpi=300)
    
    # Extract text from each page
    all_text = []
    for i, img in enumerate(images):
        print(f"\nProcessing page {i+1}/{len(images)}")
        try:
            page_text = extract_text(img)
            all_text.append({
                "page": i+1,
                "text": page_text
            })
            print(f"âœ“ Page {i+1} completed")
        except Exception as e:
            print(f"âœ— Error on page {i+1}: {str(e)}")
            all_text.append({
                "page": i+1,
                "text": f"ERROR: {str(e)}"
            })
    
    # Save results
    output_file = "extracted.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_text, f, indent=4, ensure_ascii=False)
    
    print(f"\nâœ“ All text saved to {output_file}")

if __name__ == "__main__":
    main()








import os
import json
import torch
import fitz  # PyMuPDF
from PIL import Image
from transformers import AutoProcessor, Qwen2VLForConditionalGeneration

# Paths
MODEL_PATH = "/domina/cdv/modelhub/Modelhub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"

# Load model and processor
processor = AutoProcessor.from_pretrained(
    MODEL_PATH, 
    trust_remote_code=True
)

model = Qwen2VLForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,  # Use float16 instead of float32 for better performance
    device_map="auto",  # Auto device mapping
    trust_remote_code=True
)

def pdf_to_images(pdf_path, save_dir="/mnt/pdf_pages", dpi=300):
    """Convert PDF pages to images"""
    os.makedirs(save_dir, exist_ok=True)
    
    doc = fitz.open(pdf_path)
    images = []
    
    for i, page in enumerate(doc):
        # Render page to pixmap
        pix = page.get_pixmap(dpi=dpi)
        
        # Convert to PIL Image
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Save image
        img_path = f"{save_dir}/page_{i+1}.png"
        img.save(img_path)
        print(f"Saved page {i+1}: {img_path} ({img.width}x{img.height})")
        
        images.append(img)
    
    doc.close()
    return images

def extract_text(image):
    """Extract text from image using Qwen2.5-VL"""
    
    # Qwen2.5-VL specific prompt format
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text", 
                    "text": "Extract ALL visible text from this document page. Preserve the structure and formatting. Return plain text only."
                }
            ],
        }
    ]
    
    # Apply chat template
    text = processor.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    # Process inputs
    inputs = processor(
        text=[text],
        images=[image],
        padding=True,
        return_tensors="pt"
    )
    
    # Move inputs to same device as model
    inputs = inputs.to(model.device)
    
    # Generate
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=4096,
            temperature=0.1,
            do_sample=False
        )
    
    # Decode output
    generated_ids = [
        output_ids[len(input_ids):]
        for input_ids, output_ids in zip(inputs.input_ids, output_ids)
    ]
    
    result = processor.batch_decode(
        generated_ids, 
        skip_special_tokens=True, 
        clean_up_tokenization_spaces=False
    )[0]
    
    return result

# Main execution
def main():
    OUTPUT_IMG_DIR = "/mnt/pdf_pages"
    
    # Create output directory
    if not os.path.exists(OUTPUT_IMG_DIR):
        os.makedirs(OUTPUT_IMG_DIR)
    
    # Convert PDF to images
    print(f"Converting PDF: {PDF_PATH}")
    images = pdf_to_images(PDF_PATH, save_dir=OUTPUT_IMG_DIR, dpi=300)
    
    # Extract text from each page
    all_text = []
    for i, img in enumerate(images):
        print(f"\nProcessing page {i+1}/{len(images)}")
        try:
            page_text = extract_text(img)
            all_text.append({
                "page": i+1,
                "text": page_text
            })
            print(f"âœ“ Page {i+1} completed")
        except Exception as e:
            print(f"âœ— Error on page {i+1}: {str(e)}")
            all_text.append({
                "page": i+1,
                "text": f"ERROR: {str(e)}"
            })
    
    # Save results
    output_file = "extracted.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_text, f, indent=4, ensure_ascii=False)
    
    print(f"\nâœ“ All text saved to {output_file}")

if __name__ == "__main__":
    main()




import fitz  # PyMuPDF
from PIL import Image
import torch
from transformers import DonutProcessor, VisionEncoderDecoderModel
import json
import os

# =====================
# CONFIG
# =====================
MODEL_NAME = "naver-clova-ix/donut-base-finetuned-docvqa"  # Donut model
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"
OUTPUT_DIR = "/mnt/pdf_images"  # where we save images to check readability
os.makedirs(OUTPUT_DIR, exist_ok=True)

# =====================
# Load Donut Model
# =====================
processor = DonutProcessor.from_pretrained(MODEL_NAME)
model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)
model.eval()

# =====================
# Convert PDF â†’ Images
# =====================
def pdf_to_images(pdf_path, dpi=400):
    doc = fitz.open(pdf_path)
    images = []
    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        image_path = os.path.join(OUTPUT_DIR, f"page_{i+1}.png")
        img.save(image_path)  # Save image so you can check
        images.append(img)
        print(f"Saved page {i+1} image at: {image_path}")
    return images

# =====================
# Extract text from image
# =====================
def extract_text(image):
    # Donut expects a "task prompt" to describe what you want to extract
    prompt = "<s_docvqa> Please extract all text from this page. </s_docvqa>"
    pixel_values = processor(image, return_tensors="pt").pixel_values

    with torch.no_grad():
        output_ids = model.generate(pixel_values, max_length=4096)
    
    result = processor.batch_decode(output_ids, skip_special_tokens=True)[0]
    return result

# =====================
# MAIN
# =====================
images = pdf_to_images(PDF_PATH, dpi=400)

all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}...")
    page_text = extract_text(img)
    all_text.append({
        "page": i+1,
        "text": page_text
    })

# Save results
with open("/mnt/extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("Done! Text extracted to /mnt/extracted.json")















def pdf_to_model_images(pdf_path, size=1024):
    import os
    import fitz
    from PIL import Image

    save_dir = "/mnt/pdf_pages"
    os.makedirs(save_dir, exist_ok=True)
    doc = fitz.open(pdf_path)
    images = []

    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)  # keep 300 DPI, not too big
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        # Resize to model-friendly square
        img.thumbnail((size, size), Image.LANCZOS)
        square_img = Image.new("RGB", (size, size), "white")
        square_img.paste(img, (0, 0))

        img_path = f"{save_dir}/page_{i+1}.png"
        square_img.save(img_path)
        print(f"Saved {img_path} ({square_img.width}x{square_img.height})")
        images.append(square_img)

    return images





def pdf_to_images_readable(pdf_path, save_dir="/mnt/pdf_pages", dpi=400):
    os.makedirs(save_dir, exist_ok=True)
    doc = fitz.open(pdf_path)
    images = []

    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        # Pad to square without resizing smaller
        max_side = max(img.width, img.height)
        img_square = Image.new("RGB", (max_side, max_side), "white")
        img_square.paste(img, (0, 0))

        img_path = f"{save_dir}/page_{i+1}.png"
        img_square.save(img_path)
        print(f"Saved readable page {i+1} image: {img_path} ({img_square.width}x{img_square.height})")
        images.append(img_square)

    return images




def pdf_to_images(pdf_path, save_dir="/mnt/pdf_pages"):
    doc = fitz.open(pdf_path)
    images = []
    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        # --- Resize and convert to RGB for Qwen ---
        img = img.convert("RGB")
        img = img.resize((512, 512))  # adjust size if needed

        images.append(img)

        # Save the image to check
        img_path = f"{save_dir}/page_{i+1}.png"
        img.save(img_path)
        print(f"âœ“ Saved page {i+1} as image: {img_path}")

    return images






OUTPUT_IMG_DIR = "/mnt/pdf_pages"

# Make sure output folder exists
if not os.path.exists(OUTPUT_IMG_DIR):
    os.makedirs(OUTPUT_IMG_DIR)

img_path = f"{OUTPUT_IMG_DIR}/page_{i+1}.png"





OUTPUT_IMG_DIR = "/mnt/user-data/pdf_pages"  # folder to save images
Path(OUTPUT_IMG_DIR).mkdir(parents=True, exist_ok=True)


def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)

        # Save the image to disk for inspection
        img_path = Path(OUTPUT_IMG_DIR) / f"page_{i+1}.png"
        img.save(img_path)
        print(f"âœ“ Saved page {i+1} as image: {img_path}")

    return images











def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.

<image>
"""  # <-- crucial marker

    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result





import fitz  # PyMuPDF
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq
import torch
import json

# =====================
# CONFIG
# =====================
MODEL_PATH = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"

# =====================
# LOAD MODEL
# =====================
print("Loading model...")
processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,  # CPU safe
    trust_remote_code=True
)
model.eval()
print("Model loaded.")

# =====================
# PDF â†’ Images
# =====================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page in doc:
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# =====================
# Extract text from an image
# =====================
def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.
"""

    # Prepare input
    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    )

    # Forward pass
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0
        )

    # Decode result
    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# =====================
# MAIN
# =====================
images = pdf_to_images(PDF_PATH)

all_text = []
for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}")
    page_text = extract_text(img)
    all_text.append({
        "page": i+1,
        "text": page_text
    })

# Save results
with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("âœ… Done! Text saved to extracted.json")













import fitz  # PyMuPDF
from PIL import Image
import torch
from transformers import AutoProcessor, AutoModelForVision2Seq
import json

# =====================
# CONFIG
# =====================
MODEL_PATH = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"
DEVICE = "cpu"  # force CPU

# =====================
# Load Qwen Model
# =====================
print("Loading model...")
processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,  # CPU safe
    trust_remote_code=True
).to(DEVICE)
model.eval()
print("Model loaded.")

# =====================
# PDF â†’ Images
# =====================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page in doc:
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# =====================
# Extract Text per Page
# =====================
def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.
"""
    # Prepare inputs
    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    ).to(DEVICE)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# =====================
# Main
# =====================
images = pdf_to_images(PDF_PATH)
all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}")
    page_text = extract_text(img)
    all_text.append({
        "page": i+1,
        "text": page_text
    })

# Save JSON
with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("âœ… Done! Saved to extracted.json")









import fitz  # PyMuPDF
from PIL import Image
import torch
from transformers import AutoProcessor, AutoModelForVision2Seq
import json

# =====================
# CONFIG
# =====================
MODEL_PATH = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"
DEVICE = "cpu"  # force CPU

# =====================
# Load Qwen Model
# =====================
print("Loading model...")
processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,  # CPU safe
    trust_remote_code=True
).to(DEVICE)
model.eval()
print("Model loaded.")

# =====================
# PDF â†’ Images
# =====================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page in doc:
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# =====================
# Extract Text per Page
# =====================
def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.
"""
    # Prepare inputs
    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    ).to(DEVICE)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# =====================
# Main
# =====================
images = pdf_to_images(PDF_PATH)
all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}")
    page_text = extract_text(img)
    all_text.append({
        "page": i+1,
        "text": page_text
    })

# Save JSON
with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("âœ… Done! Saved to extracted.json")







import fitz  # PyMuPDF
from PIL import Image
import easyocr
import json

PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"

# Initialize EasyOCR reader (French + English)
reader = easyocr.Reader(['fr', 'en'], gpu=False)

# Convert PDF â†’ Images
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page in doc:
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# OCR each image
def extract_text_from_images(images):
    results = []
    for i, img in enumerate(images):
        print(f"Processing page {i+1}/{len(images)}")
        ocr_result = reader.readtext(np.array(img), detail=0)  # detail=0 â†’ just text
        page_text = "\n".join(ocr_result)
        results.append({"page": i+1, "text": page_text})
    return results

# Main
images = pdf_to_images(PDF_PATH)
all_text = extract_text_from_images(images)

# Save JSON
with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("âœ… Done! Saved to extracted.json")






pip install --upgrade --no-cache-dir torch

pip install --upgrade --no-cache-dir transformers>=4.40.0



import torch
import fitz
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq
import json

# =====================
# CONFIG
# =====================
MODEL_PATH = "/domino/edv/modelhub/.../Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/your_file.pdf"

# =====================
# Load Model (CPU ONLY)
# =====================
print("Loading model on CPU...")

processor = AutoProcessor.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True
)

model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,   # CPU safe
    trust_remote_code=True
)

model.eval()

print("Model loaded.")

# =====================
# Convert PDF â†’ Images
# =====================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []

    for page in doc:
        pix = page.get_pixmap(dpi=200)  # lower DPI for CPU
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)

    return images

# =====================
# OCR Function
# =====================
def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.
"""

    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=1024,   # smaller for CPU
            temperature=0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# =====================
# MAIN
# =====================
images = pdf_to_images(PDF_PATH)

all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)} (CPU mode, may be slow)")
    page_text = extract_text(img)

    all_text.append({
        "page": i + 1,
        "text": page_text
    })

with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("Done. Saved to extracted.json")











from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "Qwen/Qwen2.5-3B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

inputs = tokenizer("Explain transformers briefly:", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))






from transformers import AutoProcessor, AutoModelForVision2Seq

model = AutoModelForVision2Seq.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    trust_remote_code=True
)

processor = AutoProcessor.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    trust_remote_code=True
)

pip uninstall qwen -y
pip uninstall zeta -y

pip install transformers accelerate torch




Bonjour,

Veuillez trouver en piÃ¨ce jointe les rÃ©sultats de Sentinella pour le trimestre arrÃªtÃ© Ã  fin fÃ©vrier 2026.

Les donnÃ©es couvrent la pÃ©riode du 01/01/2024 au 28/02/2026 et prÃ©sentent la rÃ©partition des communautÃ©s selon le nombre de clients ainsi que les montants par pays.

En rÃ©sumÃ© :

- 64 communautÃ©s au total
- Montant global : 5 346 762 â‚¬

La ventilation dÃ©taillÃ©e par rubrique et par pays est disponible dans le fichier joint.

Pour toute question ou besoin de complÃ©ment d'information, Nihad prendra en charge la suite.

Cordialement,
Omar



#!/usr/bin/env python3
"""
Extract text from multi-page scanned PDFs using OpenAI GPT-4V (Vision).
Each PDF page is converted to an image and processed individually.
"""

import base64
from pathlib import Path
from typing import List, Dict
import fitz  # PyMuPDF
from openai import OpenAI


# -----------------------------
# 1ï¸âƒ£ Convert PDF pages to images
# -----------------------------
def pdf_to_images(pdf_path: str) -> List[str]:
    """
    Convert each page of PDF to a PNG image and return list of image file paths.
    """
    pdf_file = Path(pdf_path)
    if not pdf_file.exists():
        raise FileNotFoundError(f"PDF not found: {pdf_path}")
    
    doc = fitz.open(pdf_file)
    images = []

    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)
        img_path = f"{pdf_file.stem}_page_{i+1}.png"
        pix.save(img_path)
        images.append(img_path)
    
    print(f"âœ“ Converted {len(images)} pages to images.")
    return images


# -----------------------------
# 2ï¸âƒ£ Encode image to base64
# -----------------------------
def encode_image_to_base64(image_path: str) -> str:
    """
    Encode image to base64 for GPT-4V input.
    """
    path = Path(image_path)
    if not path.exists():
        raise FileNotFoundError(f"Image not found: {image_path}")

    with open(path, "rb") as f:
        encoded = base64.b64encode(f.read()).decode("utf-8")
    
    return encoded


# -----------------------------
# 3ï¸âƒ£ Extract text from image using GPT-4V
# -----------------------------
def extract_text_gpt4v(api_key: str, base64_image: str, model: str = "gpt-4o") -> Dict:
    """
    Send image to GPT-4V and return extracted text.
    """
    client = OpenAI(api_key=api_key)

    # Default to JPEG MIME
    mime_type = "image/png"  # since we converted pages to PNG

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": """Extract ALL text from this image with maximum accuracy.
Preserve structure, tables, line breaks, and French text exactly.
Output markdown for tables."""
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:{mime_type};base64,{base64_image}"}
                    }
                ]
            }],
            max_tokens=4096,
            temperature=0
        )
        extracted = response.choices[0].message.content
        return {"success": True, "text": extracted, "tokens_used": response.usage.total_tokens}
    except Exception as e:
        return {"success": False, "error": str(e)}


# -----------------------------
# 4ï¸âƒ£ Process multi-page PDF
# -----------------------------
def extract_text_from_pdf(api_key: str, pdf_path: str) -> str:
    """
    Convert PDF to images, extract text for each page, and combine results.
    """
    images = pdf_to_images(pdf_path)
    full_text = ""

    for i, img_path in enumerate(images):
        print(f"ðŸ”„ Processing page {i+1}/{len(images)}: {img_path}")
        b64 = encode_image_to_base64(img_path)
        result = extract_text_gpt4v(api_key, b64)

        if result["success"]:
            full_text += f"\n\n---\n\n## Page {i+1}\n\n"
            full_text += result["text"]
        else:
            full_text += f"\n\n---\n\n## Page {i+1}\n\nâŒ Extraction failed: {result['error']}\n"
    
    return full_text


# -----------------------------
# 5ï¸âƒ£ Save extracted text
# -----------------------------
def save_text(text: str, output_path: str):
    """
    Save extracted text to a file.
    """
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(text)
    print(f"âœ“ Extracted text saved to {output_path}")


# -----------------------------
# 6ï¸âƒ£ Main function
# -----------------------------
def main():
    PDF_PATH = "/mnt/user-data/uploads/scanned_document.pdf"
    OUTPUT_PATH = "/mnt/user-data/outputs/extracted_text_gpt4v.md"
    OPENAI_API_KEY = input("Enter your OpenAI API key: ").strip()

    print("ðŸ”¹ Starting PDF text extraction using GPT-4V...")
    extracted_text = extract_text_from_pdf(OPENAI_API_KEY, PDF_PATH)
    save_text(extracted_text, OUTPUT_PATH)
    print("âœ… Extraction complete!")


if __name__ == "__main__":
    main()




#!/usr/bin/env python3
"""
Text extraction from images using Vision-Language Models (VLMs)
Supports: OpenAI GPT-4V, Anthropic Claude, Google Gemini
No OCR preprocessing required
"""

import os
import base64
from pathlib import Path
from typing import Optional, Dict, Any
import json

# Installation instructions:
# pip install openai anthropic google-generativeai pillow --break-system-packages

class VLMTextExtractor:
    """Extract text from images using various VLM models"""
    
    def __init__(self):
        self.image_path = None
        self.base64_image = None
    
    def load_image(self, image_path: str) -> None:
        """Load and encode image to base64"""
        self.image_path = Path(image_path)
        if not self.image_path.exists():
            raise FileNotFoundError(f"Image not found: {image_path}")
        
        with open(self.image_path, "rb") as image_file:
            self.base64_image = base64.b64encode(image_file.read()).decode('utf-8')
        
        print(f"âœ“ Loaded image: {self.image_path.name}")
    
    def extract_with_gpt4v(self, api_key: str, model: str = "gpt-4o") -> Dict[str, Any]:
        """
        Extract text using OpenAI GPT-4 Vision
        Models: gpt-4o, gpt-4o-mini, gpt-4-turbo
        """
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=api_key)
            
            # Determine image format
            ext = self.image_path.suffix.lower()
            mime_type = {
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.png': 'image/png',
                '.webp': 'image/webp',
                '.gif': 'image/gif'
            }.get(ext, 'image/jpeg')
            
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": """Extract ALL text from this image with maximum accuracy. 
                                
Instructions:
1. Preserve the exact structure and layout
2. Maintain all tables, headers, and formatting
3. Include ALL numerical values, percentages, and currency amounts
4. Keep French text exactly as shown
5. Preserve line breaks and spacing where meaningful
6. Output in markdown format for tables

Please extract the complete text."""
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:{mime_type};base64,{self.base64_image}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=4096,
                temperature=0
            )
            
            extracted_text = response.choices[0].message.content
            
            return {
                "success": True,
                "model": model,
                "text": extracted_text,
                "tokens_used": response.usage.total_tokens
            }
            
        except Exception as e:
            return {
                "success": False,
                "model": model,
                "error": str(e)
            }
    
    def extract_with_claude(self, api_key: str, model: str = "claude-3-5-sonnet-20241022") -> Dict[str, Any]:
        """
        Extract text using Anthropic Claude Vision
        Models: claude-3-5-sonnet-20241022, claude-3-opus-20240229, claude-3-haiku-20240307
        """
        try:
            from anthropic import Anthropic
            
            client = Anthropic(api_key=api_key)
            
            # Determine media type
            ext = self.image_path.suffix.lower()
            media_type = {
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.png': 'image/png',
                '.webp': 'image/webp',
                '.gif': 'image/gif'
            }.get(ext, 'image/jpeg')
            
            message = client.messages.create(
                model=model,
                max_tokens=4096,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": media_type,
                                    "data": self.base64_image,
                                },
                            },
                            {
                                "type": "text",
                                "text": """Extract ALL text from this image with maximum accuracy.

Instructions:
1. Preserve the exact structure and layout
2. Maintain all tables, headers, and formatting
3. Include ALL numerical values, percentages, and currency amounts
4. Keep French text exactly as shown
5. Preserve line breaks and spacing where meaningful
6. Output in markdown format for tables

Please extract the complete text."""
                            }
                        ],
                    }
                ],
            )
            
            extracted_text = message.content[0].text
            
            return {
                "success": True,
                "model": model,
                "text": extracted_text,
                "tokens_used": message.usage.input_tokens + message.usage.output_tokens
            }
            
        except Exception as e:
            return {
                "success": False,
                "model": model,
                "error": str(e)
            }
    
    def extract_with_gemini(self, api_key: str, model: str = "gemini-1.5-pro") -> Dict[str, Any]:
        """
        Extract text using Google Gemini Vision
        Models: gemini-1.5-pro, gemini-1.5-flash, gemini-pro-vision
        """
        try:
            import google.generativeai as genai
            from PIL import Image
            
            genai.configure(api_key=api_key)
            
            model_instance = genai.GenerativeModel(model)
            
            # Load image with PIL
            img = Image.open(self.image_path)
            
            prompt = """Extract ALL text from this image with maximum accuracy.

Instructions:
1. Preserve the exact structure and layout
2. Maintain all tables, headers, and formatting
3. Include ALL numerical values, percentages, and currency amounts
4. Keep French text exactly as shown
5. Preserve line breaks and spacing where meaningful
6. Output in markdown format for tables

Please extract the complete text."""
            
            response = model_instance.generate_content([prompt, img])
            
            return {
                "success": True,
                "model": model,
                "text": response.text,
                "tokens_used": "N/A (not provided by Gemini API)"
            }
            
        except Exception as e:
            return {
                "success": False,
                "model": model,
                "error": str(e)
            }
    
    def save_results(self, results: Dict[str, Any], output_path: str) -> None:
        """Save extraction results to file"""
        output_file = Path(output_path)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"# Text Extraction Results\n\n")
            f.write(f"**Source Image:** {self.image_path.name}\n")
            f.write(f"**Model:** {results['model']}\n")
            f.write(f"**Success:** {results['success']}\n")
            
            if results['success']:
                f.write(f"**Tokens Used:** {results.get('tokens_used', 'N/A')}\n\n")
                f.write("---\n\n")
                f.write("## Extracted Text\n\n")
                f.write(results['text'])
            else:
                f.write(f"**Error:** {results['error']}\n")
        
        print(f"âœ“ Results saved to: {output_file}")


def main():
    """Main execution function"""
    print("=" * 70)
    print("VLM Text Extraction Tool")
    print("=" * 70)
    print("\nSupported Models:")
    print("  1. OpenAI GPT-4V (gpt-4o, gpt-4o-mini)")
    print("  2. Anthropic Claude (claude-3-5-sonnet, claude-3-opus)")
    print("  3. Google Gemini (gemini-1.5-pro, gemini-1.5-flash)")
    print("\n")
    
    # Configuration
    IMAGE_PATH = "/mnt/user-data/uploads/1000024457.webp"
    
    # Initialize extractor
    extractor = VLMTextExtractor()
    extractor.load_image(IMAGE_PATH)
    
    print("\n" + "=" * 70)
    print("Select extraction method:")
    print("=" * 70)
    print("1. OpenAI GPT-4V")
    print("2. Anthropic Claude")
    print("3. Google Gemini")
    print("4. Extract with all models (requires all API keys)")
    print("\n")
    
    choice = input("Enter your choice (1-4): ").strip()
    
    results = []
    
    if choice == "1":
        api_key = input("\nEnter your OpenAI API key: ").strip()
        model = input("Enter model name (default: gpt-4o): ").strip() or "gpt-4o"
        print(f"\nðŸ”„ Extracting text using {model}...")
        result = extractor.extract_with_gpt4v(api_key, model)
        results.append(("gpt4v", result))
        
    elif choice == "2":
        api_key = input("\nEnter your Anthropic API key: ").strip()
        model = input("Enter model name (default: claude-3-5-sonnet-20241022): ").strip() or "claude-3-5-sonnet-20241022"
        print(f"\nðŸ”„ Extracting text using {model}...")
        result = extractor.extract_with_claude(api_key, model)
        results.append(("claude", result))
        
    elif choice == "3":
        api_key = input("\nEnter your Google API key: ").strip()
        model = input("Enter model name (default: gemini-1.5-pro): ").strip() or "gemini-1.5-pro"
        print(f"\nðŸ”„ Extracting text using {model}...")
        result = extractor.extract_with_gemini(api_key, model)
        results.append(("gemini", result))
        
    elif choice == "4":
        openai_key = input("\nEnter your OpenAI API key: ").strip()
        claude_key = input("Enter your Anthropic API key: ").strip()
        gemini_key = input("Enter your Google API key: ").strip()
        
        print("\nðŸ”„ Extracting with GPT-4V...")
        results.append(("gpt4v", extractor.extract_with_gpt4v(openai_key)))
        
        print("ðŸ”„ Extracting with Claude...")
        results.append(("claude", extractor.extract_with_claude(claude_key)))
        
        print("ðŸ”„ Extracting with Gemini...")
        results.append(("gemini", extractor.extract_with_gemini(gemini_key)))
    
    else:
        print("âŒ Invalid choice!")
        return
    
    # Save results
    print("\n" + "=" * 70)
    print("Saving Results")
    print("=" * 70)
    
    for name, result in results:
        if result['success']:
            output_path = f"/mnt/user-data/outputs/extracted_text_{name}.md"
            extractor.save_results(result, output_path)
            print(f"âœ“ {name.upper()}: Extraction successful")
            print(f"  Preview: {result['text'][:100]}...")
        else:
            print(f"âŒ {name.upper()}: Extraction failed - {result['error']}")
    
    print("\nâœ… Process complete!")


if __name__ == "__main__":
    main()






import fitz  # PyMuPDF
from PIL import Image
from transformers import DonutProcessor, VisionEncoderDecoderModel

# -----------------------------
# ðŸ”¥ CHANGE THIS
# -----------------------------
MODEL_PATH = "/path_to_your_local_donut_model"
PDF_PATH = "your_scanned_file.pdf"

# -----------------------------
# 1ï¸âƒ£ Convert PDF â†’ Images
# -----------------------------
def pdf_to_images(pdf_path, dpi=200):
    doc = fitz.open(pdf_path)
    images = []

    for page in doc:
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)

    return images

# -----------------------------
# 2ï¸âƒ£ Load Model (CPU Only)
# -----------------------------
print("Loading local Donut model (CPU mode)...")

processor = DonutProcessor.from_pretrained(MODEL_PATH, local_files_only=True)
model = VisionEncoderDecoderModel.from_pretrained(MODEL_PATH, local_files_only=True)
model.eval()

print("Model loaded successfully âœ…")

# -----------------------------
# 3ï¸âƒ£ Donut Extraction Function
# -----------------------------
def run_donut(image):
    # Resize for CPU stability
    image = image.resize((1280, 1280))

    prompt = (
        "<s_docvqa>"
        "<s_question>Extract all information from this document</s_question>"
        "<s_answer>"
    )

    pixel_values = processor(image, return_tensors="pt").pixel_values
    decoder_input_ids = processor.tokenizer(
        prompt,
        add_special_tokens=False,
        return_tensors="pt"
    ).input_ids

    outputs = model.generate(
        pixel_values,
        decoder_input_ids=decoder_input_ids,
        max_length=512,  # lower for CPU
        early_stopping=True,
        pad_token_id=processor.tokenizer.pad_token_id,
        eos_token_id=processor.tokenizer.eos_token_id,
    )

    result = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return result

# -----------------------------
# 4ï¸âƒ£ Process PDF
# -----------------------------
pages = pdf_to_images(PDF_PATH)
results = []

for i, page in enumerate(pages):
    print(f"Processing page {i+1}/{len(pages)}")
    text = run_donut(page)
    results.append(text)

final_text = "\n\n".join(results)
print("\n========== FINAL RESULT ==========\n")
print(final_text)






import torch
import fitz  # PyMuPDF
from PIL import Image
from transformers import DonutProcessor, VisionEncoderDecoderModel

# -----------------------------
# 1ï¸âƒ£ Convert PDF to Images
# -----------------------------
def pdf_to_images(pdf_path, dpi=300):
    doc = fitz.open(pdf_path)
    images = []

    for page in doc:
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)

    return images


# -----------------------------
# 2ï¸âƒ£ Load Donut Model
# -----------------------------
model_name = "naver-clova-ix/donut-base"

processor = DonutProcessor.from_pretrained(model_name)
model = VisionEncoderDecoderModel.from_pretrained(model_name)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)


# -----------------------------
# 3ï¸âƒ£ Run Donut on Image
# -----------------------------
def run_donut(image, task_prompt="<s_docvqa><s_question>Extract all information</s_question><s_answer>"):
    
    pixel_values = processor(image, return_tensors="pt").pixel_values.to(device)

    decoder_input_ids = processor.tokenizer(
        task_prompt,
        add_special_tokens=False,
        return_tensors="pt"
    ).input_ids.to(device)

    outputs = model.generate(
        pixel_values,
        decoder_input_ids=decoder_input_ids,
        max_length=1024,
        early_stopping=True,
        pad_token_id=processor.tokenizer.pad_token_id,
        eos_token_id=processor.tokenizer.eos_token_id,
    )

    result = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return result


# -----------------------------
# 4ï¸âƒ£ Process Full PDF
# -----------------------------
pdf_path = "your_scanned.pdf"

images = pdf_to_images(pdf_path)

all_results = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}...")
    result = run_donut(img)
    all_results.append(result)

# Combine all pages
final_output = "\n\n".join(all_results)

print("\n===== FINAL OUTPUT =====\n")
print(final_output)







pip install transformers torch torchvision pillow pymupdf sentencepiece



task_prompt = """
<s_docvqa>
<s_question>
Extract all structured information and return valid JSON.
</s_question>
<s_answer>
"""






pip install paddlepaddle paddleocr


import fitz
from PIL import Image

doc = fitz.open("your_scanned.pdf")
page = doc[0]

pix = page.get_pixmap(dpi=300)
img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
img.save("page.png")


from paddleocr import PaddleOCR

ocr = PaddleOCR(lang='en')
result = ocr.ocr("page.png")

text = "\n".join([line[1][0] for line in result[0]])
print(text)





import fitz
from PIL import Image

def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

images = pdf_to_images("scanned.pdf")





pip uninstall qwen zeta transformers torch triton -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install qwen==0.1.1 qwen-agent qwen-vl-utils
pip install transformers==4.39.1 triton==2.2.0


# Uninstall potentially conflicting packages
pip uninstall qwen zeta -y

# Install PyTorch first (if not already installed)
pip install torch torchvision --break-system-packages

# Install the correct Qwen package
pip install transformers --break-system-packages
pip install qwen-vl-utils --break-system-packages




pip install torch torchvision torchaudio  # Core PyTorch
pip install pillow                         # Image handling
pip install pymupdf                        # PDF â†’ images
pip install qwen                           # Official Qwen VL package




pip install tqdm                            # Progress bars
pip install numpy                           # Array operations (often required by Qwen)
pip install transformers                     # Only if you need extra transformer utils


pip install tqdm                            # Progress bars
pip install numpy                           # Array operations (often required by Qwen)
pip install transformers                     # Only if you need extra transformer utils






import fitz  # PyMuPDF
import json
from PIL import Image
from qwen import QwenVL, QwenProcessor  # official Qwen VL loader

# ===============================
# CONFIG
# ===============================
MODEL_PATH = "/path/to/Qwen2.5-VL-32B-Instruct"  # your local folder
PDF_PATH = "scanned.pdf"
OUTPUT_FILE = "output.json"

# ===============================
# LOAD MODEL
# ===============================
print("Loading Qwen2.5-VL-32B-Instruct...")
processor = QwenProcessor.from_pretrained(MODEL_PATH)
model = QwenVL.from_pretrained(MODEL_PATH)
model.eval()  # CPU by default
print("Model loaded successfully.")

# ===============================
# CONVERT PDF TO IMAGES
# ===============================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page_number in range(len(doc)):
        page = doc[page_number]
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# ===============================
# EXTRACT JSON FROM IMAGE
# ===============================
def extract_json_from_image(image):
    prompt = """
You are a document extraction engine.

Extract ALL visible text and structure from this scanned document.

Return strictly valid JSON only.
Do not add explanations or summaries.
"""

    # Preprocess the image + prompt
    inputs = processor(prompt, image, return_tensors="pt")

    # Generate the model output
    with torch.no_grad():
        output = model.generate(**inputs, max_new_tokens=2048, temperature=0.0)

    # Decode text
    text_output = processor.decode(output[0])
    return text_output

# ===============================
# MAIN PIPELINE
# ===============================
def main():
    images = pdf_to_images(PDF_PATH)
    all_pages = []

    for i, img in enumerate(images):
        print(f"Processing page {i+1}...")
        result_text = extract_json_from_image(img)

        # Attempt to parse JSON, fallback to raw output
        try:
            parsed = json.loads(result_text)
        except json.JSONDecodeError:
            parsed = {"raw_output": result_text}

        all_pages.append({
            "page_number": i + 1,
            "content": parsed
        })

    final_output = {
        "document_name": PDF_PATH,
        "total_pages": len(all_pages),
        "pages": all_pages
    }

    # Save to JSON file
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)

    print(f"Extraction complete. Saved to {OUTPUT_FILE}")

# ===============================
# ENTRY POINT
# ===============================
if __name__ == "__main__":
    main()




import fitz  # PyMuPDF
import json
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq

# ===============================
# CONFIG
# ===============================
MODEL_PATH = "/path/to/Qwen2.5-VL-32B-Instruct"  # Your local folder
PDF_PATH = "scanned.pdf"
OUTPUT_FILE = "output.json"

# ===============================
# Load Model
# ===============================
print("Loading model...")
processor = AutoProcessor.from_pretrained(MODEL_PATH)
model = AutoModelForVision2Seq.from_pretrained(MODEL_PATH)
model.eval()
print("Model loaded.")

# ===============================
# Convert PDF to Images
# ===============================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page_number in range(len(doc)):
        page = doc[page_number]
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# ===============================
# Extract JSON from Image
# ===============================
def extract_json_from_image(image):
    prompt = """
You are a document extraction engine.

Extract ALL visible text and structure from this scanned document.

Return strictly valid JSON.
Do not add explanations.
"""

    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0.0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# ===============================
# MAIN
# ===============================
def main():
    images = pdf_to_images(PDF_PATH)
    all_pages = []

    for i, img in enumerate(images):
        print(f"Processing page {i+1}...")
        result = extract_json_from_image(img)

        try:
            parsed = json.loads(result)
        except:
            parsed = {"raw_output": result}

        all_pages.append({
            "page_number": i + 1,
            "content": parsed
        })

    final_output = {
        "document_name": PDF_PATH,
        "total_pages": len(all_pages),
        "pages": all_pages
    }

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)

    print(f"Extraction complete. Saved to {OUTPUT_FILE}")

if __name__ == "__main__":
    main()





pip install torch torchvision transformers pymupdf pillow




if "chat_sessions" not in st.session_state:
    st.session_state.chat_sessions = load_chat_sessions()

import json
import os

CHAT_STORAGE_PATH = "chat_sessions.json"


def load_chat_sessions():
    if os.path.exists(CHAT_STORAGE_PATH):
        with open(CHAT_STORAGE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_chat_sessions(chat_sessions):
    with open(CHAT_STORAGE_PATH, "w", encoding="utf-8") as f:
        json.dump(chat_sessions, f, indent=2, ensure_ascii=False)









if st.sidebar.button("âž• Start New Chat"):
    chat_id = str(datetime.datetime.now())
    st.session_state.current_chat = chat_id
    st.session_state.chat_sessions[chat_id] = {
        "title": "New Chat",
        "messages": []
    }
    save_chat_sessions(st.session_state.chat_sessions)  # âœ… ADD THIS





chat_data["messages"].append({
    "role": "user",
    "content": user_input,
    "timestamp": timestamp
})

save_chat_sessions(st.session_state.chat_sessions)  # âœ… ADD THIS






import os
import glob

from langchain.document_loaders import (
    PyPDFLoader,
    CSVLoader,
    UnstructuredExcelLoader,
    UnstructuredWordDocumentLoader
)

# Your real documents directory
DOCS_DIR = "ant/"

def load_documents():
    documents = []

    procedures_dir = os.path.join(DOCS_DIR, "Procedures")

    # ---------- Load PDFs ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.pdf")):
            try:
                loader = PyPDFLoader(file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "pdf"

                documents.extend(docs)
                print(f"Loaded PDF: {file_name}")

            except Exception as e:
                print(f"Error loading PDF {file}: {e}")

    # ---------- Load CSVs ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.csv")):
            try:
                loader = CSVLoader(file_path=file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "csv"

                documents.extend(docs)
                print(f"Loaded CSV: {file_name}")

            except Exception as e:
                print(f"Error loading CSV {file}: {e}")

    # ---------- Load Excel ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.xlsx")):
            try:
                loader = UnstructuredExcelLoader(file_path=file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "excel"

                documents.extend(docs)
                print(f"Loaded Excel: {file_name}")

            except Exception as e:
                print(f"Error loading Excel {file}: {e}")

    # ---------- Load Word (.docx) ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.docx")):
            try:
                loader = UnstructuredWordDocumentLoader(file_path=file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "word"

                documents.extend(docs)
                print(f"Loaded Word: {file_name}")

            except Exception as e:
                print(f"Error loading Word {file}: {e}")

    print(f"\nLoaded {len(documents)} documents in total.")
    return documents













def main():
    st.title("ðŸ’¬ Chatbot BNP ED")

    # -----------------------------
    # 1ï¸âƒ£ Session initialization
    # -----------------------------
    if "chat_sessions" not in st.session_state:
        st.session_state.chat_sessions = {}

    if "current_chat" not in st.session_state:
        chat_id = str(datetime.datetime.now())
        st.session_state.current_chat = chat_id
        st.session_state.chat_sessions[chat_id] = {
            "title": "New Chat",
            "messages": []
        }

    # -----------------------------
    # 2ï¸âƒ£ Start new chat
    # -----------------------------
    if st.sidebar.button("âž• Start New Chat"):
        chat_id = str(datetime.datetime.now())
        st.session_state.current_chat = chat_id
        st.session_state.chat_sessions[chat_id] = {
            "title": "New Chat",
            "messages": []
        }

    # -----------------------------
    # 3ï¸âƒ£ Chat selector
    # -----------------------------
    chat_ids = list(st.session_state.chat_sessions.keys())

    selected_chat = st.sidebar.radio(
        "Chat Sessions",
        chat_ids,
        format_func=lambda k: st.session_state.chat_sessions[k]["title"],
        index=chat_ids.index(st.session_state.current_chat)
        if st.session_state.current_chat in chat_ids else 0
    )

    st.session_state.current_chat = selected_chat
    chat_data = st.session_state.chat_sessions[selected_chat]

    # -----------------------------
    # 4ï¸âƒ£ Display chat history
    # -----------------------------
    for message in chat_data["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"], unsafe_allow_html=True)

    # -----------------------------
    # 5ï¸âƒ£ User input
    # -----------------------------
    user_input = st.chat_input("Type your message...")

    if user_input:
        timestamp = datetime.datetime.now().isoformat()

        # Rename chat on first message
        if chat_data["title"] == "New Chat":
            chat_data["title"] = user_input[:30] + "..."

        # Save user message
        chat_data["messages"].append({
            "role": "user",
            "content": user_input,
            "timestamp": timestamp
        })

        with st.chat_message("user"):
            st.markdown(user_input)

        # Generate assistant response
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = rag_chatbot(user_input)
                answer = response["answer"]
                st.markdown(answer, unsafe_allow_html=True)

        # Save assistant message
        chat_data["messages"].append({
            "role": "assistant",
            "content": answer,
            "timestamp": timestamp
        })










def get_chat_history(max_turns=6):
    messages = st.session_state.chat_sessions[
        st.session_state.current_chat
    ]["messages"]

    history = []
    for msg in messages[-max_turns:]:
        role = msg["role"].capitalize()
        history.append(f"{role}: {msg['content']}")

    return history

from openai import AzureOpenAI
import httpx


def generate_response(query, docs):
    context = build_context(docs)
    chat_history = get_chat_history()

    history_text = "\n".join(chat_history)

    prompt = f"""
Vous Ãªtes un assistant RAG spÃ©cialisÃ© dans les systÃ¨mes bancaires,
lâ€™architecture de donnÃ©es et les produits financiers.

[Historique]
{history_text}

[Contexte]
{context}

[Question]
{query}

INSTRUCTIONS:
- Utilisez exclusivement le contexte
- Citez les sources: [Source: fichier.pdf, Page X]
- Si l'information n'existe pas, dites-le clairement
"""

    client = AzureOpenAI(
        api_key=AZURE_OPENAI_API_KEY,
        api_version=AZURE_OPENAI_API_VERSION,
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        http_client=httpx.Client(verify=False)
    )

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "Assistant RAG bancaire"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3
    )

    return response.choices[0].message.content


def rag_chatbot(query):
    chat_history = get_chat_history()

    retriever = CustomRetrieverWithHistory(
        collection=collection,
        chat_history=chat_history,
        k=2500,
        rerank_k=50
    )

    docs = retriever.get_relevant_documents(query)
    answer = generate_response(query, docs)

    return {
        "answer": answer,
        "documents": docs
    }














def generate_response(query, context):
    # Collect sources
    sources = list(set([chunk["metadata"]["source"] for chunk in context]))

    # Build prompt
    context_text = build_context(context)
    prompt = f"""
Vous Ãªtes un assistant RAG spÃ©cialisÃ© dans les systÃ¨mes bancaires,
l'architecture de donnÃ©es, et les produits financiers. Votre rÃ´le est de rÃ©pondre et d'expliquer clairement les
informations issues des documents, en les rendant comprÃ©hensibles pour tout type d'utilisateur, du plus gÃ©nÃ©ral au plus technique.

[Contexte]
{context_text}

[RequÃªte]
{query}

[INSTRUCTIONS GENERALES]
1. Utilisez exclusivement les informations prÃ©sentes dans le contexte ci-dessus.
2. Fournissez une rÃ©ponse claire, structurÃ©e et dÃ©taillÃ©e, adaptÃ©e Ã  la comprÃ©hension de tout utilisateur.
3. Citez immÃ©diatement la source sous le format [Source: <nom du fichier.pdf, Page <numÃ©ro>].
4. Si l'information n'existe pas dans les documents, indiquez-le clairement.
5. Structure attendue:
   a. RÃ©ponse dÃ©taillÃ©e
   b. SynthÃ¨se (si utile)
   c. Sources
6. Objectif: rendre la rÃ©ponse informative, explicative et vÃ©rifiable.
"""

    # Initialize Azure/OpenAI client
    # Make sure the API key, endpoint, and version are correct
    import httpx
    from openai import OpenAI  # or your wrapper

    client = OpenAI(
        api_key="YOUR_API_KEY",
        api_base="YOUR_AZURE_ENDPOINT",
        api_type="azure",
        api_version="2023-03-15-preview"  # replace with your version
    )

    # Call the model
    completion = client.chat.completions.create(
        model="gpt4o",
        messages=[
            {"role": "system", "content": "You are an assistant based on RAG for banking system architecture."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.5  # 0.5 is reasonable; 8.5 is invalid
    )

    # Handle return type (Azure wrapper may return string or dict)
    if isinstance(completion, str):
        response = completion
    elif hasattr(completion, "choices"):
        response = completion.choices[0].message["content"]
    else:
        raise ValueError(f"Unexpected completion type: {type(completion)}")

    # Clean up whitespace for readability
    lines = [line.strip() for line in response.split("\n") if line.strip()]
    readable_response = "\n".join(lines)

    return readable_response










Hereâ€™s a polished French version of your email:


---

Objet : Mise Ã  jour de la documentation Â« Usine de Fiable Â»

Bonjour [Nom du collÃ¨gue],

Veuillez trouver ci-joint la version mise Ã  jour de la documentation Â« Usine de Fiable Â», Ã  laquelle jâ€™ai ajoutÃ© le point [ou â€œcette sectionâ€ selon ce que vous avez ajoutÃ©].

Merci de bien vouloir confirmer la rÃ©ception et nâ€™hÃ©sitez pas Ã  mâ€™ajouter tout Ã©lÃ©ment que vous souhaitez discuter ou revoir.

Cordialement,
[Votre nom]


---

If you want, I can also make a slightly more friendly and collaborative version that encourages discussion without sounding too formal. Do you want me to do that?






appliquÃ©es aprÃ¨s la production de ce mois.


Ajout d'une nouvelle donnÃ©e / nouveau contrÃ´le Â« DCC Â» :
Ainsi, si vous avez reÃ§u un nouveau contrÃ´le de donnÃ©e, effectuÃ© une modification sur un contrÃ´le existant, ou rÃ©activÃ© un ancien contrÃ´le de donnÃ©e dans le pÃ©rimÃ¨tre, ou effectuÃ© toute autre forme de modification, merci de ne pas appliquer ces modifications aprÃ¨s le 20 de chaque mois.




# Initialize retriever
retriever = CustomRetrieverWithHistory(
    collection=collection,      # your Chroma collection
    query_model=query_model,    # BGE-M3 with instruction
    doc_model=doc_model,        # BGE-M3 without instruction
    chat_history=[],            # optional
    k=100,                      # fetch top 100 dense docs
    rerank_k=50                 # rerank top 50 with BM25
)

query = "Which table stores AML risk classification for retail clients?"

docs = retriever.get_relevant_documents(query)

# Output results
for d in docs:
    print(d["metadata"], "\n", d["text"])




import string
import re
import numpy as np
from rank_bm25 import BM25Okapi

# ---------- BM25 Reranker ----------
class BM25Reranker:
    def __init__(self, k=50):
        self.k = k
        self.bm25 = None
        self.document_map = {}
        self.tokenizer = str.maketrans("", "", string.punctuation)

    def _tokenize(self, text):
        if not isinstance(text, str):
            return []
        cleaned = text.translate(self.tokenizer).lower()
        return [t for t in cleaned.split() if t.strip()]

    def fit(self, documents):
        if not documents:
            raise ValueError("Empty document list given to BM25Reranker")
        tokenized_corpus = []
        valid_docs = []
        for i, doc in enumerate(documents):
            text = doc.get("text") or doc.get("page content") or doc.get("content")
            tokens = self._tokenize(text)
            if tokens:
                tokenized_corpus.append(tokens)
                valid_docs.append((i, doc))
        if not tokenized_corpus:
            raise ValueError("No valid documents to fit BM25 model")
        self.document_map = {i: doc for i, doc in valid_docs}
        self.bm25 = BM25Okapi(tokenized_corpus)

    def rerank(self, query, documents):
        if not documents:
            return []
        self.fit(documents)
        tokens = self._tokenize(query)
        if not tokens:
            return documents[:self.k]
        scores = self.bm25.get_scores(tokens)
        ranked = sorted(
            zip(scores, self.document_map.values()),
            key=lambda x: x[0],
            reverse=True
        )
        return [doc for _, doc in ranked[:self.k]]



# ---------- Custom Retriever with Query & Document Embeddings ----------
class CustomRetrieverWithHistory:
    def __init__(self, collection, query_model, doc_model, chat_history=None, k=50, rerank_k=50):
        """
        collection: Chroma collection
        query_model: BGE-M3 model with query_instruction
        doc_model: BGE-M3 model without instruction
        chat_history: optional chat context
        k: number of dense results to fetch
        rerank_k: number of results for BM25 reranking
        """
        self.collection = collection
        self.chat_history = chat_history or []
        self.k = k
        self.rerank_k = rerank_k
        self.query_model = query_model
        self.doc_model = doc_model
        self.reranker = BM25Reranker(k=rerank_k)

    def get_relevant_documents(self, query):
        # --- combine chat history if exists ---
        if self.chat_history:
            full_query = " ".join(self.chat_history) + " " + query
        else:
            full_query = query

        # --- encode the query using the correct query embedding model ---
        query_embedding = self.query_model.encode([full_query])[0].tolist()

        # --- dense retrieval using query embedding ---
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=self.k,
            include=["documents", "metadatas"]
        )

        # --- collect dense docs ---
        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(results["documents"][0], results["metadatas"][0])
        ]

        # --- BM25 rerank over dense docs as hybrid step ---
        reranked_docs = self.reranker.rerank(full_query, dense_docs)

        return reranked_docs


query = "Which table stores AML risk classification for retail clients?"

query_embedding = query_model.encode([query])[0].tolist()

results = collection.query(
    query_embeddings=[query_embedding],
    n_results=10,
    include=["documents", "metadatas"]
)




MODEL_PATH = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"

# ---- Document embedding model (NO instruction) ----
doc_model = FlagModel(
    MODEL_PATH,
    use_fp16=True
)

# ---- Query embedding model (WITH instruction) ----
query_model = FlagModel(
    MODEL_PATH,
    query_instruction_for_retrieval=(
        "Generate representation for this sentence to retrieve relevant articles:"
    ),
    use_fp16=True
)












class BGE_M3_EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        self.dimension = 1024  # BGE-M3 dimension

    def __call__(self, inputs: List[str]) -> List[List[float]]:
        if isinstance(inputs, str):
            inputs = [inputs]

        embeddings = self.model.encode(inputs)

        if len(embeddings[0]) != self.dimension:
            raise ValueError(
                f"Embedding dimension mismatch: expected {self.dimension}, "
                f"got {len(embeddings[0])}"
            )

        return embeddings.tolist()collection = chroma_client.get_or_create_collection(
    name="my_documents",
    embedding_function=BGE_M3_EmbeddingFunction(doc_model),
    metadata={
        "hnsw:space": "cosine",
        "hnsw:construction_ef": 200,
        "hnsw:M": 16
    }
)




query = "CLIENT_ACCOUNT risk_level AML"

dense = chroma_collection.query(
    query_texts=[query],
    n_results=1000,   # very large on purpose
    include=["documents"]
)

dense_docs = dense["documents"][0]

found = any(
    any(chunk_id in d for chunk_id in ["CLIENT_ACCOUNT", "risk_level"])
    for d in dense_docs
)

print("Found in dense@1000:", found)




class BM25Reranker:
    def __init__(self, documents, k=50):
        """
        documents: list of {"text": ..., "metadata": ...}
        """
        self.k = k
        self.documents = documents
        self.tokenized_corpus = [self._tokenize(d["text"]) for d in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)

    def _tokenize(self, text: str):
        # preserves table names, columns, acronyms
        return re.findall(r"[A-Za-z0-9_\.]+", text.lower())

    def retrieve(self, query: str):
        tokens = self._tokenize(query)
        scores = self.bm25.get_scores(tokens)

        ranked = sorted(
            zip(scores, self.documents),
            key=lambda x: x[0],
            reverse=True
        )

        return [doc for _, doc in ranked[:self.k]]


class HybridRetriever:
    def __init__(
        self,
        chroma_collection,
        bm25_reranker,
        k_dense=200,
        k_final=20
    ):
        self.collection = chroma_collection
        self.bm25 = bm25_reranker
        self.k_dense = k_dense
        self.k_final = k_final

    def retrieve(self, query: str):
        # ---- 1. Query expansion (light, safe) ----
        expanded_query = (
            query
            + " table database schema column risk AML architecture"
        )

        # ---- 2. Dense retrieval (high recall) ----
        dense = self.collection.query(
            query_texts=[expanded_query],
            n_results=self.k_dense,
            include=["documents", "metadatas"]
        )

        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(dense["documents"][0], dense["metadatas"][0])
        ]

        # ---- 3. Sparse retrieval (BM25 rescue) ----
        sparse_docs = self.bm25.retrieve(query)

        # ---- 4. Union + dedup ----
        merged = {}
        for d in dense_docs + sparse_docs:
            merged[hash(d["text"])] = d

        merged_docs = list(merged.values())

        return merged_docs[:self.k_final]

def output_matching_chunks(collection, keyword: str):
    data = collection.get(include=["documents", "metadatas"])

    documents = data["documents"]
    metadatas = data["metadatas"]

    # handle list-of-lists
    if documents and isinstance(documents[0], list):
        documents = documents[0]
        metadatas = metadatas[0]

    hits = 0
    for i, (doc, meta) in enumerate(zip(documents, metadatas)):
        if keyword.lower() in doc.lower():
            print(f"\n--- MATCHING CHUNK {i} ---")
            print("Metadata:", meta)
            print(doc)
            hits += 1

    print("\nTotal matching chunks:", hits)

def build_context(docs):
    blocks = []
    for i, d in enumerate(docs):
        meta = d.get("metadata", {})
        header = f"""
[Chunk {i+1}]
Database: {meta.get('db', 'N/A')}
Schema: {meta.get('schema', 'N/A')}
Table: {meta.get('table', 'N/A')}
"""
        blocks.append(header + d["text"])
    return "\n\n".join(blocks)

# documents = same docs used for Chroma ingestion
bm25 = BM25Reranker(documents, k=100)

retriever = HybridRetriever(
    chroma_collection=chroma_collection,
    bm25_reranker=bm25,
    k_dense=250,
    k_final=20
)

query = "Which table stores AML risk classification for retail clients?"

docs = retriever.retrieve(query)

for d in docs:
    print(d["metadata"])query = "Which table stores AML risk classification for retail clients?"

docs = retriever.retrieve(query)

for d in docs:
    print(d["metadata"])

context = build_context(docs)

prompt = f"""
You are a banking information system architect.

Answer ONLY using the context.
If not found, say "Information not found".

Context:
{context}

Question:
{query}
"""








keyword = "CLIENT_ACCOUNT"

data = chroma_collection.get(
    include=["documents", "metadatas"]
)

hits = 0

documents = data["documents"]
metadatas = data["metadatas"]

# Handle both flat list and list-of-lists
if documents and isinstance(documents[0], list):
    documents = documents[0]
    metadatas = metadatas[0]

for i, (doc, meta) in enumerate(zip(documents, metadatas)):
    if keyword.lower() in doc.lower():
        print(f"\n--- MATCHING CHUNK {i} ---")
        print("Metadata:", meta)
        print(doc)
        hits += 1

print("\nTotal matching chunks:", hits)











data = chroma_collection.get(
    where={"source": "core_banking_architecture.pdf"},
    include=["documents", "metadatas"]
)

for i, doc in enumerate(data["documents"]):
    print(f"\n--- CHUNK {i} ---\n")
    print(doc)





bm25_reranker = BM25Reranker(
    documents=documents,
    k=50   # how many BM25 docs you want
)


hybrid_retriever = HybridRetriever(
    collection=chroma_collection,
    bm25_reranker=bm25_reranker,
    k_dense=120,   # large recall
    k_final=20     # final docs sent to LLM
)

query = "Which database stores AML risk classification for retail clients?"

retrieved_docs = hybrid_retriever.retrieve(query)



from rank_bm25 import BM25Okapi
import re

class BM25Reranker:
    def __init__(self, documents, k=20):
        self.k = k
        self.documents = documents
        self.tokenized_corpus = [self._tokenize(d["text"]) for d in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)

    def _tokenize(self, text):
        return re.findall(r"[A-Za-z0-9_\.]+", text.lower())

    def rerank(self, query):
        query_tokens = self._tokenize(query)
        scores = self.bm25.get_scores(query_tokens)

        ranked = sorted(
            zip(scores, self.documents),
            key=lambda x: x[0],
            reverse=True
        )
        return [doc for _, doc in ranked[:self.k]]class HybridRetriever:
    def __init__(self, collection, bm25_reranker, k_dense=100, k_final=20):
        self.collection = collection
        self.bm25 = bm25_reranker
        self.k_dense = k_dense
        self.k_final = k_final

    def retrieve(self, query):
        dense = self.collection.query(
            query_texts=[query],
            n_results=self.k_dense,
            include=["documents", "metadatas"]
        )

        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(dense["documents"][0], dense["metadatas"][0])
        ]

        # BM25 rescue
        sparse_docs = self.bm25.rerank(query)

        # Union + dedup
        seen = set()
        merged = []
        for d in dense_docs + sparse_docs:
            h = hash(d["text"])
            if h not in seen:
                seen.add(h)
                merged.append(d)

        return merged[:self.k_final]from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_community.document_compressors.rankllm_rerank import RankLLMRerank
from typing import List

def hybrid_retrieve_langchain(
    collection,           # Your vector DB collection (Chroma/FAISS)
    llm_client,           # Azure/OpenAI client (LLM)
    query: str,
    k_dense: int = 50,    # Number of top docs from embeddings
    k_final: int = 20     # Number of final docs after reranking
) -> List[dict]:
    """
    Hybrid Retrieval using:
    Step 1: Dense retrieval from vector DB
    Step 2: RankLLM reranking (via LangChain)
    Returns top documents (text + metadata)
    """

    # --------------------------
    # Step 1: Dense retrieval
    # --------------------------
    dense_results = collection.query(
        query_texts=[query],
        n_results=k_dense,
        include=["documents", "metadatas"]
    )

    candidates = []
    for text, meta in zip(dense_results["documents"][0], dense_results["metadatas"][0]):
        candidates.append({"text": text, "metadata": meta})

    if not candidates:
        return []

    # --------------------------
    # Step 2: RankLLM reranker
    # --------------------------
    reranker = RankLLMRerank.from_llm(
        llm=llm_client,  # your Azure/OpenAI client
        top_n=k_final,
        verbose=True
    )

    # Optional: wrap with ContextualCompressionRetriever if you want
    # to compress context before passing to reranker (useful for long chunks)
    retriever = ContextualCompressionRetriever(
        base_retriever=candidates,  # here just the list of candidates
        compressor=reranker
    )

    # Run reranking
    reranked_docs = reranker.rerank(query=query, documents=candidates)

    return reranked_docs[:k_final]






query = "How to configure user permissions in the system"

top_docs = hybrid_retrieve_langchain(
    collection=chroma_collection,
    llm_client=azure_openai_client,
    query=query,
    k_dense=80,
    k_final=20
)

for doc in top_docs:
    print(doc["text"][:300], "...\n")







import streamlit as st
import chromadb
from chromadb.utils import embedding_functions
import fitz  # PyMuPDF
from docx import Document
import pandas as pd
from datetime import datetime
import json
import os
from typing import List, Dict, Tuple, Optional
import re
from collections import defaultdict
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
import hashlib

# Azure OpenAI imports
from openai import AzureOpenAI
import httpx

# ==============================================================================
# CONFIGURATION
# ==============================================================================

class Config:
    """Centralized configuration management"""
    
    # Azure OpenAI Configuration
    OIDC_CLIENT_ID = os.getenv("OIDC_CLIENT_ID", "your_client_id")
    OIDC_CLIENT_SECRET = os.getenv("OIDC_CLIENT_SECRET", "your_client_secret")
    OIDC_ENDPOINT = os.getenv("OIDC_ENDPOINT", "https://alfactory.api.staging.schonet/auth/oauth2/v2/token")
    OIDC_SCOPE = os.getenv("OIDC_SCOPE", "genai-model")
    
    AZURE_ENDPOINT = os.getenv("AZURE_ENDPOINT", "https://alfactory.api.staging.schonet/genai-model/v1")
    AZURE_API_VERSION = os.getenv("AZURE_API_VERSION", "2024-05-01-preview")
    AZURE_API_KEY = os.getenv("AZURE_API_KEY", "FAKE_KEY")
    AZURE_MODEL_DEPLOYMENT = os.getenv("AZURE_MODEL_DEPLOYMENT", "gpt-4o")
    
    # Embedding Configuration
    EMBEDDING_MODEL_PATH = "/domino/datasets/local/test-prd-base"
    EMBEDDING_DIMENSION = 1024
    
    # ChromaDB Configuration
    CHROMA_PERSIST_PATH = "/domino/datasets/local/chroma_persistent_db"
    COLLECTION_NAME = "banking_documents"
    
    # Chunking Configuration for Banking Documents
    CHUNK_SIZES = {
        'header': 300,      # For sections with headers
        'table': 500,       # For tables and structured data
        'paragraph': 400,   # For regular paragraphs
        'list': 350,        # For lists and enumerations
    }
    CHUNK_OVERLAP = 100
    
    # BM25 Configuration
    BM25_INDEX_PATH = "/domino/datasets/local/bm25_index.pkl"
    BM25_K1 = 1.5
    BM25_B = 0.75
    
    # Retrieval Configuration
    INITIAL_RETRIEVAL_K = 20  # Retrieve more for reranking
    FINAL_RESULTS_K = 5       # Final results after reranking
    HYBRID_ALPHA = 0.5        # Balance between semantic (0) and keyword (1)
    
    # Batch Processing Configuration
    BATCH_SIZE = 100
    MAX_WORKERS = 4

# ==============================================================================
# DOCUMENT PROCESSING & CHUNKING
# ==============================================================================

class BankingDocumentProcessor:
    """Advanced document processor optimized for banking/technical documents"""
    
    def __init__(self):
        self.chunk_patterns = {
            'section_header': re.compile(r'^(Sect\.|Section|Chapitre|Article|Â§)\s*[\dIVX]+[\.\s]', re.IGNORECASE),
            'subsection': re.compile(r'^[A-Z]{1,3}\.\d+(\.\d+)*\s+', re.MULTILINE),
            'table_marker': re.compile(r'(Description Rubrique|Num\s+donn\.|Val\s+init)', re.IGNORECASE),
            'code_block': re.compile(r'(Code|RÃ©fÃ©rence|Type de contrat):\s*\d+', re.IGNORECASE),
            'list_item': re.compile(r'^\s*[-â€¢*]\s+|\d+\.\s+', re.MULTILINE),
        }
    
    def extract_text_from_file(self, uploaded_file) -> Optional[Dict]:
        """Extract text with metadata from uploaded files"""
        filename = uploaded_file.name.lower()
        
        try:
            if filename.endswith('.pdf'):
                return self._extract_from_pdf(uploaded_file)
            elif filename.endswith('.txt'):
                text = uploaded_file.read().decode('utf-8')
                return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
            elif filename.endswith('.docx'):
                return self._extract_from_docx(uploaded_file)
            elif filename.endswith(('.csv', '.xlsx')):
                return self._extract_from_spreadsheet(uploaded_file, filename)
            else:
                return None
        except Exception as e:
            st.error(f"Error extracting from {uploaded_file.name}: {str(e)}")
            return None
    
    def _extract_from_pdf(self, uploaded_file) -> Dict:
        """Extract text from PDF with page-level granularity"""
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        pages = []
        
        for page_num, page in enumerate(doc, start=1):
            text = page.get_text()
            pages.append({
                'page_num': page_num,
                'text': text
            })
        
        full_text = "\n".join([p['text'] for p in pages])
        return {'text': full_text, 'pages': pages}
    
    def _extract_from_docx(self, uploaded_file) -> Dict:
        """Extract text from DOCX"""
        doc = Document(uploaded_file)
        text = "\n".join([p.text for p in doc.paragraphs])
        return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
    
    def _extract_from_spreadsheet(self, uploaded_file, filename: str) -> Dict:
        """Extract text from CSV/Excel"""
        if filename.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)
        
        text = df.to_string(index=False)
        return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
    
    def clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove excessive whitespace
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        
        # Normalize line breaks
        text = text.replace('\r\n', '\n')
        
        # Remove page headers/footers (common patterns)
        text = re.sub(r'Page \d+( of \d+)?', '', text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def smart_chunk_document(self, doc_data: Dict, source_file: str) -> List[Dict]:
        """
        Intelligent chunking that preserves document structure
        Optimized for banking/technical documents
        """
        chunks = []
        pages = doc_data.get('pages', [])
        
        for page_info in pages:
            page_num = page_info['page_num']
            text = self.clean_text(page_info['text'])
            
            # Detect document structure
            if self._is_table_content(text):
                page_chunks = self._chunk_table_content(text, source_file, page_num)
            elif self._has_clear_sections(text):
                page_chunks = self._chunk_by_sections(text, source_file, page_num)
            else:
                page_chunks = self._chunk_semantic(text, source_file, page_num)
            
            chunks.extend(page_chunks)
        
        return chunks
    
    def _is_table_content(self, text: str) -> bool:
        """Detect if content is primarily tabular"""
        return bool(self.chunk_patterns['table_marker'].search(text))
    
    def _has_clear_sections(self, text: str) -> bool:
        """Detect if content has clear section structure"""
        headers = self.chunk_patterns['section_header'].findall(text)
        return len(headers) >= 2
    
    def _chunk_table_content(self, text: str, source: str, page: int) -> List[Dict]:
        """Chunk table content preserving structure"""
        chunks = []
        lines = text.split('\n')
        current_chunk = []
        current_size = 0
        
        for line in lines:
            line_size = len(line)
            
            if current_size + line_size > Config.CHUNK_SIZES['table']:
                if current_chunk:
                    chunks.append(self._create_chunk(
                        '\n'.join(current_chunk), source, page, 'table'
                    ))
                    # Overlap: keep last few lines
                    overlap_lines = current_chunk[-3:] if len(current_chunk) > 3 else current_chunk
                    current_chunk = overlap_lines
                    current_size = sum(len(l) for l in current_chunk)
            
            current_chunk.append(line)
            current_size += line_size
        
        if current_chunk:
            chunks.append(self._create_chunk(
                '\n'.join(current_chunk), source, page, 'table'
            ))
        
        return chunks
    
    def _chunk_by_sections(self, text: str, source: str, page: int) -> List[Dict]:
        """Chunk by document sections"""
        chunks = []
        sections = re.split(r'(\n(?:Sect\.|Section|Chapitre|Article)\s+[\dIVX]+)', text, flags=re.IGNORECASE)
        
        current_section = ""
        
        for i, section in enumerate(sections):
            if i % 2 == 0:  # Content
                current_section += section
            else:  # Header
                if current_section.strip():
                    chunks.append(self._create_chunk(
                        current_section.strip(), source, page, 'section'
                    ))
                current_section = section
        
        if current_section.strip():
            chunks.append(self._create_chunk(
                current_section.strip(), source, page, 'section'
            ))
        
        return chunks
    
    def _chunk_semantic(self, text: str, source: str, page: int) -> List[Dict]:
        """Semantic chunking with overlap for regular content"""
        chunks = []
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            if current_size + sentence_size > Config.CHUNK_SIZES['paragraph']:
                if current_chunk:
                    chunks.append(self._create_chunk(
                        ' '.join(current_chunk), source, page, 'paragraph'
                    ))
                    # Overlap: keep last 2 sentences
                    overlap = current_chunk[-2:] if len(current_chunk) > 2 else current_chunk
                    current_chunk = overlap
                    current_size = sum(len(s) for s in current_chunk)
            
            current_chunk.append(sentence)
            current_size += sentence_size
        
        if current_chunk:
            chunks.append(self._create_chunk(
                ' '.join(current_chunk), source, page, 'paragraph'
            ))
        
        return chunks
    
    def _create_chunk(self, text: str, source: str, page: int, chunk_type: str) -> Dict:
        """Create a standardized chunk object"""
        chunk_id = hashlib.md5(f"{source}_{page}_{text[:50]}".encode()).hexdigest()
        
        return {
            'id': chunk_id,
            'text': text,
            'metadata': {
                'source_file': source,
                'page': page,
                'chunk_type': chunk_type,
                'char_count': len(text),
                'timestamp': str(datetime.now())
            }
        }

# ==============================================================================
# BM25 IMPLEMENTATION WITH PERSISTENT INDEX
# ==============================================================================

class PersistentBM25:
    """BM25 implementation with disk persistence for large-scale retrieval"""
    
    def __init__(self, k1: float = Config.BM25_K1, b: float = Config.BM25_B):
        self.k1 = k1
        self.b = b
        self.corpus_size = 0
        self.avgdl = 0
        self.doc_freqs = defaultdict(int)
        self.idf = {}
        self.doc_len = []
        self.doc_ids = []
        self.tokenized_corpus = []
        
    def tokenize(self, text: str) -> List[str]:
        """Tokenize with banking-specific preservation"""
        # Preserve codes and references
        text = re.sub(r'([A-Z]{2,}\d+)', r' \1 ', text)
        
        # Remove punctuation but keep hyphens in codes
        text = re.sub(r'[^\w\s-]', ' ', text.lower())
        
        # Tokenize
        tokens = text.split()
        
        # Filter stopwords (French banking context)
        stopwords = {'le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'ou', 'dans', 'pour', 'sur', 'avec'}
        tokens = [t for t in tokens if t and t not in stopwords]
        
        return tokens
    
    def fit(self, documents: List[Dict]):
        """Build BM25 index from documents"""
        self.corpus_size = len(documents)
        self.doc_ids = [doc['id'] for doc in documents]
        self.tokenized_corpus = []
        
        # Tokenize all documents
        for doc in documents:
            tokens = self.tokenize(doc['text'])
            self.tokenized_corpus.append(tokens)
            self.doc_len.append(len(tokens))
            
            # Count document frequencies
            unique_tokens = set(tokens)
            for token in unique_tokens:
                self.doc_freqs[token] += 1
        
        # Calculate average document length
        self.avgdl = sum(self.doc_len) / self.corpus_size if self.corpus_size > 0 else 0
        
        # Calculate IDF values
        for token, freq in self.doc_freqs.items():
            self.idf[token] = np.log((self.corpus_size - freq + 0.5) / (freq + 0.5) + 1)
    
    def get_scores(self, query: str) -> Dict[str, float]:
        """Calculate BM25 scores for a query"""
        query_tokens = self.tokenize(query)
        scores = {}
        
        for idx, (doc_id, doc_tokens, doc_length) in enumerate(
            zip(self.doc_ids, self.tokenized_corpus, self.doc_len)
        ):
            score = 0
            for token in query_tokens:
                if token not in self.idf:
                    continue
                
                # Calculate term frequency in document
                tf = doc_tokens.count(token)
                
                # BM25 formula
                numerator = tf * (self.k1 + 1)
                denominator = tf + self.k1 * (1 - self.b + self.b * (doc_length / self.avgdl))
                score += self.idf[token] * (numerator / denominator)
            
            scores[doc_id] = score
        
        return scores
    
    def save(self, filepath: str):
        """Save BM25 index to disk"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'k1': self.k1,
                'b': self.b,
                'corpus_size': self.corpus_size,
                'avgdl': self.avgdl,
                'doc_freqs': dict(self.doc_freqs),
                'idf': self.idf,
                'doc_len': self.doc_len,
                'doc_ids': self.doc_ids,
                'tokenized_corpus': self.tokenized_corpus
            }, f)
    
    @classmethod
    def load(cls, filepath: str) -> 'PersistentBM25':
        """Load BM25 index from disk"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        
        bm25 = cls(k1=data['k1'], b=data['b'])
        bm25.corpus_size = data['corpus_size']
        bm25.avgdl = data['avgdl']
        bm25.doc_freqs = defaultdict(int, data['doc_freqs'])
        bm25.idf = data['idf']
        bm25.doc_len = data['doc_len']
        bm25.doc_ids = data['doc_ids']
        bm25.tokenized_corpus = data['tokenized_corpus']
        
        return bm25

# ==============================================================================
# QUERY EXPANSION FOR BANKING TERMINOLOGY
# ==============================================================================

class BankingQueryExpander:
    """Query expansion specialized for banking/financial terminology"""
    
    def __init__(self):
        # Banking-specific synonym mappings
        self.synonyms = {
            'compte': ['account', 'compte support', 'compte client'],
            'contrat': ['contract', 'agreement', 'convention'],
            'dÃ©pÃ´t': ['deposit', 'versement', 'apport'],
            'garantie': ['guarantee', 'caution', 'warranty'],
            'Ã©chÃ©ance': ['maturity', 'deadline', 'due date', 'expiration'],
            'nantissement': ['pledge', 'collateral', 'guarantee'],
            'saisie': ['entry', 'input', 'capture'],
            'grille': ['grid', 'screen', 'form'],
            'transaction': ['operation', 'mouvement', 'transfer'],
            'client': ['customer', 'account holder', 'beneficiary'],
            'bÃ©nÃ©ficiaire': ['beneficiary', 'recipient', 'payee'],
            'taux': ['rate', 'interest rate', 'percentage'],
            'plafond': ['ceiling', 'limit', 'cap', 'maximum'],
            'retrait': ['withdrawal', 'extraction'],
            'virement': ['transfer', 'wire', 'payment'],
        }
        
        # Common banking abbreviations
        self.abbreviations = {
            'DAT': 'DÃ©pÃ´t Ã  Terme',
            'CNT': 'Contrat',
            'GDI': 'Gestion De Interface',
            'BDC': 'Bon De Caisse',
            'SGE': 'Code siÃ¨ge',
            'DEV': 'Code devise',
        }
    
    def expand_query(self, query: str) -> List[str]:
        """Expand query with synonyms and banking terms"""
        expanded_queries = [query]
        
        query_lower = query.lower()
        
        # Add synonym expansions
        for term, synonyms in self.synonyms.items():
            if term in query_lower:
                for synonym in synonyms:
                    expanded = query_lower.replace(term, synonym)
                    if expanded != query_lower:
                        expanded_queries.append(expanded)
        
        # Expand abbreviations
        for abbr, full_form in self.abbreviations.items():
            if abbr in query:
                expanded = query.replace(abbr, full_form)
                expanded_queries.append(expanded)
            elif full_form.lower() in query_lower:
                expanded = query_lower.replace(full_form.lower(), abbr)
                expanded_queries.append(expanded)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_queries = []
        for q in expanded_queries:
            if q not in seen:
                seen.add(q)
                unique_queries.append(q)
        
        return unique_queries[:5]  # Limit to top 5 expansions

# ==============================================================================
# HYBRID RETRIEVAL SYSTEM
# ==============================================================================

class HybridRetriever:
    """Hybrid retrieval combining semantic search (ChromaDB) and keyword search (BM25)"""
    
    def __init__(self, chroma_collection, bm25_index: PersistentBM25, alpha: float = Config.HYBRID_ALPHA):
        self.chroma_collection = chroma_collection
        self.bm25_index = bm25_index
        self.alpha = alpha  # Weight between semantic (0) and keyword (1)
        self.query_expander = BankingQueryExpander()
    
    def retrieve(self, query: str, k: int = Config.INITIAL_RETRIEVAL_K) -> List[Dict]:
        """
        Hybrid retrieval with query expansion
        Returns top k documents based on weighted combination of semantic + keyword scores
        """
        # Expand query for better recall
        expanded_queries = self.query_expander.expand_query(query)
        
        all_results = {}
        
        for exp_query in expanded_queries:
            # 1. Semantic search via ChromaDB
            semantic_results = self.chroma_collection.query(
                query_texts=[exp_query],
                n_results=k,
                include=["documents", "metadatas", "distances"]
            )
            
            # Normalize semantic scores (distances -> similarities)
            semantic_scores = {}
            if semantic_results['ids'] and semantic_results['ids'][0]:
                max_distance = max(semantic_results['distances'][0]) if semantic_results['distances'][0] else 1
                for idx, doc_id in enumerate(semantic_results['ids'][0]):
                    distance = semantic_results['distances'][0][idx]
                    similarity = 1 - (distance / max_distance) if max_distance > 0 else 1
                    semantic_scores[doc_id] = similarity
            
            # 2. Keyword search via BM25
            bm25_scores = self.bm25_index.get_scores(exp_query)
            
            # Normalize BM25 scores
            max_bm25 = max(bm25_scores.values()) if bm25_scores else 1
            normalized_bm25 = {k: v / max_bm25 for k, v in bm25_scores.items()} if max_bm25 > 0 else bm25_scores
            
            # 3. Combine scores with weighted sum
            all_doc_ids = set(semantic_scores.keys()) | set(normalized_bm25.keys())
            
            for doc_id in all_doc_ids:
                sem_score = semantic_scores.get(doc_id, 0)
                bm25_score = normalized_bm25.get(doc_id, 0)
                
                # Hybrid score
                hybrid_score = (1 - self.alpha) * sem_score + self.alpha * bm25_score
                
                # Accumulate scores across expanded queries
                if doc_id in all_results:
                    all_results[doc_id]['score'] = max(all_results[doc_id]['score'], hybrid_score)
                else:
                    # Get document metadata
                    try:
                        doc_data = self.chroma_collection.get(
                            ids=[doc_id],
                            include=["documents", "metadatas"]
                        )
                        if doc_data['ids']:
                            all_results[doc_id] = {
                                'id': doc_id,
                                'text': doc_data['documents'][0],
                                'metadata': doc_data['metadatas'][0],
                                'score': hybrid_score,
                                'semantic_score': sem_score,
                                'bm25_score': bm25_score
                            }
                    except:
                        pass
        
        # Sort by score and return top k
        ranked_results = sorted(all_results.values(), key=lambda x: x['score'], reverse=True)
        return ranked_results[:k]

# ==============================================================================
# BATCH DOCUMENT INGESTION
# ==============================================================================

class BatchDocumentIngestion:
    """Optimized batch processing for large document collections"""
    
    def __init__(self, chroma_collection, processor: BankingDocumentProcessor):
        self.collection = chroma_collection
        self.processor = processor
        self.bm25_documents = []
    
    def ingest_documents(self, uploaded_files: List, progress_callback=None) -> Dict:
        """
        Batch ingest documents with progress tracking
        Returns statistics about the ingestion process
        """
        stats = {
            'total_files': len(uploaded_files),
            'processed_files': 0,
            'total_chunks': 0,
            'failed_files': [],
            'processing_time': 0
        }
        
        start_time = datetime.now()
        
        # Get existing documents to avoid duplicates
        existing_sources = self._get_existing_sources()
        
        batch_chunks = []
        
        for file_idx, uploaded_file in enumerate(uploaded_files):
            try:
                filename = uploaded_file.name
                
                # Skip if already indexed
                if filename in existing_sources:
                    if progress_callback:
                        progress_callback(file_idx + 1, len(uploaded_files), f"Skipped (already indexed): {filename}")
                    continue
                
                if progress_callback:
                    progress_callback(file_idx + 1, len(uploaded_files), f"Processing: {filename}")
                
                # Extract and chunk document
                doc_data = self.processor.extract_text_from_file(uploaded_file)
                
                if doc_data is None:
                    stats['failed_files'].append(filename)
                    continue
                
                chunks = self.processor.smart_chunk_document(doc_data, filename)
                
                # Add to batch
                batch_chunks.extend(chunks)
                self.bm25_documents.extend(chunks)
                
                # Process batch if it reaches batch size
                if len(batch_chunks) >= Config.BATCH_SIZE:
                    self._process_batch(batch_chunks)
                    stats['total_chunks'] += len(batch_chunks)
                    batch_chunks = []
                
                stats['processed_files'] += 1
                
            except Exception as e:
                stats['failed_files'].append(f"{uploaded_file.name}: {str(e)}")
        
        # Process remaining chunks
        if batch_chunks:
            self._process_batch(batch_chunks)
            stats['total_chunks'] += len(batch_chunks)
        
        stats['processing_time'] = (datetime.now() - start_time).total_seconds()
        
        return stats
    
    def _process_batch(self, chunks: List[Dict]):
        """Process a batch of chunks into ChromaDB"""
        if not chunks:
            return
        
        ids = [chunk['id'] for chunk in chunks]
        documents = [chunk['text'] for chunk in chunks]
        metadatas = [chunk['metadata'] for chunk in chunks]
        
        self.collection.add(
            ids=ids,
            documents=documents,
            metadatas=metadatas
        )
    
    def _get_existing_sources(self) -> set:
        """Get list of already indexed source files"""
        try:
            all_data = self.collection.get(include=["metadatas"])
            if all_data and all_data['metadatas']:
                return set(meta.get('source_file', '') for meta in all_data['metadatas'])
        except:
            pass
        return set()
    
    def build_bm25_index(self) -> PersistentBM25:
        """Build and save BM25 index from ingested documents"""
        bm25 = PersistentBM25()
        bm25.fit(self.bm25_documents)
        bm25.save(Config.BM25_INDEX_PATH)
        return bm25

# ==============================================================================
# RAG GENERATION WITH IMPROVED CONTEXT
# ==============================================================================

class BankingRAGGenerator:
    """Enhanced RAG generation for banking documentation"""
    
    def __init__(self, azure_client):
        self.client = azure_client
    
    def generate_response(self, query: str, context_docs: List[Dict]) -> Dict:
        """Generate response with improved context assembly"""
        
        # Assemble context with source citations
        context_parts = []
        sources = []
        
        for idx, doc in enumerate(context_docs, 1):
            metadata = doc.get('metadata', {})
            source_file = metadata.get('source_file', 'Unknown')
            page = metadata.get('page', 'N/A')
            
            context_parts.append(
                f"[Document {idx} - {source_file}, Page {page}]\n{doc['text']}\n"
            )
            
            if source_file not in sources:
                sources.append(source_file)
        
        context = "\n".join(context_parts)
        
        # Enhanced prompt for banking documentation
        prompt = f"""Vous Ãªtes un assistant RAG spÃ©cialisÃ© dans les systÃ¨mes bancaires, l'architecture de donnÃ©es, et les produits financiers.

Votre rÃ´le est de rÃ©pondre et d'expliquer clairement les informations issues des documents, en les rendant comprÃ©hensibles pour tout type d'utilisateur, du plus gÃ©nÃ©ral au plus technique.

[Contexte]
{context}

[RequÃªte]
{query}

[INSTRUCTIONS GÃ‰NÃ‰RALES]

1. Utilisez exclusivement les informations prÃ©sentes dans le contexte ci-dessus.

2. Fournissez une rÃ©ponse claire, structurÃ©e et explicite, adaptÃ©e Ã  la comprÃ©hension de tout utilisateur.
   - Si le sujet est technique, expliquez les notions en termes simples.
   - Si la question est gÃ©nÃ©rale, donnez une rÃ©ponse complÃ¨te mais concise.
   - Si la question est complexe, dÃ©taillez le raisonnement et le fonctionnement.

3. Chaque fois que vous mentionnez une donnÃ©e, un mÃ©canisme ou un fait:
   - Citez immÃ©diatement la source sous le format: [Source: nom_du_fichier.pdf, Page <numÃ©ro>]

4. Si l'information n'existe pas dans les documents, indiquez-le clairement.

5. Structure attendue:
   a. RÃ©ponse expliquÃ©e: DÃ©taillez le contenu et son interprÃ©tation.
   b. SynthÃ¨se (si utile): RÃ©sumez la logique ou le fonctionnement global.
   c. Sources: Liste complÃ¨te des documents utilisÃ©s.

6. Objectif:
   Rendre la rÃ©ponse Ã  la fois informative, explicative et vÃ©rifiable,
   qu'il s'agisse d'un utilisateur curieux ou d'un expert technique.

[Exemple de format]
Les procÃ©dures de vÃ©rification d'identitÃ©... [Source: procedure_kyc.pdf, Page 12]. 
Selon le chapitre 3... [Source: reglement_financier.pdf, Page 45].

Sources utilisÃ©es:
- procedure_kyc.pdf (Pages 12, 15)
- reglement_financier.pdf (Page 45)
"""
        
        try:
            completion = self.client.chat.completions.create(
                model=Config.AZURE_MODEL_DEPLOYMENT,
                messages=[
                    {"role": "system", "content": "You are an expert assistant for banking system documentation based on RAG (Retrieval-Augmented Generation)."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            answer = completion.choices[0].message.content
            
            return {
                'answer': answer,
                'sources': sources,
                'context_docs': context_docs,
                'num_tokens': completion.usage.total_tokens if hasattr(completion, 'usage') else None
            }
            
        except Exception as e:
            return {
                'answer': f"Error generating response: {str(e)}",
                'sources': [],
                'context_docs': [],
                'num_tokens': None
            }

# ==============================================================================
# STREAMLIT APPLICATION
# ==============================================================================

def initialize_session_state():
    """Initialize Streamlit session state"""
    if 'chroma_client' not in st.session_state:
        st.session_state.chroma_client = chromadb.PersistentClient(path=Config.CHROMA_PERSIST_PATH)
    
    if 'collection' not in st.session_state:
        # Load embedding model
        embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL_PATH)
        
        # Create embedding function
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=Config.EMBEDDING_MODEL_PATH
        )
        
        st.session_state.collection = st.session_state.chroma_client.get_or_create_collection(
            name=Config.COLLECTION_NAME,
            embedding_function=embedding_function,
            metadata={"hnsw:space": "cosine"}
        )
    
    if 'bm25_index' not in st.session_state:
        # Load or create BM25 index
        if os.path.exists(Config.BM25_INDEX_PATH):
            st.session_state.bm25_index = PersistentBM25.load(Config.BM25_INDEX_PATH)
        else:
            st.session_state.bm25_index = PersistentBM25()
    
    if 'chat_sessions' not in st.session_state:
        st.session_state.chat_sessions = {}
    
    if 'current_chat' not in st.session_state:
        st.session_state.current_chat = str(datetime.now())
        st.session_state.chat_sessions[st.session_state.current_chat] = {
            "title": "New Chat",
            "messages": []
        }
    
    if 'processor' not in st.session_state:
        st.session_state.processor = BankingDocumentProcessor()
    
    if 'azure_client' not in st.session_state:
        # Initialize Azure OpenAI client
        http_client = httpx.Client(verify=False)
        st.session_state.azure_client = AzureOpenAI(
            api_version=Config.AZURE_API_VERSION,
            azure_endpoint=Config.AZURE_ENDPOINT,
            api_key=Config.AZURE_API_KEY,
            http_client=http_client
        )
    
    if 'rag_generator' not in st.session_state:
        st.session_state.rag_generator = BankingRAGGenerator(st.session_state.azure_client)

def main():
    st.set_page_config(page_title="BNP Banking RAG Chatbot", layout="wide")
    
    initialize_session_state()
    
    st.title("ðŸ¦ BNP Banking Documentation Chatbot")
    st.caption("Optimized RAG system with hybrid search and batch processing")
    
    # Sidebar for document management
    with st.sidebar:
        st.header("ðŸ“ Document Management")
        
        # Get collection stats
        try:
            collection_count = st.session_state.collection.count()
            st.metric("Indexed Documents", collection_count)
        except:
            collection_count = 0
            st.metric("Indexed Documents", "0")
        
        # Batch upload
        st.subheader("Upload Documents")
        uploaded_files = st.file_uploader(
            "Upload multiple documents",
            type=["pdf", "txt", "docx", "csv", "xlsx"],
            accept_multiple_files=True
        )
        
        if uploaded_files and st.button("ðŸ“¤ Process All Files"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(current, total, message):
                progress = current / total
                progress_bar.progress(progress)
                status_text.text(message)
            
            with st.spinner("Processing documents..."):
                batch_ingestion = BatchDocumentIngestion(
                    st.session_state.collection,
                    st.session_state.processor
                )
                
                stats = batch_ingestion.ingest_documents(
                    uploaded_files,
                    progress_callback=update_progress
                )
                
                # Rebuild BM25 index
                status_text.text("Building BM25 index...")
                st.session_state.bm25_index = batch_ingestion.build_bm25_index()
                
                # Display results
                st.success(f"""
                âœ… Processing complete!
                - Files processed: {stats['processed_files']}/{stats['total_files']}
                - Total chunks: {stats['total_chunks']}
                - Time: {stats['processing_time']:.2f}s
                """)
                
                if stats['failed_files']:
                    st.error(f"Failed files: {', '.join(stats['failed_files'])}")
        
        st.divider()
        
        # Chat management
        st.subheader("ðŸ’¬ Chat Sessions")
        if st.button("âž• New Chat"):
            new_chat_id = str(datetime.now())
            st.session_state.current_chat = new_chat_id
            st.session_state.chat_sessions[new_chat_id] = {
                "title": "New Chat",
                "messages": []
            }
            st.rerun()
        
        # List existing chats
        for chat_id, chat_data in list(st.session_state.chat_sessions.items()):
            if st.button(f"ðŸ“ {chat_data['title'][:30]}", key=f"chat_{chat_id}"):
                st.session_state.current_chat = chat_id
                st.rerun()
    
    # Main chat interface
    current_chat = st.session_state.chat_sessions.get(st.session_state.current_chat, {
        "title": "New Chat",
        "messages": []
    })
    
    # Display chat messages
    for message in current_chat['messages']:
        with st.chat_message(message['role']):
            st.markdown(message['content'])
            
            # Show sources if available
            if message['role'] == 'assistant' and 'sources' in message:
                with st.expander("ðŸ“š Sources"):
                    for source in message['sources']:
                        st.text(f"â€¢ {source}")
    
    # Chat input
    if prompt := st.chat_input("Ask about banking documentation..."):
        # Add user message
        current_chat['messages'].append({
            "role": "user",
            "content": prompt,
            "timestamp": str(datetime.now())
        })
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("ðŸ” Searching documentation..."):
                # Create hybrid retriever
                retriever = HybridRetriever(
                    st.session_state.collection,
                    st.session_state.bm25_index
                )
                
                # Retrieve relevant documents
                retrieved_docs = retriever.retrieve(prompt, k=Config.FINAL_RESULTS_K)
                
                # Generate response
                response = st.session_state.rag_generator.generate_response(
                    prompt,
                    retrieved_docs
                )
                
                # Display answer
                st.markdown(response['answer'])
                
                # Show sources
                if response['sources']:
                    with st.expander("ðŸ“š Sources"):
                        for source in response['sources']:
                            st.text(f"â€¢ {source}")
                
                # Add to chat history
                current_chat['messages'].append({
                    "role": "assistant",
                    "content": response['answer'],
                    "sources": response['sources'],
                    "timestamp": str(datetime.now())
                })
                
                # Update chat title if first message
                if current_chat['title'] == "New Chat" and len(current_chat['messages']) > 0:
                    current_chat['title'] = prompt[:50] + "..."

if __name__ == "__main__":
    main()







Nouvelle slide â€“ Public cible
Ã€ qui sâ€™adresse la solution RAG BNP ED ?
RAG BNP ED a Ã©tÃ© dÃ©veloppÃ©e pour accompagner diffÃ©rents profils impliquÃ©s dans lâ€™exploitation et la transformation du systÃ¨me dâ€™information :
ðŸ‘¥ Ã‰quipes mÃ©tiers
AccÃ¨s rapide et fiable Ã  la documentation ATLAS
Recherche simplifiÃ©e sans connaissance technique
RÃ©ponses claires, sourcÃ©es et contextualisÃ©es
ðŸ—ï¸ Ã‰quipes IT / DSI
ComprÃ©hension de lâ€™architecture de la documentation ATLAS
AccÃ¨s structurÃ© aux rÃ©fÃ©rentiels et aux dÃ©pendances documentaires
Support Ã  lâ€™analyse et Ã  la maintenance du SI
ðŸ”„ Ã‰quipes Transformation du SystÃ¨me dâ€™Information
Accompagnement des projets de migration et dâ€™Ã©volution du SI
Vision transverse de la documentation existante
RÃ©duction du temps de recherche et dâ€™analyse documentaire
ðŸ’¡ Variante plus synthÃ©tique (si tu veux une slide trÃ¨s lÃ©gÃ¨re)
RAG BNP ED sâ€™adresse aux Ã©quipes mÃ©tiers, Ã  la DSI et aux acteurs de la transformation du systÃ¨me dâ€™information, en facilitant lâ€™accÃ¨s, la comprÃ©hension et lâ€™exploitation de la documentation ATLAS.














RAG BNP ED est un chatbot intelligent basÃ© sur la mÃ©thode RAG (Retrieval Augmented Generation) et sur une base de connaissances centralisÃ©e contenant la documentation ATLAS.
Il permet une recherche conversationnelle rapide et fiable, avec des rÃ©ponses systÃ©matiquement sourcÃ©es Ã  partir des documents officiels.
La solution Ã©volue automatiquement avec les nouvelles documentations, garantissant des rÃ©ponses toujours Ã  jour.






RAG BNP ED est un chatbot intelligent basÃ© sur une base de connaissances centralisÃ©e (documentation ATLAS).
Il permet une recherche conversationnelle rapide et fiable, avec des rÃ©ponses systÃ©matiquement sourcÃ©es.
La solution Ã©volue automatiquement avec les nouvelles documentations, garantissant des rÃ©ponses toujours Ã  jour.








La documentation ATLAS est volumineuse et critique pour les Ã©quipes mÃ©tiers
Lâ€™accÃ¨s Ã  lâ€™information repose aujourdâ€™hui sur une recherche manuelle chronophage
Objectif : faciliter lâ€™accÃ¨s rapide, fiable et sourcÃ© Ã  lâ€™information interne via un outil intelligent




Bonjour,

Pourriez-vous sâ€™il vous plaÃ®t procÃ©der Ã  la rÃ©initialisation de mon mot de passe ? Je vous remercie par avance.

Cordialement,
Nihad Senhadji


Bonjour Ã  tous,

Suite Ã  notre prÃ©cÃ©dent Ã©change, je vous informe que nous avons pu identifier et renseigner les dates dâ€™impayÃ©s en nous basant sur lâ€™extraction transmise par Chaima.

Toutefois, 11 cas restent pour lesquels les dates dâ€™impayÃ©s nâ€™ont pas pu Ãªtre renseignÃ©es, malgrÃ© lâ€™utilisation de cette extraction. Ces cas demeurent donc identifiÃ©s comme anomalies.

Je reste Ã  votre disposition pour toute prÃ©cision complÃ©mentaire ou pour Ã©changer sur ces cas si besoin.

Cordialement,
Nihad Senhadji
GA-15








Bonjour Ã  tous,

Veuillez trouver ci-joint la nouvelle version du fichier Ã‰tat Central des Engagements arrÃªtÃ© au 31/12/2025.

Lors du traitement, nous avons constatÃ© que certaines dates dâ€™impayÃ©s nâ€™ont pas Ã©tÃ© retrouvÃ©es dans les donnÃ©es dâ€™extraction disponibles.
Pour cette raison, une nouvelle feuille a Ã©tÃ© ajoutÃ©e au fichier, intitulÃ©e :

Â« Ã‰tat des engagements â€“ Anomalies dates dâ€™impayÃ©s Â»

Cette feuille regroupe lâ€™ensemble des engagements pour lesquels les dates dâ€™impayÃ©s nâ€™ont pas pu Ãªtre identifiÃ©es Ã  partir de lâ€™extraction.

Le fichier contient dÃ©sormais les feuilles suivantes :

1. Ã‰tat Central des Engagements
2. Ã‰tat des engagements â€“ Anomalies dates dâ€™impayÃ©s
3. Ã‰tat dÃ©taillÃ© par chapitre
4. Rapprochement des crÃ©ances â€“ Bilan
5. Rapprochement des crÃ©ances hors bilan
6. Rapprochement CR / AR / CB
7. Reporting social BG
8. Ã‰tat dÃ©taillÃ© par ratios prudentiels
9. Risques â€“ Rapprochement RP
10. Risques â€“ Grands Risques
11. Risques â€“ Grands Risques par entitÃ©
12. Risques â€“ Annexe 9000
13. Rapprochement reporting provisions
14. Rapprochement affectation des garanties
15. Ã‰tat des garanties

Je reste Ã  votre disposition pour toute information complÃ©mentaire ou clarification si besoin.

Cordialement,
Nihad Senhadji
GA-15







Bien sÃ»r ! Voici une version rÃ©Ã©crite, fluide, professionnelle et claire de ton email :


---

Objet : MÃ©thodologie â€“ DÃ©tection de communautÃ©s de fraude (AI Sentinilia)

Bonjour Ryma,

Dans le cadre du projet AI Sentinilia dÃ©diÃ© Ã  la dÃ©tection de communautÃ©s de fraude, la mÃ©thodologie utilisÃ©e repose sur les critÃ¨res suivants :

Les liens entre clients sont identifiÃ©s Ã  partir de coÃ¯ncidences lorsquâ€™ils effectuent des retraits :

Ã  la mÃªme date ;

au mÃªme GAB (ATM) ;

pour un montant supÃ©rieur ou Ã©gal Ã  500 euros ;

avec une frÃ©quence dâ€™au moins 5 retraits entre deux clients ou plus.



Lorsque ces conditions sont remplies, les clients sont considÃ©rÃ©s comme liÃ©s, car leurs comportements de retrait prÃ©sentent des similitudes rÃ©pÃ©tÃ©es et inhabituelles.

Afin de dÃ©tecter les communautÃ©s de fraude, nous utilisons un algorithme appelÃ© Louvain, qui permet de regrouper automatiquement les clients ayant des comportements similaires. Chaque communautÃ© ainsi identifiÃ©e reprÃ©sente un groupe potentiel de fraude, basÃ© sur des comportements de retraits coordonnÃ©s.

Cette solution nâ€™a pas Ã©tÃ© modifiÃ©e depuis sa mise en place, et le suivi Ã©tant trimestriel, nous enverrons les prochains rÃ©sultats en fÃ©vrier 2026.

Je reste Ã  votre disposition pour toute information complÃ©mentaire ou clarification si nÃ©cessaire.

Cordialement,
Nihad Senhadji


---

Si tu veux, je peux aussi te faire une version encore plus courte et simple, parfaite pour des non-techniques, qui tient en 5-6 phrases.
Veuxâ€‘tu que je fasse Ã§aâ€¯?











Comme le suivi est trimestriel, nous enverrons les prochains rÃ©sultats en fÃ©vrier 2025.


Cet algorithme nâ€™a pas Ã©tÃ© modifiÃ© depuis sa mise en place


Afin de dÃ©tecter les communautÃ©s de fraude, nous utilisons un algorithme appelÃ© Louvain, qui permet de regrouper automatiquement les clients ayant des comportements similaires.


Bonjour,

Dans le cadre du projet AI Sentinilia dÃ©diÃ© Ã  la dÃ©tection de communautÃ©s de fraude, la mÃ©thodologie utilisÃ©e repose sur les Ã©tapes suivantes :

- Les liens entre clients sont dâ€™abord identifiÃ©s Ã  partir des critÃ¨res suivants :

   - la localisation du GAB (ATM) oÃ¹ les retraits ont Ã©tÃ© effectuÃ©s ;
   - la frÃ©quence des retraits ;
   - des coÃ¯ncidences entre plusieurs clients lorsque les retraits ont eu lieu :
      - Ã  la mÃªme date,
      - au mÃªme GAB,
      - avec un montant supÃ©rieur ou Ã©gal Ã  500 euros,
      - et ce au moins 5 fois entre deux clients ou plus.

- Lorsque ces conditions sont rÃ©unies, les clients sont considÃ©rÃ©s comme liÃ©s, car leurs comportements de retrait prÃ©sentent des similitudes rÃ©pÃ©tÃ©es et inhabituelles.

- La mÃ©thode de Louvain est ensuite utilisÃ©e pour regrouper automatiquement les clients liÃ©s :

   - elle permet de constituer des communautÃ©s de clients fortement connectÃ©s ;
   - chaque communautÃ© reprÃ©sente un groupe potentiel de fraude, basÃ© sur des comportements de retraits coordonnÃ©s.

Je reste Ã  votre disposition pour toute information complÃ©mentaire ou clarification si nÃ©cessaire.

Cordialement,

Nihad Senhadji



















Bonjour,

Je vous informe que jâ€™ai annulÃ© la version du fichier dÃ©jÃ  envoyÃ©e prÃ©cÃ©demment.
Nous sommes actuellement en train de finaliser la nouvelle version, et je vous lâ€™enverrai dÃ¨s que ce sera terminÃ©.

Merci pour votre comprÃ©hension.

Cordialement,
Nihad Senhadji

Bonjour,

Veuillez trouver ci-joint le fichier demandÃ©, comme convenu.

Nous vous enverrons la version finale, contenant lâ€™ensemble des feuilles, dans la soirÃ©e.

Cordialement,
Nihad Senhadji


Les colonnes du fichier fournissent toutes les informations nÃ©cessaires â€” identitÃ©, dates, lieux et rÃ©fÃ©rences clients â€” afin de faciliter et structurer lâ€™analyse des Ã©quipes sur ces cas sensibles.


La deuxiÃ¨me feuille, avec un score de similaritÃ© infÃ©rieur Ã  95 %, correspond aux cas oÃ¹ le modÃ¨le dâ€™IA a permis de rÃ©duire une grande partie de la recherche manuelle.
Dans cette feuille, tous les noms qui ne figurent pas dans notre base clients Atlas sont automatiquement Ã©cartÃ©s, ce qui Ã©limine les faux positifs et allÃ¨ge considÃ©rablement le travail des Ã©quipes opÃ©rationnelles.
ðŸ”¹ La troisiÃ¨me feuille, avec un score de similaritÃ© supÃ©rieur Ã  95 %, regroupe les noms nÃ©cessitant une analyse approfondie par lâ€™Ã©quipe opÃ©rationnelle, car ils prÃ©sentent une forte probabilitÃ© de correspondance.










ðŸŽ¬ Script vidÃ©o â€“ PrÃ©sentation de Decad AI Solution

> Bonjour,

Dans cette vidÃ©o, je vais vous prÃ©senter Decad AI Solution, accessible via un lien de web application Domino.

On commence par lâ€™insertion du fichier DECAD, envoyÃ© par la Banque dâ€™AlgÃ©rie, directement dans lâ€™application.

Ensuite, un simple clic permet de lancer le traitement. La solution sâ€™appuie sur un modÃ¨le dâ€™intelligence artificielle exÃ©cutÃ© en arriÃ¨re-plan, avec un temps dâ€™exÃ©cution dâ€™environ 10 minutes.

Une fois le traitement terminÃ©, on clique sur â€œRÃ©sultatsâ€ afin de tÃ©lÃ©charger le fichier Excel de sortie, comme vous pouvez le voir Ã  lâ€™Ã©cran.

Le fichier Excel gÃ©nÃ©rÃ© contient trois feuilles distinctes :

ðŸ”¹ La premiÃ¨re feuille, appelÃ©e PiÃ¨ce dâ€™audit, regroupe les rÃ©sultats avec des scores de similaritÃ© infÃ©rieurs et supÃ©rieurs Ã  95 %.

ðŸ”¹ La deuxiÃ¨me feuille contient les rÃ©sultats avec un score infÃ©rieur Ã  95 %, correspondant aux cas non identifiÃ©s par le modÃ¨le dâ€™IA.

ðŸ”¹ La troisiÃ¨me feuille regroupe les rÃ©sultats avec un score supÃ©rieur Ã  95 %, qui reprÃ©sentent les cas prioritaires Ã  analyser par lâ€™Ã©quipe opÃ©rationnelle.

Il est important de noter que les trois feuilles contiennent exactement les mÃªmes colonnes.

Voici la liste des colonnes suivantes :

COLONNE 1 et 2 DECAD

Nom interdits chequiers

Raison sociale base atlas

Nom abrÃ©gÃ© tiers

Date naissance interdits chequiers ET DE NOTRE BASE ATLAS

Date de crÃ©ation (entreprise)

Date naissance Match

Date de crÃ©ation Match

Lieu naissance interdits chequier ET DE LA BASE atlas

Lieu naissance

Lieu naissance Match

Date d'effet et de levÃ©e

Compte client

Code catÃ©gorie client

Id tiers

Score de similaritÃ©


GrÃ¢ce Ã  cette structuration, Decad AI Solution permet de rÃ©duire le temps de traitement, de prioriser les analyses, et de sÃ©curiser le processus de contrÃ´le grÃ¢ce Ã  lâ€™intelligence artificielle.

Merci pour votre attention.




---

If you want, I can also make a version more â€œoral / dynamiqueâ€, so it sounds natural when you speak it in the video, like a real demo narration.

Do you want me to do that?



















Bonjour,

Merci pour votre message et pour la transmission de lâ€™Expression de Besoin (EDB) relative Ã  lâ€™automatisation du processus de traitement des agios rÃ©servÃ©s et de leurs reprises liÃ©es aux clients douteux â€“ pÃ©rimÃ¨tre EPS.

Suite Ã  lâ€™analyse de cette EDB et conformÃ©ment aux Ã©lÃ©ments partagÃ©s, je vous prie de trouver en piÃ¨ce jointe les rÃ©sultats de lâ€™APA relatifs au mois de dÃ©cembre 2025.

Ces rÃ©sultats correspondent uniquement au traitement du mois de dÃ©cembre 2025 et constituent la sortie actuelle de lâ€™APA Ã  ce stade.
Le fichier Excel de rÃ©sultats contient Ã  la fois des clients sains et des clients douteux.

Ils sont le fruit de plusieurs ateliers de travail menÃ©s avec Mouna, au cours desquels nous avons rÃ©alisÃ© de nombreuses vÃ©rifications manuelles, ainsi que des rapprochements dÃ©taillÃ©s entre les rÃ©sultats APA et les rÃ©sultats manuels du mois dâ€™octobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les rÃ¨gles de gestion appliquÃ©es dans le cadre de cette APA, en cohÃ©rence avec les besoins fonctionnels et techniques dÃ©crits dans lâ€™EDB et avec la recommandation de la Banque dâ€™AlgÃ©rie visant Ã  remplacer le traitement manuel actuellement rÃ©alisÃ© par lâ€™Ã©quipe DRAC.

Il sâ€™agit actuellement dâ€™une phase de test. Il est proposÃ© de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilitÃ© du dispositif et de permettre la validation dÃ©finitive des rÃ©sultats par vos soins.

Par ailleurs, lâ€™APA sera livrÃ©e la semaine prochaine afin dâ€™Ãªtre dÃ©ployÃ©e et exÃ©cutÃ©e sur les postes des utilisateurs concernÃ©s.
Ã€ cet effet, nous aurons besoin :

- de lâ€™installation de lâ€™APA sur les postes utilisateurs,
- ainsi que de la mise Ã  disposition dâ€™un lien dâ€™accÃ¨s vers lâ€™emplacement de dÃ©pÃ´t des fichiers de sortie.

Je reste bien entendu Ã  votre disposition, ainsi quâ€™Amina, pour tout complÃ©ment dâ€™information ou si vous souhaitez planifier un point dâ€™Ã©change.

Cordialement,
Nihad Senhadji

---

If you want, I can now:

- make a very formal / regulatory version (signatureâ€“engagement context), or
- shorten it for a manager-level reply.












Le fichier Excel de rÃ©sultats contient Ã  la fois des clients sains et des clients douteux


Bonjour,

Merci pour votre message et pour les Ã©lÃ©ments transmis lors de notre Ã©change de jeudi.

Suite Ã  lâ€™analyse de lâ€™expression de besoin relative Ã  lâ€™automatisation du traitement des agios rÃ©servÃ©s leasing, je vous prie de trouver en piÃ¨ce jointe les rÃ©sultats de lâ€™APA relatifs au mois de dÃ©cembre 2025.

Ces rÃ©sultats correspondent uniquement au traitement du mois de dÃ©cembre 2025 et constituent la sortie actuelle de lâ€™APA Ã  ce stade.

Ils sont le fruit de plusieurs ateliers de travail menÃ©s avec Mouna, au cours desquels nous avons rÃ©alisÃ© de nombreuses vÃ©rifications manuelles, ainsi que des rapprochements dÃ©taillÃ©s entre les rÃ©sultats APA et les rÃ©sultats manuels du mois dâ€™octobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les rÃ¨gles de gestion appliquÃ©es dans le cadre de cette APA.

Il sâ€™agit actuellement dâ€™une phase de test. Il est proposÃ© de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilitÃ© du dispositif et de permettre la validation dÃ©finitive des rÃ©sultats par vos soins.

Par ailleurs, lâ€™APA sera livrÃ©e la semaine prochaine afin dâ€™Ãªtre dÃ©ployÃ©e et exÃ©cutÃ©e sur les postes des utilisateurs concernÃ©s.
Ã€ cet effet, nous aurons besoin :

- de lâ€™installation de lâ€™APA sur les postes utilisateurs,
- ainsi que de la mise Ã  disposition dâ€™un lien dâ€™accÃ¨s vers lâ€™emplacement de dÃ©pÃ´t des fichiers de sortie.

Je reste bien entendu Ã  votre disposition pour tout complÃ©ment dâ€™information, retour ou ajustement nÃ©cessaire.

Cordialement,
Nihad Senhadji

---

If you want, I can also adapt the wording to:

- explicitly request






Bonjour,

Merci pour votre message et pour les Ã©lÃ©ments transmis lors de notre Ã©change de jeudi.

Suite Ã  lâ€™analyse de lâ€™expression de besoin relative Ã  lâ€™automatisation du traitement des agios rÃ©servÃ©s leasing, je vous prie de trouver en piÃ¨ce jointe les rÃ©sultats de lâ€™APA.

Ces rÃ©sultats correspondent uniquement au traitement du mois en cours et constituent la sortie actuelle de lâ€™APA Ã  ce stade.

Ils sont le fruit de plusieurs ateliers de travail menÃ©s avec Mouna, au cours desquels nous avons rÃ©alisÃ© de nombreuses vÃ©rifications manuelles, ainsi que des rapprochements dÃ©taillÃ©s entre les rÃ©sultats APA et les rÃ©sultats manuels du mois dâ€™octobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les rÃ¨gles de gestion appliquÃ©es dans le cadre de cette APA.

Il sâ€™agit actuellement dâ€™une phase de test. Il est proposÃ© de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilitÃ© du dispositif et de permettre la validation dÃ©finitive des rÃ©sultats par vos soins.

Par ailleurs, lâ€™APA sera livrÃ©e la semaine prochaine afin dâ€™Ãªtre dÃ©ployÃ©e et exÃ©cutÃ©e sur les postes des utilisateurs concernÃ©s.
Ã€ cet effet, nous aurons besoin :

- de lâ€™installation de lâ€™APA sur les postes utilisateurs,
- ainsi que de la mise Ã  disposition dâ€™un lien dâ€™accÃ¨s vers lâ€™emplacement de dÃ©pÃ´t des fichiers de sortie.

Je reste bien entendu Ã  votre disposition pour tout complÃ©ment dâ€™information, retour ou ajustement nÃ©cessaire.

Cordialement,
Nihad Senhadji

---

If you want, I can also:

- slightly strengthen the request for validation, or
- adapt the wording to a more regulatory / audit-compliant style for DRAC.






Bonjour [Nom du manager],

Je souhaite vous partager un point de situation concernant lâ€™avancement des projets APA.

1. Avancement gÃ©nÃ©ral (APA ESP & APA ArrÃªtÃ©)
Suite aux diffÃ©rents workshops organisÃ©s avec la DRAC, lâ€™ensemble des rÃ¨gles de gestion a Ã©tÃ© confirmÃ© et validÃ© pour les deux projets.
Un rapprochement a Ã©galement Ã©tÃ© rÃ©alisÃ© sur la base de leurs fichiers de novembre 2025.

2. Projet APA ESP
Un seul point reste identifiÃ© : un problÃ¨me de code fusion concernant un compte unique.
La DRAC mâ€™a confirmÃ© que ce point est dÃ©jÃ  portÃ© Ã  votre connaissance.

3. Projet APA ArrÃªtÃ©
Un seul point est actuellement en suspens :

- Un Ã©cart liÃ© au solde du compte Ã  vue.
Ã€ ce stade, la DRAC nâ€™est pas en mesure dâ€™identifier la mÃ©thode ou la source utilisÃ©e pour le calcul de ce solde et doit effectuer des vÃ©rifications de son cÃ´tÃ© avant de nous apporter un retour dÃ©finitif.

4. Projet APA Leasing
Le projet APA Leasing nâ€™a pas encore Ã©tÃ© dÃ©marrÃ© Ã  ce stade.

Vous trouverez ci-dessous le lien regroupant les Ã©lÃ©ments pour les projets APA ESP et APA ArrÃªtÃ© :
[Lien Ã  insÃ©rer]

Je reste Ã  votre disposition pour toute prÃ©cision complÃ©mentaire ou pour un point de suivi.

Cordialement,
Nihad Senhadji






your thanks:

"Je tiens Ã  vous remercier pour votre accompagnement et vos conseils tout au long de lâ€™annÃ©e, et je souhaite continuer Ã  contribuer activement Ã  la rÃ©ussite des objectifs de la Banque, tout en participant aux projets IA et mÃ©tiers."

Itâ€™s professional, appreciative, and forward-looking.

I can also make a slightly shorter, sharper version if you want it for an email reply.

You can write it like this:

"Je souhaite continuer Ã  contribuer activement Ã  la rÃ©ussite des objectifs de la Banque, tout en participant aux projets IA et mÃ©tiers."

Itâ€™s concise, professional, and inclusive of both AI and other banking projects.

If you want, I can also suggest 2â€“3 alternative formulations that sound slightly more executive.


Understood. We need a leadership-oriented tone without implying you want to replace himâ€”showing responsibility, initiative, and team support while still highlighting AI. Hereâ€™s a refined version:


---

Bonjour [Nom du manager],

Merci pour votre retour et pour lâ€™Ã©valuation de ma contribution en 2025. Jâ€™ai bien pris en compte les axes dâ€™amÃ©lioration identifiÃ©s, notamment sur le cadrage des besoins mÃ©tiers, la structuration des travaux et la clartÃ© de la communication.

Je souhaite exprimer mon objectif professionnel, qui est dâ€™Ã©voluer vers un rÃ´le sur des projets en Intelligence Artificielle, avec des responsabilitÃ©s clairement dÃ©finies et des livrables Ã  impact opÃ©rationnel.

Je reste par ailleurs pleinement engagÃ©e Ã  soutenir lâ€™ensemble des projets bancaires et Ã  accompagner lâ€™Ã©quipe dans lâ€™organisation et la coordination des travaux, afin de garantir la qualitÃ© et lâ€™efficacitÃ© des livrables, tout en contribuant activement Ã  la rÃ©ussite des projets IA et mÃ©tiers.

Cordialement,
Nihad Senhadji


---

This version:

Shows initiative and readiness to support the team

Positions you for AI projects

Avoids sounding like you want to replace him


If you want, I can make an even snappier, executive-style version that fits in one paragraph. Do you want me to do that?
