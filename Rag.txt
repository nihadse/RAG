import streamlit as st
import datetime
import re
import fitz
import docx
from io import BytesIO
from uuid import uuid4
from chromadb import PersistentClient
from sentence_transformers import SentenceTransformer
from langchain.schema import Document

# === Chroma Setup ===
chroma_client = PersistentClient(path="/domino/datasets/local/vect-pro-base/")
model_path = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"
model = SentenceTransformer(model_path)


class BGEM3EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        self.dimension = 1024

    def __call__(self, input_texts):
        if isinstance(input_texts, str):
            input_texts = [input_texts]
        return self.model.encode(input_texts).tolist()


embedding_function = BGEM3EmbeddingFunction(model)

collection = chroma_client.get_or_create_collection(
    name="my_documents",
    embedding_function=embedding_function,
    metadata={"hnsw:space": "cosine", "dimension": embedding_function.dimension}
)


# === Text Processing Utilities ===
def extract_text_from_uploaded(uploaded_file):
    filename = uploaded_file.name.lower()

    if filename.endswith(".pdf"):
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        return "\n".join(page.get_text() for page in doc)

    elif filename.endswith(".txt"):
        return uploaded_file.read().decode("utf-8")

    elif filename.endswith(".docx"):
        doc = docx.Document(BytesIO(uploaded_file.read()))
        return "\n".join([p.text for p in doc.paragraphs])

    else:
        return "Unsupported file format."


def clean_text(text, lowercase=False):
    text = re.sub(r'\.{2,}', '.', text)
    text = re.sub(r'\t+', ' ', text)
    text = "\n".join(line.strip() for line in text.splitlines())
    text = "\n".join([line for line in text.splitlines() if line.strip() != ""])
    return text.lower() if lowercase else text


def split_into_chunks(text, chunk_size=1000, overlap=200):
    start = 0
    chunks = []
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks


def get_existing_sources():
    try:
        results = collection.query(query_texts=["."], n_results=1000, include=["metadatas"])
        metadatas = results.get("metadatas", [[]])[0]
        return set(meta["source"] for meta in metadatas if "source" in meta)
    except:
        return set()


# === RAG Chatbot Pipeline ===
def rag_chatbot(query, temp_chunks=None):
    st.markdown(f"ðŸ• {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    alternative_queries = generate_alternative_queries(query)
    all_queries = [query] + alternative_queries
    all_results = []

    retriever = CustomRetrieverWithHistory(collection=collection, k=50, rerank_k=30)

    for q in all_queries:
        context = retriever.get_relevant_documents(q)

        if temp_chunks:
            context += [{"text": chunk, "metadata": {"source": "TEMP_UPLOAD"}} for chunk in temp_chunks]

        answer = generate_response(q, context)

        all_results.append({
            "query": q,
            "answer": answer,
            "context": context,
            "num_sources": len(context)
        })

    best_result = max(all_results, key=lambda x: x["num_sources"])
    final_query = best_result["query"]
    final_answer = best_result["answer"]
    final_context = best_result["context"]

    explanation = generate_answer_explanation(final_query, final_answer, final_context)
    evaluation = evaluate_rag_response(final_query, final_answer, final_context)
    sources = extract_source_files(final_context)

    retriever.update_history(final_query, final_answer)
    retriever.save_history()

    return {
        "query": final_query,
        "answer": final_answer,
        "context": final_context,
        "evaluation": evaluation,
        "sources": sources,
        "explanation": explanation,
        "time": datetime.datetime.now()
    }


# === Streamlit UI ===
def main():
    st.set_page_config(page_title="ðŸ§  RAG Chatbot", layout="centered")
    st.title("ðŸ“š Ask Your Documents (Preloaded + Temporary)")

    temp_chunks = []
    uploaded_file = st.sidebar.file_uploader("ðŸ“¤ Upload a new document (optional)", type=["pdf", "txt", "docx"])

    if uploaded_file:
        filename = uploaded_file.name.lower()
        existing = get_existing_sources()

        if filename in existing:
            st.sidebar.warning("âš ï¸ This file is already indexed. It will not be used again.")
        else:
            with st.spinner("Extracting text..."):
                raw_text = extract_text_from_uploaded(uploaded_file)

            if raw_text.startswith("Unsupported"):
                st.error("âŒ Unsupported format.")
                return

            cleaned = clean_text(raw_text)
            temp_chunks = split_into_chunks(cleaned)
            st.sidebar.success(f"âœ… File processed. {len(temp_chunks)} temporary chunks ready.")

    query = st.text_input("ðŸ’¬ Ask a question about the documents:")

    if query:
        with st.spinner("ðŸ’¡ Running RAG..."):
            result = rag_chatbot(query, temp_chunks=temp_chunks)

            st.subheader("ðŸ¤– Answer")
            st.write(result["answer"])

            st.subheader("ðŸ“„ Source Chunks")
            for i, doc in enumerate(result["context"]):
                st.markdown(f"**{i+1}.** {doc['text'][:500]}...\n---")

            st.subheader("ðŸ“Š Evaluation")
            st.json(result["evaluation"])

            st.subheader("ðŸ“Œ Explanation")
            st.write(result["explanation"])

            st.subheader("ðŸ“ Sources")
            st.write(result["sources"])


if __name__ == "__main__":
    main()



import os
import re
import fitz
import docx
import streamlit as st
from io import BytesIO
from uuid import uuid4
from chromadb import PersistentClient
from sentence_transformers import SentenceTransformer
from langchain.schema import Document


# ========== SETUP ==========
chroma_client = PersistentClient(path="/domino/datasets/local/vect-pro-base/")

model_path = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"
model = SentenceTransformer(model_path)


class BGEM3EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        self.dimension = 1024

    def __call__(self, input_texts):
        if isinstance(input_texts, str):
            input_texts = [input_texts]
        return self.model.encode(input_texts).tolist()


embedding_function = BGEM3EmbeddingFunction(model)

collection = chroma_client.get_or_create_collection(
    name="my_documents",
    embedding_function=embedding_function,
    metadata={"hnsw:space": "cosine", "dimension": embedding_function.dimension}
)


# ========== TEXT PROCESSING UTILS ==========
def extract_text_from_uploaded(uploaded_file):
    filename = uploaded_file.name.lower()

    if filename.endswith(".pdf"):
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        return "\n".join(page.get_text() for page in doc)

    elif filename.endswith(".txt"):
        return uploaded_file.read().decode("utf-8")

    elif filename.endswith(".docx"):
        doc = docx.Document(BytesIO(uploaded_file.read()))
        return "\n".join([p.text for p in doc.paragraphs])

    else:
        return "Unsupported file format."


def clean_text(text, lowercase=False):
    text = re.sub(r'\.{2,}', '.', text)
    text = re.sub(r'\t+', ' ', text)
    text = "\n".join(line.strip() for line in text.splitlines())
    text = "\n".join([line for line in text.splitlines() if line.strip() != ""])
    return text.lower() if lowercase else text


def split_into_chunks(text, chunk_size=1000, overlap=200):
    start = 0
    chunks = []
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks


# ========== HELPER ==========
def get_existing_sources():
    """Retrieve all existing source file names from Chroma."""
    try:
        # We'll query with a dummy term just to extract metadatas
        dummy_query = collection.query(query_texts=["dummy"], n_results=1000, include=["metadatas"])
        metadatas = dummy_query.get("metadatas", [[]])[0]
        return set(meta["source"] for meta in metadatas if "source" in meta)
    except:
        return set()


# ========== STREAMLIT APP ==========
def main():
    st.set_page_config(page_title="ðŸ§  RAG Chatbot", layout="centered")
    st.title("ðŸ“š Ask Your Documents")

    new_chunks = []
    uploaded_file = st.sidebar.file_uploader("ðŸ“¤ Upload a new document (not stored)", type=["pdf", "txt", "docx"])

    if uploaded_file:
        filename = uploaded_file.name.lower()
        existing_sources = get_existing_sources()

        if filename in existing_sources:
            st.warning("âš ï¸ This document has already been embedded. It will be ignored.")
        else:
            with st.spinner("Processing document..."):
                raw_text = extract_text_from_uploaded(uploaded_file)
                if raw_text.startswith("Unsupported"):
                    st.error("âŒ Unsupported file format.")
                    return

                cleaned = clean_text(raw_text)
                new_chunks = split_into_chunks(cleaned)
                st.success(f"âœ… Document processed into {len(new_chunks)} temporary chunks (not stored).")

    query = st.text_input("ðŸ’¬ Ask a question about the documents:")

    if query:
        with st.spinner("Searching ChromaDB..."):
            results = collection.query(
                query_texts=[query],
                n_results=5,
                include=["documents", "metadatas"]
            )
            base_docs = results["documents"][0]

        # Optional: Embed new chunks and rerank manually
        if new_chunks:
            new_embeddings = embedding_function(new_chunks)
            new_scores = model.similarity_faiss([query], new_embeddings)[0]  # You can use cosine if needed
            top_k = 3
            top_indices = sorted(range(len(new_scores)), key=lambda i: -new_scores[i])[:top_k]
            extra_docs = [new_chunks[i] for i in top_indices]
        else:
            extra_docs = []

        all_docs = base_docs + extra_docs

        if all_docs:
            st.subheader("ðŸ“„ Retrieved Chunks:")
            for i, doc in enumerate(all_docs):
                st.markdown(f"**{i+1}.** {doc[:500]}...\n---")
        else:
            st.warning("No relevant documents found.")

        st.subheader("ðŸ¤– Assistant Answer (Placeholder)")
        st.success("Answer simulated from retrieved content.")


if __name__ == "__main__":
    main()




from uuid import uuid4
from typing import List
from io import BytesIO
import fitz  # PyMuPDF
import docx

def extract_text_from_uploaded(uploaded_file) -> str:
    """Extracts raw text from PDF, DOCX, or TXT."""
    filename = uploaded_file.name.lower()

    if filename.endswith(".pdf"):
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        return "\n".join(page.get_text() for page in doc)

    elif filename.endswith(".txt"):
        return uploaded_file.read().decode("utf-8")

    elif filename.endswith(".docx"):
        doc = docx.Document(BytesIO(uploaded_file.read()))
        return "\n".join([p.text for p in doc.paragraphs])

    else:
        return "Unsupported file format."


def chunk_text(text: str, chunk_size=1000, overlap=200) -> List[str]:
    """Splits text into overlapping chunks."""
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks


def embed_and_store_text(chroma_collection, text: str, file_name: str):
    """Chunks, embeds, and adds the text to an existing ChromaDB collection."""
    chunks = chunk_text(text)
    for i, chunk in enumerate(chunks):
        chroma_collection.add(
            documents=[chunk],
            metadatas=[{"source": file_name, "chunk": i}],
            ids=[str(uuid4())]
        )


Bonjour,
Tout fonctionne correctement, merci pour l'accÃ¨s.






Objet : Remboursement frais de visa Capago

Bonjour,

Veuillez trouver ci-joint la capture dâ€™Ã©cran de mes frais de visa Capago pour le remboursement.

Cordialement,
[Votre prÃ©nom et nom]



.
