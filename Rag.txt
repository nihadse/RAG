pip install sentence-transformers accelerate torch>=2.0.0


import torch
from sentence_transformers import SentenceTransformer
from accelerate import Accelerator

# Initialize accelerator (auto-distributes across GPUs)
accelerator = Accelerator()
device = accelerator.device

# Load model with FP16 and distribute across GPUs
model = SentenceTransformer("BAAI/bge-m3", device=device)
model = model.half()  # FP16 for speed
model = accelerator.prepare(model)  # Distribute across GPUs

# Example: 1 million documents (adjust to your data)
texts = ["doc1", "doc2", ..., "doc1000000"]

# Batch size per GPU (adjust based on H100 memory)
# H100 has 80GB VRAM → 12 GPUs → ~6.6GB/GPU → batch_size=256 is safe
batch_size = 256 * accelerator.num_processes  # Total batches across GPUs

# Parallel embedding generation
embeddings = model.encode(
    texts,
    batch_size=batch_size,
    convert_to_tensor=True,  # Keep tensors on GPU
    show_progress_bar=True,
    normalize_embeddings=True,
    device=device,
)

# Gather embeddings from all GPUs (if needed)
embeddings = accelerator.gather(embeddings)















def generate_response(query, context):
    # Extract unique sources with page numbers
    sources = {}
    for chunk in context:
        source = chunk["metadata"]["source"]
        page = chunk["metadata"].get("page", "N/A")  # Handle missing page numbers
        if source not in sources:
            sources[source] = set()
        sources[source].add(page)
    
    # Format sources string
    sources_str = "\n".join([
        f"- [Source: {source}, Pages: {', '.join(sorted(pages))}]"
        for source, pages in sources.items()
    ])

    # Improved French prompt with strict formatting
    prompt = f"""Vous êtes un assistant spécialisé en architecture bancaire. Répondez en français avec:
1. Une réponse précise basée exclusivement sur les documents fournis
2. La procédure d'extraction des informations
3. Les références exactes des documents utilisés

Documents disponibles:
{context}

Question: {query}

Exigez le format de réponse suivant:
[Procédure]
1. J'ai identifié les informations pertinentes dans...
2. J'ai croisé les données entre...
3. Conclusion tirée de...

[Sources]
{sources_str}"""

    client = AzureOpenAI(
        api_version="AZURE_API_VERSION",
        azure_endpoint="APIGEE_ENDPOINT",
        http_client=httpx.Client(verify=False)
    
    with httpx.Client(verify=False) as http_client:
        completion = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "Assistant expert en systèmes bancaires français utilisant RAG."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3  # Lower for more factual responses
        )
    
    return completion.choices[0].message.content


import openai
import os

# Disable SSL verification (use with caution!)
os.environ["REQUESTS_CA_BUNDLE"] = ""
openai.verify_ssl_certs = False

# Test the embedding
openai.Embedding.create(input="test", model="text-embedding-3-small")


pip install --upgrade certifi


import tiktoken

# Replace with your local path Manually download the cl100k_base.tiktoken file from a trusted network and load it locally:


encoding = tiktoken.get_encoding(
    "cl100k_base",
    pathex_to_tiktoken_encouraged_dir="path/to/local/cl100k_base.tiktoken"
)
