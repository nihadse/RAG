import streamlit as st
import httpx
from openai import AzureOpenAI

# Initialize Azure OpenAI client
client = AzureOpenAI(
    api_version="AZURE_API_VERSION",  # Replace with your Azure API version
    azure_endpoint="AZURE_ENDPOINT",  # Replace with your Azure endpoint
    api_key="AZURE_API_KEY",  # Replace with your Azure API key
    http_client=httpx.Client(verify=False)
)

# Initialize session state for chat history
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Function to generate queries
def generate_queries(original_query, num_queries=5):
    prompt = f"""
    You are an AI Language Model assistant. Your task is to generate {num_queries} different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of distance-based similarity search.

    Provide these alternative questions separated by newlines. Original Query: {original_query}
    """
    
    completion = client.chat.completions.create(
        model="gpt-4",  # Replace with your model name
        messages=[
            {"role": "system", "content": "You are a query generation assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    queries = completion.choices[0].message.content.strip().split("\n")
    return queries[:num_queries]

# Function to retrieve relevant chunks (placeholder - implement your actual retrieval)
def retrieve_relevant_chunks(query, top_k=10):
    # This should be replaced with your actual vector DB retrieval logic
    return [{"document": "Sample document text", "metadata": {"source": "doc.pdf", "page": 1}}]

# Function to generate response
def generate_response(query, context):
    prompt = f"""
    Context: {context}\n\nQuery: {query}\n\nAnswer the query based on the provided facts. 
    Extract the information and provide the pages and the procedure you have used to extract the information. 
    ALWAYS cite the page and the document using (source: filename.pdf, Page x). 
    Respond in French.
    """
    
    completion = client.chat.completions.create(
        model="gpt-4",  # Replace with your model name
        messages=[
            {"role": "system", "content": "You are an assistant based on RAG (Retrieval-Augmented Generation)."},
            {"role": "user", "content": prompt}
        ]
    )
    return completion.choices[0].message.content

# Streamlit app layout
st.title("Chat with Documents üí¨")

# CSS for chat interface
st.markdown("""
<style>
.chat-container {
    height: 500px;
    overflow-y: auto;
    border: 1px solid #e1e4e8;
    border-radius: 10px;
    padding: 20px;
    margin-bottom: 20px;
    background-color: #f9f9f9;
}

.user-message {
    background-color: #007bff;
    color: white;
    border-radius: 15px;
    padding: 10px 15px;
    margin: 10px 0;
    max-width: 80%;
    float: right;
    clear: both;
}

.bot-message {
    background-color: #e9ecef;
    color: black;
    border-radius: 15px;
    padding: 10px 15px;
    margin: 10px 0;
    max-width: 80%;
    float: left;
    clear: both;
}
</style>
""", unsafe_allow_html=True)

# Chat container
chat_placeholder = st.empty()

# Input form at bottom
with st.form("chat_form"):
    col1, col2 = st.columns([5, 1])
    user_input = col1.text_input("Type your message:", key="input", label_visibility="collapsed")
    send_button = col2.form_submit_button("Send")

# Handle user input
if send_button and user_input.strip():
    # Add user message to history
    st.session_state.chat_history.append({"role": "user", "content": user_input})
    
    # Retrieve relevant information
    relevant_chunks = retrieve_relevant_chunks(user_input)
    
    # Generate AI response
    try:
        bot_response = generate_response(user_input, relevant_chunks)
        st.session_state.chat_history.append({"role": "assistant", "content": bot_response})
    except Exception as e:
        st.session_state.chat_history.append({"role": "assistant", "content": f"Error: {str(e)}"})
    
    # Rerun to update display
    st.rerun()

# Display chat history
with chat_placeholder.container():
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    
    for message in st.session_state.chat_history:
        if message["role"] == "user":
            st.markdown(f'<div class="user-message">You: {message["content"]}</div>', unsafe_allow_html=True)
        else:
            st.markdown(f'<div class="bot-message">Assistant: {message["content"]}</div>', unsafe_allow_html=True)
    
    st.markdown('</div>', unsafe_allow_html=True)

# Auto-scroll to bottom using JavaScript
st.markdown("""
<script>
window.addEventListener('load', function() {
    const chatContainer = document.querySelector('.chat-container');
    chatContainer.scrollTop = chatContainer.scrollHeight;
});
</script>
""", unsafe_allow_html=True)










perspective" vs. "perception." Here's




# Fetch all documents and their metadata
results = collection.get(include=["metadatas"])

# Extract filenames from metadata
filenames = list(set([metadata["source"] for metadata in results["metadatas"]]))

# Print the list of filenames
print("All documents in the collection:")
for filename in filenames:
    print(f"- {filename}")







.Objet : Probl√®me de codage du num√©ro de carte VISA

Bonjour,

Le codage du num√©ro de la carte VISA n'a pas √©t√© correctement effectu√©. Il est n√©cessaire de masquer quatre chiffres, tout en conservant visibles les cinq derniers chiffres.

Merci de bien vouloir corriger cela au plus vite.

Cordialement,
[Votre Nom]








"You are a RAG-based assistant that uses PDF documents containing the documentation and procedures of our bank's information system.  
For each question:  
1. Extract relevant information from the PDF documents based on the most precise and consistent passages.  
2. Always cite sources using the format [Source: filename.pdf, Page X].  
3. If the same information appears in multiple places, use the most reliable source and mention other sources in a note.  
4. At the end of the response, list all sources used.  

Example response:  
'For [describe the procedure], the following information was extracted: [details of the information].  
This information can be found in [Source: user_guide.pdf, Page 12].  
If necessary, refer to [Source: security_policy.pdf, Page 5].  
[...]  
**Sources used**:  
- user_guide.pdf, Pages 12, 15  
- security_policy.pdf, Page 5'  






context: (context)

query: (query)

Please extract the necessary information from the provided documents to answer the question accurately. Also, provide the pages and the specific procedure from which you extracted the information. Always cite sources using [Source: filename.pdf, Page X]. If the information was extracted from multiple PDFs, list all the relevant filenames at the end.  

Respond in French.


contexte: (contexte)

requ√™te: (requ√™te)

Et pouvez-vous s'il vous pla√Æt fournir les pages et la proc√©dure √† partir de laquelle vous avez extrait l'information, ainsi que les pages o√π vous avez trouv√© cette information ? Citez toujours les sources en utilisant [Source: nom_du_fichier.pdf, Page X]. Et si vous avez extrait l'information d'un autre fichier PDF, ajoutez √©galement le nom de ce PDF apr√®s avoir donn√© la r√©ponse. √Ä la fin, fournissez les sources, et pouvez-vous s'il vous pla√Æt indiquer les pages et la proc√©dure √† partir de laquelle vous avez extrait l'information ainsi que les pages o√π vous avez trouv√© cette information ?







Vous √™tes un assistant bas√© sur un syst√®me RAG (Retrieval-Augmented Generation) qui utilise des documents PDF contenant la documentation et les proc√©dures du syst√®me d'information de notre banque.  

**Exigences** :  
1. Pour chaque r√©ponse, **indiquez pr√©cis√©ment** :  
   - Les **pages exactes** des PDF o√π l'information est extraite.  
   - La **proc√©dure sp√©cifique** concern√©e (nom ou r√©f√©rence).  
   - **Citez syst√©matiquement les sources** avec le format :  
     `[Source : nom_du_fichier.pdf, Page X]`.  
   - Si l'information provient de plusieurs PDF, mentionnez tous les fichiers concern√©s.  

2. **Structure de r√©ponse demand√©e** :  
   - R√©ponse d√©taill√©e + sources int√©gr√©es dans le texte.  
   - √Ä la fin, ajoutez une section **"Sources utilis√©es"** listant tous les documents cit√©s (ex: `- nom_du_fichier.pdf, Pages X, Y`).  

**Exemple de r√©ponse** :  
*"Pour [d√©crire la proc√©dure], consultez [Source : guide_utilisateur.pdf, Page 12].  
Si n√©cessaire, reportez-vous √† [Source : politique_securite.pdf, Page 5].  
[...]  
**Sources utilis√©es** :  
- guide_utilisateur.pdf, Pages 12, 15  
- politique_securite.pdf, Page 5"*  






Solution :
Un Assistant Intelligent avec une base de connaissances centralis√©e contenant la documentation Atlas ainsi que les proc√©dures de la banque. Il permet une recherche intelligente dans ces documents et propose une interface conversationnelle intuitive pour un acc√®s rapide √† l'information. Le syst√®me √©volue avec les nouvelles documentations, garantissant des r√©ponses toujours √† jour.



Voici la version avec un mot plus adapt√© :


---

Slide 2 : Probl√©matique

üîç Les d√©fis actuels

üìë Trop de documents, trop de temps perdu

Recherche manuelle longue et inefficace (15+ minutes par requ√™te dans 2000+ pages PDF).

Informations dispers√©es et non centralis√©es.

Frustration des employ√©s et impact sur la productivit√©.


üö® Risques majeurs li√©s √† la recherche manuelle

‚ùå Perte de temps excessive

Navigation complexe dans des fichiers multiples (dossier partag√©, intranet, emails).

Difficult√© croissante √† retrouver l‚Äôinformation pertinente avec l‚Äôaugmentation des documents.


‚ö†Ô∏è Risque d‚Äôerreurs et d‚Äôinformations obsol√®tes

Utilisation de proc√©dures p√©rim√©es et risque d‚Äôerreur critique.

Absence de version unique et centralis√©e des documents.


üîç Informations introuvables ou incompl√®tes

R√©sultats incoh√©rents ou inexistants en fonction des mots-cl√©s utilis√©s.

Multiplication des sources √† consulter (PDF, SharePoint, emails, intranet).


üõë Charge mentale et fatigue accrue

Perte de concentration due √† des recherches r√©p√©titives et chronophages.

D√©pendance aux coll√®gues pour obtenir des informations pr√©cises.


üí° Exemple : Un conseiller bancaire face √† une recherche complexe sur la politique des pr√™ts. R√©sultat : 20 minutes perdues entre plusieurs documents et demandes aux coll√®gues, contre 5 secondes avec une IA.

üì∑ Visuel : Employ√© d√©pass√©, documents √©parpill√©s, chronom√®tre affichant "15 min+".


---

Le mot "complexe" remplace "laborieuse", rendant la phrase plus fluide et naturelle. Cela te convient-il ?






Ta pr√©sentation est d√©j√† bien structur√©e et percutante pour un public non technique. Voici une version am√©lior√©e avec plus de d√©tails et d'impact visuel :


---

Slide 1 : Titre

ü¶∏‚Äç‚ôÇÔ∏è Assistant Intelligent pour la Documentation Technique
"Posez une question, obtenez une r√©ponse instantan√©e, appuy√©e par les documents internes."
‚úÖ R√©duction du temps de recherche
‚úÖ Acc√®s rapide aux bonnes proc√©dures
‚úÖ Moins d'erreurs et de frustration
Visuel : Un employ√© posant une question √† un chatbot avec une r√©ponse instantan√©e li√©e √† un document


---

Slide 2 : Probl√©matique

üîç Les d√©fis actuels
üìë Trop de documents, trop de temps perdu

Les √©quipes passent 15+ minutes par recherche dans 2000+ pages PDF

Informations dispers√©es, pas toujours √† jour

Frustration des employ√©s + impact sur la productivit√©


üö® Risques

Utilisation de proc√©dures obsol√®tes ‚Üí erreurs critiques

Apprentissage difficile pour les nouveaux employ√©s

Temps perdu = co√ªts cach√©s pour l‚Äôentreprise


üí° Imaginez : Un technicien doit trouver comment red√©marrer un serveur‚Ä¶ Il passe 20 minutes √† chercher alors qu‚Äôune IA aurait r√©pondu en 10 secondes.

Visuel : Chronom√®tre affichant "15 min" + ic√¥ne d‚Äôun employ√© cherchant dans des fichiers


---

Slide 3 : Solution en image

ü§ñ Notre Assistant Intelligent

üèõÔ∏è Base de connaissances centralis√©e

üîç Recherche intelligente sur les documents

üí¨ Interface conversationnelle intuitive

üõ†Ô∏è Syst√®me qui √©volue avec les nouvelles documentations


üìå C‚Äôest comme un Google interne, con√ßu pour nos documents techniques.

üì∑ Visuel : Un diagramme avec [Documents PDF] ‚Üí [Base de Connaissance IA] ‚Üí [Chatbot] ‚Üí [R√©ponse pertinente]


---

Slide 4 : Fonctionnement simplifi√© (3 √©tapes)

üîß Sous le capot, un assistant ultra-performant

1Ô∏è‚É£ Biblioth√®que Num√©rique

Regroupe, classe et met √† jour les documents automatiquement

üìö Un biblioth√©caire digital


2Ô∏è‚É£ Moteur de Recherche Intelligent

Analyse les intentions derri√®re les questions

üß† Comme un expert qui comprend vraiment votre besoin


3Ô∏è‚É£ Expert Virtuel

G√©n√®re des r√©ponses claires avec r√©f√©rences aux documents sources

üéØ Transparence totale : toujours une source v√©rifiable


üì∑ Infographie illustrant les trois √©tapes avec des ic√¥nes


---

Slide 5 : Processus Visuel

üîÑ Flux de Travail en un coup d'≈ìil

üìë [Documents PDF] ‚Üí üì• [Extraction et indexation] ‚Üí ü§ñ [IA Recherche] ‚Üí üí¨ [Interface Chat] ‚Üí ‚úÖ [R√©ponse avec sources]

üìå Chaque r√©ponse est accompagn√©e d‚Äôun lien direct vers la source.

üì∑ Diagramme avec des fl√®ches montrant le parcours d‚Äôune requ√™te utilisateur


---

Slide 6 : Avantages Concrets

üöÄ Pourquoi c‚Äôest r√©volutionnaire ?

‚è±Ô∏è Gain de temps massif

70% de temps √©conomis√© par rapport √† une recherche manuelle


‚úÖ Fiabilit√© garantie

Toutes les r√©ponses sont sourc√©es et √† jour


üìà Adoption des proc√©dures facilit√©e

Moins d‚Äôerreurs, meilleure conformit√© aux processus internes


üå± Scalable et adaptable

Ajout automatique des nouveaux documents sans effort manuel


üì∑ Graphique en bulles mettant en avant ces b√©n√©fices


---

Slide 7 : D√©mo Visuelle

üé• Cas d‚Äôusage concret

Exemple de conversation :
üîç Question : "Comment escalader un incident technique ?"
üí¨ R√©ponse IA :
"Proc√©dure en 3 √©tapes :

1. Connectez-vous √† Atlas


2. Remplissez le formulaire X


3. Notifiez l‚Äô√©quipe IT via l‚Äôemail Y
üìå Source : Proc√©dure_V2.pdf (page 12)"*



üìå L‚ÄôIA r√©pond en moins de 2 secondes et donne la source exacte !

üì∑ Mockup d‚Äôinterface avec une conversation entre un utilisateur et l‚Äôassistant IA


---

Slide 8 : Prochaines √âtapes

üìÖ Roadmap d‚Äôint√©gration

üöÄ Phase 1 : D√©ploiement IT (Q3 2024)
üë®‚Äçüíª Test sur les proc√©dures techniques

üìë Phase 2 : Extension aux RH (Q4 2024)
üìã Gestion des demandes administratives

üì° Phase 3 : Int√©gration Microsoft Teams (Q1 2025)
üí¨ Accessible directement via Teams & intranet

üì∑ Timeline visuelle avec des jalons clairs


---

Slide 9 : Questions & Discussion

üí° Et Maintenant ?
"Comment voyez-vous cette solution s‚Äôint√©grer dans vos √©quipes ?"

üìå Possibilit√©s :

D√©monstration en live

Cas d‚Äôusage sp√©cifique √† votre entreprise

Retours et suggestions


üì∑ Visuel d‚Äôune √©quipe en brainstorming autour d‚Äôun √©cran affichant l‚ÄôIA en action


---

Ajouts Bonus :

üìå Conseils pour renforcer l‚Äôimpact de la pr√©sentation :
‚úî Utiliser des analogies ‚Üí ‚ÄúC‚Äôest un GPS pour nos documents.‚Äù
‚úî Montrer un vrai cas d‚Äôusage en live
‚úî Illustrer l‚Äô√©conomie de temps ‚Üí ‚Äú15 min √©conomis√©es par requ√™te = X heures gagn√©es par an.‚Äù
‚úî Souligner la s√©curit√© ‚Üí ‚ÄúDonn√©es enti√®rement internes.‚Äù
‚úî Storytelling ‚Üí ‚ÄúJean, nouveau technicien, cherche une proc√©dure‚Ä¶‚Äù

üìÑ Version imprimable √† laisser aux participants :

üó∫Ô∏è Sch√©ma simplifi√© de l‚Äôarchitecture

üó£Ô∏è T√©moignage utilisateur fictif

üìä ROI estim√© en temps et argent √©conomis√©



---

Cette version am√©liore la clart√© et l‚Äôimpact en ajoutant plus d‚Äôexemples concrets et visuels. Est-ce que √ßa correspond √† ce que tu voulais ?














import chromadb
import httpx
import streamlit as st

# ‚úÖ Connect to ChromaDB
CHROMA_DB_PATH = "chroma_db"
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
collection = chroma_client.get_or_create_collection(name="pdf_documents")

# ‚úÖ Azure OpenAI Configuration
AZURE_OPENAI_ENDPOINT = "YOUR_AZURE_OPENAI_ENDPOINT"
AZURE_API_KEY = "YOUR_AZURE_API_KEY"
AZURE_API_VERSION = "2023-12-01-preview"
HEADERS = {
    "Authorization": f"Bearer {AZURE_API_KEY}",
    "Content-Type": "application/json"
}

# ‚úÖ Retrieve Relevant Chunks from ChromaDB
def retrieve_relevant_chunks(query, top_k=3):
    results = collection.query(
        query_texts=[query],
        n_results=top_k,
        include=["documents", "metadatas"]
    )

    relevant_chunks = []
    for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
        relevant_chunks.append(f"{doc} (Source: {meta['source']})")

    return "\n".join(relevant_chunks)

# ‚úÖ Generate Conversational Response using Azure OpenAI
def generate_response(chat_history):
    prompt = "This is a conversation. Use the provided document context to answer accurately.\n\n"
    
    for msg in chat_history:
        role = "User" if msg["role"] == "user" else "Assistant"
        prompt += f"{role}: {msg['content']}\n"

    prompt += "Assistant:"

    payload = {
        "model": "gpt-4-turbo",
        "messages": [{"role": "system", "content": "You are an AI assistant that keeps track of conversations."},
                     {"role": "user", "content": prompt}]
    }

    try:
        response = httpx.post(f"{AZURE_OPENAI_ENDPOINT}/openai/deployments/gpt-4-turbo/chat/completions",
                              headers=HEADERS, json=payload, timeout=10)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"‚ùå Error: {str(e)}"


# ‚úÖ Streamlit Conversational Chatbot
st.title("üí¨ Conversational RAG Chatbot with ChromaDB & Azure OpenAI")

# ‚úÖ Initialize session state for chat history
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ‚úÖ User Input
query = st.chat_input("Type your message...")

if query:
    # ‚úÖ Retrieve relevant chunks
    relevant_chunks = retrieve_relevant_chunks(query)

    # ‚úÖ Add user message to chat history
    st.session_state.chat_history.append({"role": "user", "content": query})

    # ‚úÖ Generate response with full conversation history
    response = generate_response(st.session_state.chat_history)

    # ‚úÖ Add AI response to chat history
    st.session_state.chat_history.append({"role": "assistant", "content": response})

# ‚úÖ Display Chat History
for msg in st.session_state.chat_history:
    if msg["role"] == "user":
        st.chat_message("user").write(msg["content"])
    else:
        st.chat_message("assistant").write(msg["content"])








# ‚úÖ Streamlit Chatbot UI
st.title("ü§ñ RAG Chatbot with ChromaDB & Azure OpenAI")

query = st.text_input("üîç Enter your question:")

if query:
    st.subheader("ü§ñ AI Response:")
    response = rag_chatbot(query)
    st.write(response)





import chromadb

try:
    chroma_client = chromadb.PersistentClient(path="chroma_db")
    collection = chroma_client.get_or_create_collection(name="pdf_documents")
    print("‚úÖ ChromaDB connected successfully!")
except Exception as e:
    print("‚ùå ChromaDB connection failed:", str(e))





import streamlit as st
import chromadb
import httpx
from openai import AzureOpenAI
from langchain.embeddings import SentenceTransformerEmbeddings

# ‚úÖ 1Ô∏è‚É£ Initialize ChromaDB Connection
chroma_client = chromadb.PersistentClient(path="chroma_db")
collection = chroma_client.get_or_create_collection(name="pdf_documents")

# ‚úÖ 2Ô∏è‚É£ Load Embedding Model
model_path = "path/to/Sentence_model_transformer"
sentence_transformer_ef = SentenceTransformerEmbeddings(model_name=model_path)

# ‚úÖ 3Ô∏è‚É£ Function to Retrieve Relevant Chunks
def retrieve_relevant_chunks(query, top_k=3):
    """Retrieves the most relevant chunks from ChromaDB."""
    results = collection.query(
        query_texts=[query],
        n_results=top_k,
        include=["documents", "metadatas", "distances"]
    )

    retrieved_chunks = []
    for doc, meta, dist in zip(results["documents"][0], results["metadatas"][0], results["distances"][0]):
        retrieved_chunks.append({
            "document": doc,
            "metadata": meta,
            "distance": dist
        })

    return retrieved_chunks

# ‚úÖ 4Ô∏è‚É£ Function to Generate AI Response
def generate_response(query, context):
    """Generates AI response using retrieved context."""
    prompt = f"""
    {context}

    Query: {query}

    Please provide the sources, extracted pages, and procedure used.
    """

    client = AzureOpenAI(
        api_version="AZURE_ADAT_API_VERSION",
        azure_endpoint="APIGEE_ENDPOINT",
        api_key="FAKE_KEY",
        http_client=httpx.Client(verify=False)
    )

    with httpx.Client(verify=False) as http_client:
        completion = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are an AI assistant."},
                {"role": "user", "content": prompt},
            ]
        )

    return completion.choices[0].message.content

# ‚úÖ 5Ô∏è‚É£ Streamlit UI
st.title("üìÑ AI PDF Chatbot (ChromaDB + GPT-4o)")
st.write("Ask questions about your stored PDFs!")

query = st.text_input("üîç Enter your query:")
if st.button("Search"):
    if query.strip():
        # üîç Retrieve relevant chunks
        relevant_chunks = retrieve_relevant_chunks(query)

        if not relevant_chunks:
            st.warning("No relevant chunks found!")
        else:
            # üìå Display Retrieved Chunks
            st.subheader("üìñ Retrieved Chunks:")
            for res in relevant_chunks:
                st.markdown(f"**Source:** {res['metadata']['source']}")
                st.text_area("Chunk:", res["document"], height=100)

            # ü§ñ Generate AI Response
            context = "\n".join([res["document"] for res in relevant_chunks])
            response = generate_response(query, context)

            # üí¨ Display AI Response
            st.subheader("ü§ñ AI Response:")
            st.write(response)
    else:
        st.error("Please enter a query!")







import chromadb

# Connect to ChromaDB
chroma_client = chromadb.PersistentClient(path="chroma_db")
collection = chroma_client.get_collection(name="pdf_documents")

# Retrieve stored document count
stored_data = collection.get()
print(f"‚úÖ ChromaDB contains {len(stored_data['documents'])} stored chunks.")

# Preview some stored documents
if stored_data["documents"]:
    print("\nüîπ Sample stored document:\n", stored_data["documents"][:1])
else:
    print("‚ö†Ô∏è No stored data found. You may need to reprocess PDFs.")





import streamlit as st
import chromadb
from chromadb.utils import embedding_functions
from openai import AzureOpenAI
import httpx

# Connect to ChromaDB (No Need to Reprocess PDFs)
chroma_client = chromadb.PersistentClient(path="chroma_db")
collection = chroma_client.get_collection(name="pdf_documents")

# OpenAI API Client
client = AzureOpenAI(
    api_version="AZURE_ADAT_API_VERSION",
    azure_endpoint="APIGEE_ENDPOINT",
    api_key="FAKE_KEY",
    http_client=httpx.Client(verify=False)
)

# Retrieve Relevant Chunks
def retrieve_relevant_chunks(query, top_k=3):
    """Fetch relevant chunks from ChromaDB."""
    results = collection.query(query_texts=[query], n_results=top_k)
    return results["documents"][0] if results["documents"] else []

# Generate AI Response
def generate_response(query, context):
    """Generate AI response from retrieved chunks."""
    prompt = f"{context}\n\nQuery: {query}\n\nProvide sources & pages."

    with httpx.Client(verify=False) as http_client:
        completion = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are an AI assistant."},
                {"role": "user", "content": prompt},
            ]
        )
    
    return completion.choices[0].message.content

# ---- STREAMLIT APP ----
st.title("üìÑ AI-Powered PDF Search with ChromaDB")

query = st.text_input("üîç Enter your query:", "")

if st.button("Search"):
    if query:
        st.write("‚è≥ Searching in ChromaDB...")
        relevant_chunks = retrieve_relevant_chunks(query)

        if relevant_chunks:
            context = "\n".join(relevant_chunks)
            response = generate_response(query, context)

            st.subheader("ü§ñ AI Response:")
            st.write(response)

            st.subheader("üìö Retrieved Documents:")
            for i, chunk in enumerate(relevant_chunks):
                st.write(f"**Chunk {i+1}:** {chunk}")
        else:
            st.warning("No relevant documents found.")
    else:
        st.warning("Please enter a query.")





import streamlit as st
import chromadb
from chromadb.utils import embedding_functions
from openai import AzureOpenAI
import httpx

# Connect to ChromaDB (No Need to Reprocess PDFs)
chroma_client = chromadb.PersistentClient(path="chroma_db")
collection = chroma_client.get_collection(name="pdf_documents")

# OpenAI API Client
client = AzureOpenAI(
    api_version="AZURE_ADAT_API_VERSION",
    azure_endpoint="APIGEE_ENDPOINT",
    api_key="FAKE_KEY",
    http_client=httpx.Client(verify=False)
)

# Retrieve Relevant Chunks
def retrieve_relevant_chunks(query, top_k=3):
    """Fetch relevant chunks from ChromaDB."""
    results = collection.query(query_texts=[query], n_results=top_k)
    return results["documents"][0] if results["documents"] else []

# Generate AI Response
def generate_response(query, context):
    """Generate AI response from retrieved chunks."""
    prompt = f"{context}\n\nQuery: {query}\n\nProvide sources & pages."

    with httpx.Client(verify=False) as http_client:
        completion = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are an AI assistant."},
                {"role": "user", "content": prompt},
            ]
        )
    
    return completion.choices[0].message.content

# ---- STREAMLIT APP ----
st.title("üìÑ AI-Powered PDF Search with ChromaDB")

query = st.text_input("üîç Enter your query:", "")

if st.button("Search"):
    if query:
        st.write("‚è≥ Searching in ChromaDB...")
        relevant_chunks = retrieve_relevant_chunks(query)

        if relevant_chunks:
            context = "\n".join(relevant_chunks)
            response = generate_response(query, context)

            st.subheader("ü§ñ AI Response:")
            st.write(response)

            st.subheader("üìö Retrieved Documents:")
            for i, chunk in enumerate(relevant_chunks):
                st.write(f"**Chunk {i+1}:** {chunk}")
        else:
            st.warning("No relevant documents found.")
    else:
        st.warning("Please enter a query.")











import chromadb
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm

# Initialize ChromaDB client
chroma_client = chromadb.PersistentClient(path="chroma_db")

# Initialize embedding function
sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Create collection (if it doesn't exist)
collection = chroma_client.get_or_create_collection(
    name="pdf_documents", embedding_function=sentence_transformer_ef
)

# PDF Processing
def load_and_chunk_pdfs(pdf_directory):
    """Loads and chunks PDFs into smaller parts."""
    loader = PyPDFDirectoryLoader(pdf_directory)
    documents = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=200, length_function=len, add_start_index=True
    )
    
    return text_splitter.split_documents(documents)

# Store in ChromaDB (Only if new chunks exist)
def store_chunks_in_chroma(chunks):
    """Stores new chunks in ChromaDB while avoiding duplicates."""
    
    existing_sources = set()
    existing_metadata = collection.get(include=["metadatas"])["metadatas"]
    if existing_metadata:
        existing_sources = {meta["source"] for meta in existing_metadata}

    new_chunks = [chunk for chunk in chunks if chunk.metadata["source"] not in existing_sources]

    if not new_chunks:
        print("No new chunks to add.")
        return

    print(f"Storing {len(new_chunks)} new chunks in ChromaDB...")

    for i in tqdm(range(0, len(new_chunks), 5000), desc="Processing batches"):
        batch = new_chunks[i:i + 5000]

        metadatas = [{"source": chunk.metadata["source"]} for chunk in batch]
        texts = [chunk.page_content for chunk in batch]
        embeddings = sentence_transformer_ef(texts)

        ids = [f"{md['source']}_{i+j}" for j, md in enumerate(metadatas)]
        collection.add(documents=texts, embeddings=embeddings, metadatas=metadatas, ids=ids)

    print("‚úÖ Chunks successfully stored in ChromaDB!")

# ---- RUN ONCE ----
pdf_path = "/path/to/your/pdf/folder"
chunks = load_and_chunk_pdfs(pdf_path)
store_chunks_in_chroma(chunks)






if __name__ == "__main__":
    # Load chunks and stored IDs if available
    chunks = load_from_file(CHUNKS_FILE)
    stored_ids = load_from_file(STORED_IDS_FILE) or set()

    if chunks is None:
        print("Processing and saving chunks...")
        chunks = load_and_chunk_pdf(PDF_DIRECTORY)
        save_to_file(chunks, CHUNKS_FILE)

    if not stored_ids:  # If no stored IDs, store the chunks
        print("Storing chunks in ChromaDB for the first time...")
        store_chunks_in_chroma(chunks)
        stored_ids = set(collection.get()["ids"])  # Store IDs of inserted chunks
        save_to_file(stored_ids, STORED_IDS_FILE)
    else:
        print("Chunks already stored. Skipping ChromaDB insertion.")

    # Launch chatbot
    rag_chatbot()






def store_chunks_in_chroma(chunks: List[Document]):
    """Stores chunks in ChromaDB, avoiding duplicates using stored IDs."""

    # Load stored IDs from file
    stored_ids = load_from_file(STORED_IDS_FILE) or set()

    # Filter only new chunks
    new_chunks = [chunk for chunk in chunks if chunk.metadata["source"] not in stored_ids]

    if not new_chunks:
        print("No new chunks to add to ChromaDB.")
        return

    # Batch processing
    for i in tqdm(range(0, len(new_chunks), MAX_BATCH_SIZE), desc="Processing batches"):
        batch = new_chunks[i:i+MAX_BATCH_SIZE]
        
        metadatas = [{
            "source": chunk.metadata["source"],
            "page": chunk.metadata.get("page", 0),
            "start_index": chunk.metadata.get("start_index", 0)
        } for chunk in batch]
        
        texts = [chunk.page_content for chunk in batch]
        ids = [f"{meta['source']}_{i+j}" for j, meta in enumerate(metadatas)]
        
        collection.add(
            documents=texts,
            metadatas=metadatas,
            ids=ids
        )

    # Update and save stored IDs
    stored_ids.update([chunk.metadata["source"] for chunk in new_chunks])
    save_to_file(stored_ids, STORED_IDS_FILE)

    print(f"Stored {len(new_chunks)} new chunks in ChromaDB.")





import pickle
import os

CHUNKS_FILE = "chunks.pkl"
STORED_IDS_FILE = "stored_ids.pkl"

def save_to_file(data, file_path):
    """Generic function to save data to a pickle file."""
    with open(file_path, "wb") as f:
        pickle.dump(data, f)

def load_from_file(file_path):
    """Generic function to load data from a pickle file if it exists."""
    if os.path.exists(file_path):
        with open(file_path, "rb") as f:
            return pickle.load(f)
    return None









import pickle
import os

CHUNKS_FILE = "chunks.pkl"

def save_chunks_to_file(chunks):
    """Saves chunks to a pickle file."""
    with open(CHUNKS_FILE, "wb") as f:
        pickle.dump(chunks, f)

def load_chunks_from_file():
    """Loads chunks from a pickle file if it exists."""
    if os.path.exists(CHUNKS_FILE):
        with open(CHUNKS_FILE, "rb") as f:
            return pickle.load(f)
    return None



if __name__ == "__main__":
    # Try to load chunks from file
    chunks = load_chunks_from_file()

    if chunks is None:
        # If no saved chunks, process and save them
        chunks = load_and_chunk_pdf(PDF_DIRECTORY)
        save_chunks_to_file(chunks)
        store_chunks_in_chroma(chunks)
    else:
        print("Chunks loaded from file. Skipping reprocessing.")




import streamlit as st
import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from langchain_community.document_loaders import DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from openai import AzureOpenAI
import os
import time

# Configuration
PDF_PATH = "/mnt/LMMS/Procedures"
MAX_BATCH_SIZE = 100
AZURE_AOAI_API_VERSION = "2023-05-15"
APIGEE_ENDPOINT = "https://your-endpoint.openai.azure.com/"
model_path = "/path/to/your/sentence-transformers-model"

@st.cache_resource
def initialize_chroma():
    """Initialize ChromaDB and preprocess documents once"""
    client = chromadb.PersistentClient(path="chrome_db")
    sentence_transformer_ef = SentenceTransformerEmbeddingFunction(model_name=model_path)
    collection = client.get_or_create_collection(
        name="pdf_documents",
        embedding_function=sentence_transformer_ef
    )
    return collection

@st.cache_resource
def load_and_process_documents():
    """Load and process documents once at startup"""
    # Loading documents
    with st.spinner('Loading PDF documents...'):
        loader = DirectoryLoader(PDF_PATH, glob="**/*.pdf")
        documents = loader.load()
        if not documents:
            st.error("No PDF documents found in the specified directory!")
            st.stop()

    # Chunking documents
    with st.spinner('Processing documents...'):
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1260,
            chunk_overlap=100,
            add_start_index=True
        )
        chunks = text_splitter.split_documents(documents)
    
    # Store in ChromaDB
    with st.spinner('Indexing documents...'):
        collection = initialize_chroma()
        existing_sources = set(m["source"] for m in collection.get()["metadatas"])
        new_chunks = [chunk for chunk in chunks if chunk.metadata["source"] not in existing_sources]
        
        for i in range(0, len(new_chunks), MAX_BATCH_SIZE):
            batch = new_chunks[i:i+MAX_BATCH_SIZE]
            texts = [chunk.page_content for chunk in batch]
            metadatas = [{"source": chunk.metadata["source"], "page": chunk.metadata.get("page", 0)} 
                        for chunk in batch]
            embeddings = sentence_transformer_ef(texts)
            
            ids = [str(i + j) for j, _ in enumerate(batch)]
            collection.add(
                documents=texts,
                embeddings=embeddings,
                metadatas=metadatas,
                ids=ids
            )
    
    return collection

# Initialize app once
collection = load_and_process_documents()
client = AzureOpenAI(
    api_version=AZURE_AOAI_API_VERSION,
    azure_endpoint=APIGEE_ENDPOINT,
    api_key=os.getenv("AZURE_OPENAI_KEY"),
)

# Streamlit UI
st.title("PDF Document Assistant")
st.write("Ask questions about your PDF documents")

# Chat input
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Enter your question..."):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Display user message
    with st.chat_message("user"):
        st.markdown(prompt)

    # Retrieve relevant chunks
    with st.spinner('Searching documents...'):
        relevant_chunks = retrieve_relevant_chunks(prompt)
    
    # Generate response
    with st.spinner('Generating answer...'):
        response = generate_response(prompt, relevant_chunks)
    
    # Display assistant response
    with st.chat_message("assistant"):
        st.markdown(response)
    
    # Add assistant response to chat history
    st.session_state.messages.append({"role": "assistant", "content": response})

# (Include the retrieve_relevant_chunks and generate_response functions from previous implementation)


import chromadb
from chromadb.utils import embedding_functions
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from openai import AzureOpenAI
import httpx
import os

# Initialize ChromaDB client and collection
chroma_client = chromadb.PersistentClient(path="chroma_db")
sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="all-MiniLM-L6-v2"
)
collection = chroma_client.get_or_create_collection(
    name="pdf_documents",
    embedding_function=sentence_transformer_ef
)

MAX_BATCH_SIZE = 1000  # Batch size for Chroma inserts

# --- Query Generation Function ---
def generate_queries(original_query, num_queries=3):
    """Generate multiple queries using Azure OpenAI"""
    client = AzureOpenAI(
        api_version=os.getenv("AZURE_API_VERSION"),
        azure_endpoint=os.getenv("APIGEE_ENDPOINT"),
        api_key=os.getenv("IAŒöŒï_ŒïŒ£Œ•")
    )
    
    prompt = f"""Generate {num_queries} different search queries that are rephrased variations of:
    '{original_query}'. Return each query on a new line."""
    
    with httpx.Client(verify=False) as http_client:
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a query generation assistant."},
                {"role": "user", "content": prompt}
            ]
        )
    
    queries = completion.choices[0].message.content.strip().split('\n')
    return queries[:num_queries]

# --- Modified Retrieval Function with Multi-Query ---
def retrieve_relevant_chunks(query, top_k=3):
    """Retrieve chunks using multiple generated queries"""
    generated_queries = generate_queries(query)
    all_results = []

    for q in generated_queries:
        results = collection.query(
            query_texts=[q],
            n_results=top_k,
            include=["documents", "metadatas", "distances"]
        )
        # Process results for each query
        for doc, meta, dist in zip(results["documents"][0],
                                  results["metadatas"][0],
                                  results["distances"][0]):
            all_results.append({
                "document": doc,
                "metadata": meta,
                "distance": dist
            })
    
    # Deduplicate by document content and sort by relevance
    seen = set()
    unique_results = []
    for res in sorted(all_results, key=lambda x: x["distance"]):
        if res["document"] not in seen:
            seen.add(res["document"])
            unique_results.append(res)
    
    return unique_results[:top_k]

# --- Modified Response Generation with Source Attribution ---
def generate_response(query, context_chunks):
    """Generate answer with source attribution"""
    context_str = "\n\n".join([
        f"Source: {chunk['metadata']['source']} (Page {chunk['metadata'].get('page', '?')})\n"
        f"Content: {chunk['document']}"
        for chunk in context_chunks
    ])
    
    client = AzureOpenAI(
        api_version=os.getenv("AZURE_API_VERSION"),
        azure_endpoint=os.getenv("APIGEE_ENDPOINT"),
        api_key=os.getenv("IAŒöŒï_ŒïŒ£Œ•")
    )
    
    prompt = f"""Answer this query: {query}
    Using ONLY these sources:
    {context_str}
    
    - ALWAYS cite sources using [Source: filename.pdf, Page X]
    - If unsure, say "I couldn't find relevant information"
    - List ALL used sources at the end"""
    
    with httpx.Client(verify=False) as http_client:
        completion = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a technical assistant."},
                {"role": "user", "content": prompt}
            ]
        )
    
    return completion.choices[0].message.content

# --- Rest of your existing functions remain mostly unchanged ---
# (Keep your load_and_chunk_pdf, store_chunks_in_chroma, etc.)
# ... [Your existing PDF loading/chunking code here] ...

def rag_chatbot():
    while True:
        query = input("You: ")
        if query.lower() in ["exit", "quit"]:
            print("Goodbye!")
            break
        
        # Retrieve with multi-query
        context_chunks = retrieve_relevant_chunks(query)
        
        # Generate response with sources
        response = generate_response(query, context_chunks)
        print(f"Bot: {response}\n")

# --- Execution Flow ---
if __name__ == "__main__":
    pdf_path = "/mnt/DS/Procedures"
    chunks = load_and_chunk_pdf(pdf_path)
    store_chunks_in_chroma(chunks)
    rag_chatbot()







def rag_chatbot():
    print("PDF has been processed and stored in Chroma. You can now ask questions.")

    while True:
        query = input("You: ")
        
        if query.lower() in ["exit", "quit"]:
            print("Goodbye!")
            break

        # Retrieve relevant chunks
        relevant_chunks = retrieve_relevant_chunks(query)
        context = "\n".join(relevant_chunks)

        # Generate response using GPT-4
        response = generate_response(query, context)
        
        print(f"Bot: {response}")

# Load and chunk the PDF
pdf_path = "your_pdf.pdf"  # Replace with the actual PDF path
chunks = load_and_chunk_pdf(pdf_path)

# Store chunks in Chroma
store_chunks_in_chroma(chunks)

# Start chatbot
rag_chatbot()






import os
import chromadb
from chromadb.utils import embedding_functions
from langchain.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm

# Initialize Chroma client with persistent storage
chroma_client = chromadb.PersistentClient(path="chroma_db")

# Path to local model directory
model_path = "./models/all-MiniLM-L6-v2"  # Update this path

# Initialize embedding function with local model
sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name=model_path
)

# Create/load collection with metadata support
collection = chroma_client.get_or_create_collection(
    name="pdf_documents",
    embedding_function=sentence_transformer_ef,
    metadata={"hnsw:space": "cosine"}  # Optimize for similarity search
)

# Batch processing parameters
MAX_BATCH_SIZE = 10000  # Adjust based on your system's RAM

def load_and_split_pdfs(pdf_directory):
    print(f"Loading PDFs from {pdf_directory}...")
    loader = PyPDFDirectoryLoader(pdf_directory)
    documents = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        add_start_index=True
    )
    return text_splitter.split_documents(documents)

def store_chunks_in_chroma(chunks):
    # Check existing documents to avoid duplicates
    existing_sources = collection.get()["metadatas"]
    new_chunks = [chunk for chunk in chunks if chunk.metadata["source"] not in existing_sources]

    if not new_chunks:
        print("All documents already processed.")
        return

    print(f"Processing {len(new_chunks)} new chunks...")
    
    # Process in batches
    for i in tqdm(range(0, len(new_chunks), MAX_BATCH_SIZE), desc="Processing batches"):
        batch = new_chunks[i:i+MAX_BATCH_SIZE]
        
        # Extract metadata and texts
        metadatas = [{"source": chunk.metadata["source"]} for chunk in batch]
        texts = [chunk.page_content for chunk in batch]
        
        # Generate embeddings
        embeddings = sentence_transformer_ef(texts)
        
        # Create unique IDs
        ids = [f"{md['source']}_{i+j}" for j, md in enumerate(metadatas)]
        
        # Add to collection
        collection.add(
            documents=texts,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids
        )

def rag_chatbot(pdf_directory):
    # Load and split PDFs
    chunks = load_and_split_pdfs(pdf_directory)
    
    # Store chunks with batch processing
    store_chunks_in_chroma(chunks)
    
    print("Ready for queries. Type 'exit' to quit.")
    while True:
        query = input("\nYou: ")
        if query.lower() in ["exit", "quit"]:
            break
            
        # Retrieve context
        results = collection.query(
            query_texts=[query],
            n_results=5
        )
        context = "\n\n".join(results["documents"][0])
        
        # Generate response (replace with your LLM)
        print(f"\nBot: [Response based on: {context[:500]}...]")

if __name__ == "__main__":
    pdf_directory = input("Enter PDF directory path: ")
    if os.path.isdir(pdf_directory):
        rag_chatbot(pdf_directory)
    else:
        print("Invalid directory path!")














import os
import chromadb
from chromadb.utils import embedding_functions
from langchain.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm

# Initialize Chroma client with persistent storage
chroma_client = chromadb.PersistentClient(path="chroma_db")

# Path to your local SentenceTransformer model directory (e.g., "./models/all-MiniLM-L6-v2")
model_path = "./models/all-MiniLM-L6-v2"  # Replace with your model directory path

# Initialize embedding function with local model
sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name=model_path  # Use local model directory
)

# Create or load a collection in Chroma
collection = chroma_client.get_or_create_collection(
    name="pdf_documents",
    embedding_function=sentence_transformer_ef
)

# Function to load and split PDFs from a directory
def load_and_split_pdfs(pdf_directory):
    print(f"Loading PDFs from {pdf_directory}...")
    loader = PyPDFDirectoryLoader(pdf_directory)
    documents = loader.load()
    
    # Split documents into chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100
    )
    chunks = text_splitter.split_documents(documents)
    return chunks

# Function to store chunks in Chroma
def store_chunks_in_chroma(chunks):
    print("Storing chunks in ChromaDB...")
    texts = [chunk.page_content for chunk in chunks]
    metadatas = [{"source": chunk.metadata["source"]} for chunk in chunks]
    
    # Generate embeddings and store in Chroma
    embeddings = sentence_transformer_ef(texts)  # Uses local model
    
    collection.add(
        documents=texts,
        embeddings=embeddings,
        metadatas=metadatas,
        ids=[f"chunk_{i}" for i in range(len(texts))]
    )

# Function to retrieve relevant chunks
def retrieve_relevant_chunks(query, top_k=3):
    results = collection.query(
        query_texts=[query],
        n_results=top_k
    )
    return results["documents"][0]

# Main chatbot function
def rag_chatbot(pdf_directory):
    # Load and split PDFs
    chunks = load_and_split_pdfs(pdf_directory)
    
    # Store chunks in Chroma
    store_chunks_in_chroma(chunks)
    print("PDFs processed and stored. Ask questions now!")
    
    while True:
        query = input("You: ")
        if query.lower() in ["exit", "quit"]:
            print("Goodbye!")
            break
        
        # Retrieve relevant chunks
        context = "\n".join(retrieve_relevant_chunks(query))
        
        # Generate response (replace with your LLM, e.g., GPT-4)
        print(f"Bot: [Response based on context: {context[:100]}...]")

# Run the code
if __name__ == "__main__":
    pdf_directory = input("Enter PDF directory path: ")
    if not os.path.isdir(pdf_directory):
        print("Invalid directory!")
    else:
        rag_chatbot(pdf_directory)
























import os
import chromadb
from chromadb.utils import embedding_functions
import openai
from langchain.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm

# Set up OpenAI API key
openai.api_key = "your_openai_api_key"

# Initialize Chroma client with persistent storage
chroma_client = chromadb.PersistentClient(path="chroma_db")  # Data will be stored in the "chroma_db" folder

# Use OpenAI embeddings
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key=openai.api_key,
    model_name="text-embedding-ada-002"  # OpenAI's default embedding model
)

# Create or load a collection in Chroma
collection = chroma_client.get_or_create_collection(
    name="pdf_documents",
    embedding_function=openai_ef
)

# Function to load and split PDFs from a directory
def load_and_split_pdfs(pdf_directory):
    print(f"Loading PDFs from {pdf_directory}...")
    
    # Use PyPDFDirectoryLoader to load all PDFs in the directory
    loader = PyPDFDirectoryLoader(pdf_directory)
    documents = loader.load()
    
    # Split documents into smaller chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # Adjust based on your needs
        chunk_overlap=100  # Overlap to preserve context
    )
    chunks = text_splitter.split_documents(documents)
    
    return chunks

# Function to store chunks in Chroma
def store_chunks_in_chroma(chunks):
    print("Storing chunks in ChromaDB...")
    for i, chunk in enumerate(tqdm(chunks, desc="Storing chunks")):
        # Add each chunk to ChromaDB with a unique ID
        collection.add(
            documents=[chunk.page_content],
            ids=[f"chunk_{i}"]
        )

# Function to retrieve relevant chunks
def retrieve_relevant_chunks(query, top_k=3):
    results = collection.query(
        query_texts=[query],
        n_results=top_k
    )
    return results["documents"][0]

# Function to generate response using GPT-4
def generate_response(query, context):
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response['choices'][0]['message']['content']

# Main chatbot function
def rag_chatbot(pdf_directory):
    # Load and split PDFs
    chunks = load_and_split_pdfs(pdf_directory)
    
    # Store chunks in Chroma
    store_chunks_in_chroma(chunks)
    print("PDFs have been processed and stored in Chroma. You can now ask questions.")
    
    while True:
        query = input("You: ")
        if query.lower() in ["exit", "quit"]:
            print("Goodbye!")
            break
        
        # Retrieve relevant chunks
        relevant_chunks = retrieve_relevant_chunks(query)
        context = "\n".join(relevant_chunks)
        
        # Generate response using GPT-4
        response = generate_response(query, context)
        print(f"Bot: {response}")

# Example usage
if __name__ == "__main__":
    # Ask the user for the PDF directory path
    pdf_directory = input("Enter the path to the directory containing PDFs: ")
    
    # Validate the directory path
    if not os.path.isdir(pdf_directory):
        print("Invalid directory path. Please try again.")
    else:
        # Start the chatbot
        rag_chatbot(pdf_directory)


import os
import pdfplumber  # Faster than PyPDF2
import chromadb
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
import openai
from multiprocessing import Pool, cpu_count
from tqdm import tqdm

# Set up OpenAI API key
openai.api_key = "your_openai_api_key"

# Initialize Chroma client with persistent storage
chroma_client = chromadb.PersistentClient(path="chroma_db")  # Data will be stored in the "chroma_db" folder

# Create or load a collection in Chroma
collection = chroma_client.get_or_create_collection(
    name="pdf_documents",
    embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="all-MiniLM-L6-v2"
    )
)

# Initialize embedding model (e.g., SentenceTransformer)
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Function to extract text from a single page
def extract_text_from_page(page):
    return page.extract_text() or ""

# Function to process a PDF in parallel
def process_pdf(pdf_path, chunk_size=500):
    print(f"Processing {pdf_path}...")
    chunks = []
    
    with pdfplumber.open(pdf_path) as pdf:
        # Use multiprocessing to extract text from pages in parallel
        with Pool(cpu_count()) as pool:
            pages = pdf.pages
            page_texts = list(tqdm(pool.imap(extract_text_from_page, pages), total=len(pages), desc="Extracting text"))
        
        # Combine text and chunk it
        full_text = "".join(page_texts)
        chunks = [full_text[i:i + chunk_size] for i in range(0, len(full_text), chunk_size)]
    
    return chunks

# Function to store chunks in Chroma in batches
def store_chunks_in_chroma(chunks, pdf_name, batch_size=100):
    print("Generating embeddings and storing chunks...")
    for i in tqdm(range(0, len(chunks), batch_size), desc="Storing chunks"):
        batch_chunks = chunks[i:i + batch_size]
        embeddings = embedding_model.encode(batch_chunks)
        
        # Add chunks to ChromaDB with unique IDs
        collection.add(
            documents=batch_chunks,
            embeddings=embeddings.tolist(),
            ids=[f"{pdf_name}_chunk_{i + j}" for j in range(len(batch_chunks))]
        )

# Function to retrieve relevant chunks
def retrieve_relevant_chunks(query, top_k=3):
    query_embedding = embedding_model.encode([query])[0].tolist()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    return results["documents"][0]

# Function to generate response using GPT-4
def generate_response(query, context):
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response['choices'][0]['message']['content']

# Main chatbot function
def rag_chatbot(pdf_path):
    # Check if the PDF has already been processed
    pdf_name = os.path.basename(pdf_path)
    existing_ids = collection.get(where={"source": pdf_name})["ids"]
    
    if existing_ids:
        print(f"PDF '{pdf_name}' has already been processed. Skipping extraction.")
    else:
        # Process the PDF
        chunks = process_pdf(pdf_path)
        
        # Store chunks in Chroma with metadata
        store_chunks_in_chroma(chunks, pdf_name)
        print(f"PDF '{pdf_name}' has been processed and stored in Chroma.")
    
    print("You can now ask questions.")
    
    while True:
        query = input("You: ")
        if query.lower() in ["exit", "quit"]:
            print("Goodbye!")
            break
        
        # Retrieve relevant chunks
        relevant_chunks = retrieve_relevant_chunks(query)
        context = "\n".join(relevant_chunks)
        
        # Generate response using GPT-4
        response = generate_response(query, context)
        print(f"Bot: {response}")

# Example usage
if __name__ == "__main__":
    # Ask the user for the PDF file path
    pdf_path = input("Enter the path to the PDF file: ")
    
    # Validate the file path
    if not os.path.isfile(pdf_path):
        print("Invalid file path. Please try again.")
    else:
        # Start the chatbot
        rag_chatbot(pdf_path)













pip install PyPDF2 chromadb sentence-transformers openai


import os
import PyPDF2
import chromadb
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
import openai

# Set up OpenAI API key
openai.api_key = "your_openai_api_key"

# Initialize Chroma client
chroma_client = chromadb.Client()

# Create or load a collection in Chroma
collection = chroma_client.create_collection(name="pdf_documents")

# Initialize embedding model (e.g., SentenceTransformer)
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Function to load and chunk PDF
def load_and_chunk_pdf(pdf_path, chunk_size=500):
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        
        # Split text into chunks
        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
        return chunks

# Function to store chunks in Chroma
def store_chunks_in_chroma(chunks):
    embeddings = embedding_model.encode(chunks)
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        collection.add(
            documents=[chunk],
            embeddings=[embedding.tolist()],
            ids=[str(i)]
        )

# Function to retrieve relevant chunks
def retrieve_relevant_chunks(query, top_k=3):
    query_embedding = embedding_model.encode([query])[0].tolist()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    return results["documents"][0]

# Function to generate response using GPT-4
def generate_response(query, context):
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response['choices'][0]['message']['content']

# Main chatbot function
def rag_chatbot(pdf_path):
    # Load and chunk the PDF
    chunks = load_and_chunk_pdf(pdf_path)
    
    # Store chunks in Chroma
    store_chunks_in_chroma(chunks)
    
    print("PDF has been processed and stored in Chroma. You can now ask questions.")
    
    while True:
        query = input("You: ")
        if query.lower() in ["exit", "quit"]:
            print("Goodbye!")
            break
        
        # Retrieve relevant chunks
        relevant_chunks = retrieve_relevant_chunks(query)
        context = "\n".join(relevant_chunks)
        
        # Generate response using GPT-4
        response = generate_response(query, context)
        print(f"Bot: {response}")

# Example usage
pdf_path = "your_pdf_file.pdf"
rag_chatbot(pdf_path)





import os
import PyPDF2
import chromadb
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
import openai

# Set up OpenAI API key
openai.api_key = "your_openai_api_key"

# Initialize Chroma client
chroma_client = chromadb.Client()

# Create or load a collection in Chroma
collection = chroma_client.create_collection(name="pdf_documents")

# Initialize embedding model (e.g., SentenceTransformer)
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Function to load and chunk PDF
def load_and_chunk_pdf(pdf_path, chunk_size=500):
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        
        # Split text into chunks
        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
        return chunks

# Function to store chunks in Chroma
def store_chunks_in_chroma(chunks):
    embeddings = embedding_model.encode(chunks)
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        collection.add(
            documents=[chunk],
            embeddings=[embedding.tolist()],
            ids=[str(i)]
        )

# Function to retrieve relevant chunks
def retrieve_relevant_chunks(query, top_k=3):
    query_embedding = embedding_model.encode([query])[0].tolist()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    return results["documents"][0]

# Function to generate response using GPT-4
def generate_response(query, context):
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response['choices'][0]['message']['content']

# Main chatbot function
def rag_chatbot(pdf_path):
    # Load and chunk the PDF
    chunks = load_and_chunk_pdf(pdf_path)
    
    # Store chunks in Chroma
    store_chunks_in_chroma(chunks)
    
    print("PDF has been processed and stored in Chroma. You can now ask questions.")
    
    while True:
        query = input("You: ")
        if query.lower() in ["exit", "quit"]:
            print("Goodbye!")
            break
        
        # Retrieve relevant chunks
        relevant_chunks = retrieve_relevant_chunks(query)
        context = "\n".join(relevant_chunks)
        
        # Generate response using GPT-4
        response = generate_response(query, context)
        print(f"Bot: {response}")

# Example usage
pdf_path = "your_pdf_file.pdf"
rag_chatbot(pdf_path)





pip install transformers



from transformers import BertTokenizer

def chunk_text_with_bert(text, max_tokens=512):
    """
    Chunk text using BERT's tokenizer.
    """
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    tokens = tokenizer.encode(text)
    chunks = []

    for i in range(0, len(tokens), max_tokens):
        chunk_tokens = tokens[i:i + max_tokens]
        chunk_text = tokenizer.decode(chunk_tokens)
        chunks.append(chunk_text)

    return chunks






from transformers import BertTokenizer

def chunk_text_with_bert(text, max_tokens=512):
    """
    Chunk text using BERT's tokenizer.
    """
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    tokens = tokenizer.encode(text)
    chunks = []

    for i in range(0, len(tokens), max_tokens):
        chunk_tokens = tokens[i:i + max_tokens]
        chunk_text = tokenizer.decode(chunk_tokens)
        chunks.append(chunk_text)

    return chunks









from transformers import GPT2Tokenizer 


# Step 2: Chunk the extracted text using transformers

def chunk_text_with_transformers(text, max_tokens=500):
    """
    Chunk text into smaller parts using Hugging Face's tokenizers.
    :param text: The input text to chunk.
    :param max_tokens: Maximum number of tokens per chunk.
    :return: List of text chunks.
    """
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")  # Load GPT2 tokenizer
    tokens = tokenizer.encode(text)  # Tokenize the text
    chunks = []

    for i in range(0, len(tokens), max_tokens):
        chunk_tokens = tokens[i:i + max_tokens]
        chunk_text = tokenizer.decode(chunk_tokens)
        chunks.append(chunk_text)
    
    return chunk








pip install fitz python-docx nltk tiktoken sentence-transformers chromadb openai






# Import necessary libraries
import fitz  # PyMuPDF for PDF processing
from docx import Document  # For Word file processing
from nltk.tokenize import sent_tokenize  # For text chunking
import tiktoken  # For token-based chunking
from sentence_transformers import SentenceTransformer  # For text embeddings
import chromadb  # Vector database
import openai  # For GPT-based generation

# Step 1: Extract text, tables, and images from files

# Extract text and images from a PDF file
def extract_from_pdf(pdf_path):
    """
    Extract text and images from a PDF file.
    :param pdf_path: Path to the PDF file.
    :return: Extracted text and images as a list of bytes.
    """
    doc = fitz.open(pdf_path)
    text = ""
    images = []
    
    for page in doc:
        # Extract text from the page
        text += page.get_text()
        # Extract images from the page
        for img in page.get_images(full=True):
            xref = img[0]
            pix = fitz.Pixmap(doc, xref)
            if pix.n > 4:  # Convert CMYK to RGB
                pix = fitz.Pixmap(fitz.csRGB, pix)
            # Save image as PNG bytes
            images.append(pix.tobytes("png"))
    return text, images

# Extract text from a Word document
def extract_from_word(doc_path):
    """
    Extract text from a Word document.
    :param doc_path: Path to the Word document.
    :return: Extracted text as a string.
    """
    doc = Document(doc_path)
    # Concatenate all paragraph texts
    text = "\n".join([p.text for p in doc.paragraphs])
    return text

# Step 2: Chunk the extracted text

def chunk_text(text, max_tokens=500):
    """
    Chunk the extracted text into smaller parts for embedding.
    :param text: The input text to chunk.
    :param max_tokens: Maximum number of tokens per chunk.
    :return: List of text chunks.
    """
    tokenizer = tiktoken.get_encoding("cl100k_base")  # Tokenizer for counting tokens
    sentences = sent_tokenize(text)  # Split text into sentences
    chunks = []
    current_chunk = []
    current_length = 0

    for sentence in sentences:
        tokenized = tokenizer.encode(sentence)
        # If adding this sentence exceeds max tokens, finalize current chunk
        if current_length + len(tokenized) > max_tokens:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            current_length = 0
        current_chunk.append(sentence)
        current_length += len(tokenized)
    
    # Add the last chunk
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    return chunks

# Step 3: Store chunks in ChromaDB

# Initialize ChromaDB
client = chromadb.Client()  # Initialize the ChromaDB client
collection = client.get_or_create_collection("document_chunks")  # Create or get the collection

# Embed text chunks and store them in ChromaDB
def store_chunks_in_chroma(chunks):
    """
    Store text chunks into ChromaDB after embedding them.
    :param chunks: List of text chunks.
    """
    model = SentenceTransformer('all-MiniLM-L6-v2')  # Load the sentence transformer model
    embeddings = model.encode(chunks)  # Generate embeddings for each chunk
    for i, chunk in enumerate(chunks):
        # Add the chunk to the collection
        collection.add(
            documents=[chunk],
            metadatas=[{"chunk_id": i}],  # Optional metadata
            embeddings=[embeddings[i]]
        )

# Step 4: Perform RAG-based retrieval and response generation

# Retrieve the most relevant chunks and generate a GPT-based response
def retrieve_and_generate_response(query):
    """
    Retrieve relevant chunks from ChromaDB and generate a GPT response.
    :param query: User query.
    :return: Generated response from GPT.
    """
    model = SentenceTransformer('all-MiniLM-L6-v2')  # Load the embedding model
    query_embedding = model.encode([query])  # Embed the query
    # Search for the most relevant chunks in ChromaDB
    results = collection.query(
        query_embeddings=query_embedding,
        n_results=5  # Number of results to retrieve
    )
    
    # Combine retrieved chunks into a single text
    retrieved_text = " ".join([doc for doc in results['documents'][0]])
    
    # Generate a response using GPT
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are an AI assistant."},
            {"role": "user", "content": f"Answer based on the following:\n\n{retrieved_text}\n\nQuestion: {query}"}
        ]
    )
    return response['choices'][0]['message']['content']

# Step 5: Putting it all together

def process_file_and_respond(file_path, file_type, query):
    """
    Process the file, store its contents in ChromaDB, and generate a response.
    :param file_path: Path to the input file (PDF/Word).
    :param file_type: Type of the file ('pdf' or 'word').
    :param query: User query.
    :return: GPT-generated response.
    """
    if file_type == 'pdf':
        text, images = extract_from_pdf(file_path)  # Extract text and images from PDF
    elif file_type == 'word':
        text = extract_from_word(file_path)  # Extract text from Word document
        images = []  # Word file images are not handled here

    # Chunk the extracted text
    chunks = chunk_text(text)
    # Store the chunks in ChromaDB
    store_chunks_in_chroma(chunks)
    # Generate a response for the query
    return retrieve_and_generate_response(query)

# Example usage:
if __name__ == "__main__":
    # File path and type
    file_path = "sample.pdf"  # Replace with the actual file path
    file_type = "pdf"  # 'pdf' or 'word'
    user_query = "What is the content of the document about?"
    
    # Process the file and respond to the query
    response = process_file_and_respond(file_path, file_type, user_query)
    print("Response:", response)









import openai
import requests

# Set up your OpenAI API key
openai.api_key = "your_openai_api_key"

# Function to extract and process PDF content
def extract_and_process_pdf(pdf_file_path):
    # Upload the PDF to the Domino workspace (if applicable)
    # Use an API call or platform-specific method to upload the PDF

    # Open the PDF and extract its content (you can also extract the text before sending to GPT-4)
    with open(pdf_file_path, 'rb') as file:
        pdf_content = file.read()  # Simulate reading the PDF file content (you would process it before)

    # Construct the prompt to send to ChatGPT 4.0
    prompt = """
    I have uploaded a PDF document. Please extract all the text from it and return the content in a clean and readable format.
    If there are any tables, images, or special sections, please indicate them clearly.
    """

    # Send the request to ChatGPT 4.0 (you could also send raw text if PDF parsing is handled earlier)
    response = openai.Completion.create(
        engine="gpt-4",  # or 'gpt-4-domino' if that is available
        prompt=prompt,
        max_tokens=1000
    )

    # Print the response (extracted text)
    return response.choices[0].text.strip()

# Example usage
pdf_path = "path_to_your_pdf_file.pdf"
extracted_text = extract_and_process_pdf(pdf_path)
print(extracted_text)




import fitz  # PyMuPDF
import re
import os

# Function to extract text and images from each page in the PDF
def extract_text_and_images(pdf_path):
    doc = fitz.open(pdf_path)
    
    all_text = ""
    images = []
    
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        
        # Extract text from the page
        page_text = page.get_text("text")
        all_text += page_text  # Add text from this page to all_text
        
        # Extract images from the page
        image_list = page.get_images(full=True)
        for img_index, img in enumerate(image_list):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]  # Binary image data
            images.append(image_data)
    
    return all_text, images

# Function to identify tables in the text (basic pattern detection)
def identify_tables(text):
    # Example: Look for common table patterns like rows and columns
    rows = text.split("\n")
    table_data = []
    for row in rows:
        # Split each row by whitespace or tabs (based on PDF formatting)
        columns = re.split(r'\s{2,}', row.strip())  # Assuming columns are separated by at least two spaces
        if len(columns) > 1:  # Heuristic: rows with multiple columns might be tables
            table_data.append(columns)
    
    return table_data

# Function to process the entire PDF
def process_pdf(pdf_path, output_dir="output"):
    # Ensure output directory exists
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # Extract text and images from the PDF
    text, images = extract_text_and_images(pdf_path)
    
    # Identify tables in the extracted text
    table_data = identify_tables(text)
    
    # Save the identified tables to a file (optional)
    table_file = os.path.join(output_dir, "tables.txt")
    with open(table_file, "w") as f:
        for table in table_data:
            f.write("\t".join(table) + "\n")
    
    print(f"Extracted {len(table_data)} tables from the PDF.")
    
    # Optionally save the images to separate files
    for i, image_data in enumerate(images):
        image_file = os.path.join(output_dir, f"image_{i + 1}.png")
        with open(image_file, "wb") as img_f:
            img_f.write(image_data)
    
    print(f"Extracted {len(images)} images from the PDF and saved them.")

# Example usage
pdf_path = "your_pdf_file.pdf"  # Replace with your PDF file path
process_pdf(pdf_path)
