import weaviate

client = weaviate.Client(
    embedded_options=weaviate.embedded.EmbeddedOptions()
)



from sentence_transformers import SentenceTransformer
import weaviate

# Load BGE-M3 model
bge_model = SentenceTransformer("BAAI/bge-m3")

# Create Weaviate client
client = weaviate.Client("embedded")

WEAVIATE_CLASS = "BankDocChunk"

# Clean previous class if needed
if client.schema.exists(WEAVIATE_CLASS):
    client.schema.delete_class(WEAVIATE_CLASS)

# Create schema
client.schema.create_class({
    "class": WEAVIATE_CLASS,
    "vectorizer": "none",  # because we provide vectors ourselves
    "properties": [
        {"name": "text", "dataType": ["text"]},
        {"name": "source_file", "dataType": ["string"]},
        {"name": "source_type", "dataType": ["string"]},
        {"name": "page", "dataType": ["int"]},
    ],
})

# Now embed your documents manually
for doc in chunks:  # Assuming 'chunks' is a list of Document objects
    text = doc.page_content
    metadata = doc.metadata

    # Create the embedding vector
    vector = bge_model.encode(text, normalize_embeddings=True)

    # Prepare data object
    data_object = {
        "text": text,
        "source_file": metadata.get("source_file", ""),
        "source_type": metadata.get("source_type", ""),
        "page": metadata.get("page", -1),
    }

    # Insert into Weaviate
    client.data_object.create(
        data_object=data_object,
        class_name=WEAVIATE_CLASS,
        vector=vector.tolist()
    )

print(f"Inserted {len(chunks)} vector chunks successfully!")










from sentence_transformers import SentenceTransformer
from langchain_community.embeddings import HuggingFaceEmbeddings

# Load BGE-M3 model
bge_model = SentenceTransformer("BAAI/bge-m3")

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model=bge_model,
    encode_kwargs={"normalize_embeddings": True}
)




with open("all_documents.txt", "w", encoding="utf-8") as f: f.write("\n\n".join(doc.page_content for doc in docs))



with open("all_documents.txt", "w", encoding="utf-8") as f: f.write("\n\n".join(doc.page_content for doc in docs))









from langchain_community.document_loaders import PyPDFDirectoryLoader, CSVLoader, UnstructuredExcelLoader
from langchain.schema import Document
import glob
import os
import re
import string

DOCS_DIR = "/mnt/LLMS/Procedures"  # Your corrected path ✅

# Text cleaning function
def clean_text(text, lowercase=False):
    """Cleans raw text: removes unwanted characters, extra spaces, fixes multiple dots"""
    
    # Remove non-printable characters
    text = ''.join(filter(lambda x: x in string.printable, text))
    
    # Replace multiple consecutive dots (...) with a single dot (.)
    text = re.sub(r'\.{2,}', '.', text)
    
    # Replace multiple line breaks and spaces
    text = re.sub(r'\n+', '\n', text)  # collapse multiple newlines
    text = re.sub(r'[ \t]+', ' ', text)  # collapse multiple spaces

    # Remove leading/trailing spaces on each line
    text = "\n".join(line.strip() for line in text.splitlines())
    
    # Remove empty lines
    text = "\n".join([line for line in text.splitlines() if line.strip() != ""])
    
    if lowercase:
        text = text.lower()
    
    return text

# Apply cleaning to list of documents
def clean_documents(documents, lowercase=False):
    cleaned_docs = []
    for doc in documents:
        cleaned_text = clean_text(doc.page_content, lowercase=lowercase)
        cleaned_doc = Document(page_content=cleaned_text, metadata=doc.metadata)
        cleaned_docs.append(cleaned_doc)
    return cleaned_docs

# Loader function
def load_documents():
    documents = []
    
    # PDFs
    if os.path.exists(os.path.join(DOCS_DIR, "pdfs")):
        for file in glob.glob(os.path.join(DOCS_DIR, "pdfs", "*.pdf")):
            try:
                loader = PyPDFDirectoryLoader(os.path.dirname(file))
                docs = loader.load()
                file_name = os.path.basename(file)
                
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "pdf"
                
                documents.extend(docs)
                print(f"Loaded {file_name}")
            except Exception as e:
                print(f"Error loading {file}: {e}")

    # CSVs
    csv_dir = os.path.join(DOCS_DIR, "csv")
    if os.path.exists(csv_dir):
        for file in glob.glob(os.path.join(csv_dir, '*.csv')):
            try:
                docs = CSVLoader(file_path=file).load()
                file_name = os.path.basename(file)
                
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "csv"
                    
                documents.extend(docs)
                print(f"Loaded {file_name}")
            except Exception as e:
                print(f"Error loading {file}: {e}")

    # Excels
    xls_dir = os.path.join(DOCS_DIR, "excel")
    if os.path.exists(xls_dir):
        for file in glob.glob(os.path.join(xls_dir, '*.xls*')):
            try:
                docs = UnstructuredExcelLoader(file_path=file).load()
                file_name = os.path.basename(file)
                
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "excel"
                    
                documents.extend(docs)
                print(f"Loaded {file_name}")
            except Exception as e:
                print(f"Error loading {file}: {e}")

    print(f"Loaded {len(documents)} documents in total")
    
    # ✨ Clean the documents immediately after loading
    cleaned_documents = clean_documents(documents)
    
    print(f"Cleaned {len(cleaned_documents)} documents.")
    return cleaned_documents

# Call the function
docs = load_documents()
