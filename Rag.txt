def split_documents_semantic(documents):
    """Hybrid semantic+size-based splitting"""
    model = SentenceTransformer("BAAI/bge-m3", device="cuda" if torch.cuda.is_available() else "cpu")
    
    # 1. Semantic chunking
    semantic_splitter = SemanticChunker(
        embeddings=model,
        breakpoint_threshold=0.82,  # More splits
        add_start_index=True
    )
    semi_chunks = semantic_splitter.split_documents(documents)
    
    # 2. Size enforcement
    size_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1200,
        chunk_overlap=200,
        separators=["\n\n## ", "\n\n", ". ", "; "]
    )
    
    final_chunks = size_splitter.split_documents(semi_chunks)
    
    # 3. Validate chunks
    avg_size = sum(len(c.page_content) for c in final_chunks)/len(final_chunks)
    print(f"Average chunk size: {avg_size:.0f} chars")
    
    return final_chunks
