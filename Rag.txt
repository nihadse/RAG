import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from FlagEmbedding import FlagModel

# Initialize Chroma client
chroma_client = chromadb.PersistentClient(path="chroma_db")

# Initialize BGE-M3 model
model_path = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"
model = FlagModel(
    model_path,
    query_instruction_for_retrieval="Generate representation for this sentence to retrieve relevant articles:",
    use_fp16=True  # Enable if using GPU
)

# Create custom embedding function
def bge_m3_embedding_function(texts):
    return model.encode(texts).tolist()

# Create collection with custom embedding function
collection = chroma_client.get_or_create_collection(
    name="pdf_documents3",
    embedding_function=bge_m3_embedding_function
)

# Query the collection
results = collection.query(
    query_texts=["example query"],
    n_results=20,
    include=["documents"]
)

print(results)





import streamlit as st
from your_rag_functions import (  # Replace with your actual imports
    generate_alternative_queries,
    generate_response,
    generate_answer_explanation,
    evaluate_rag_response,
    extract_source_files
)

# Initialize session state for chat history
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "current_chat" not in st.session_state:
    st.session_state.current_chat = []

# Initialize chat sessions
if "chat_sessions" not in st.session_state:
    st.session_state.chat_sessions = {"New Chat": []}

def rag_chatbot(query):
    # Your existing RAG processing logic
    all_queries = [query] + generate_alternative_queries(query)
    all_results = []

    for q in all_queries:
        context = retriever.get_relevant_documents(q)
        answer = generate_response(q, context)
        all_results.append({
            "query": q,
            "answer": answer,
            "context": context,
            "num_sources": len(context)
        })

    best_result = max(all_results, key=lambda x: x["num_sources"])
    explanation = generate_answer_explanation(best_result["query"], best_result["answer"], best_result["context"])
    evaluation = evaluate_rag_response(best_result["query"], best_result["answer"], best_result["context"])
    sources = extract_source_files(best_result["context"])

    # Update history
    retriever.update_history(best_result["query"], best_result["answer"])
    retriever.save_history()

    return {
        "original_query": query,
        "alternative_queries": all_queries[1:],
        "final_query": best_result["query"],
        "answer": best_result["answer"],
        "explanation": explanation,
        "evaluation": evaluation,
        "sources": sources
    }

# Sidebar for chat sessions
with st.sidebar:
    st.header("Chat Sessions")
    
    # Create new chat
    if st.button("+ New Chat"):
        new_chat_name = f"Chat {len(st.session_state.chat_sessions) + 1}"
        st.session_state.chat_sessions[new_chat_name] = []
        st.session_state.current_chat = new_chat_name
    
    # Display existing chats
    selected_chat = st.radio(
        "Select Chat",
        options=list(st.session_state.chat_sessions.keys()),
        index=0
    )
    st.session_state.current_chat = selected_chat

# Main chat interface
st.title("RAG Chatbot")

# Display chat history
chat_container = st.container()
with chat_container:
    for message in st.session_state.chat_sessions[st.session_state.current_chat]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if message["role"] == "assistant":
                with st.expander("Details"):
                    st.write("**Sources:**", message["sources"])
                    st.write("**Evaluation:**", message["evaluation"])

# Input area
user_input = st.chat_input("Type your message here...")
if user_input:
    # Add user message to chat history
    st.session_state.chat_sessions[st.session_state.current_chat].append({
        "role": "user",
        "content": user_input
    })
    
    # Process query
    with st.spinner("Thinking..."):
        response = rag_chatbot(user_input)
    
    # Add assistant response to chat history
    st.session_state.chat_sessions[st.session_state.current_chat].append({
        "role": "assistant",
        "content": response["answer"],
        "sources": response["sources"],
        "evaluation": response["evaluation"]
    })
    
    # Rerun to update the chat display
    st.rerun()
