https://docs.google.com/spreadsheets/d/1wJJrMHGs2WjRZbh0odWm8jSbkQx_dhPkHW-uflAuxYg/edit?fbclid=IwY2xjawJmUZlleHRuA2FlbQIxMAABHuilyz8eRjkA5x-fRt0uQ_DX3utPYvikm1SRco72eyCDIb_ESvY6QbFk0mnr_aem_ob096nmUzHLPC2tKpC_X2w&gid=0#gid=0




import chromadb
import torch
from typing import List, Dict, Optional
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from accelerate import Accelerator

class AutoMergeRetriever:
    def __init__(self, collection_name: str, model_path: str, persist_dir: str = "chroma_db"):
        """
        Initialize retriever with error handling for all components
        
        Args:
            collection_name: Name of ChromaDB collection
            model_path: Path to SentenceTransformer model
            persist_dir: Directory for ChromaDB persistence
        """
        # Initialize Accelerator for multi-GPU
        self.accelerator = Accelerator()
        self.device = self.accelerator.device
        
        try:
            # Initialize ChromaDB client with explicit settings
            self.client = chromadb.PersistentClient(
                path=persist_dir,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=False,
                    is_persistent=True
                )
            )
            
            # Verify collection exists
            if collection_name not in [col.name for col in self.client.list_collections()]:
                raise ValueError(f"Collection '{collection_name}' not found. Available collections: {[col.name for col in self.client.list_collections()]}")
                
            self.collection = self.client.get_collection(collection_name)
            
            # Initialize model with GPU support
            self.model = SentenceTransformer(model_path)
            self.model = self.model.half().to(self.device)
            self.model = self.accelerator.prepare(self.model)
            
            # Warmup GPU
            with torch.no_grad():
                _ = self.model.encode(["warmup"], convert_to_tensor=True)
                
            print(f"Retriever initialized successfully on {self.device}")
            
        except Exception as e:
            raise RuntimeError(f"Initialization failed: {str(e)}")

    def retrieve(
        self,
        query: str,
        exact_match_field: Optional[str] = None,
        exact_match_value: Optional[str] = None,
        lang: Optional[str] = None,
        top_k: int = 5,
        boost_exact: float = 2.0,
        boost_contains: float = 1.5
    ) -> List[Dict]:
        """
        Robust retrieval with auto-merging and error handling
        
        Args:
            query: Search query
            exact_match_field: Metadata field for exact matching
            exact_match_value: Value to match exactly
            lang: Language filter (e.g., 'fr', 'en')
            top_k: Number of results
            boost_exact: Score multiplier for exact matches
            boost_contains: Score multiplier for text contains
        """
        try:
            # Build query filters
            where_filter = {}
            if exact_match_field and exact_match_value:
                where_filter[exact_match_field] = {"$eq": exact_match_value}
            if lang:
                where_filter["language"] = {"$eq": lang}
                
            # Stage 1: Exact matches
            exact_results = []
            if exact_match_field and exact_match_value:
                exact_results = self.collection.get(
                    where=where_filter,
                    include=["documents", "metadatas"]
                )
            
            # Stage 2: Semantic search
            semantic_results = self.collection.query(
                query_texts=[query],
                n_results=top_k * 3,
                where=where_filter if where_filter else None,
                include=["documents", "metadatas", "distances"]
            )
            
            # Merge results with GPU acceleration
            with torch.no_grad():
                # Process exact matches
                merged = []
                if exact_results.get("ids"):
                    for doc, meta in zip(exact_results["documents"], exact_results["metadatas"]):
                        merged.append({
                            "text": doc,
                            "metadata": meta,
                            "score": 1.0 * boost_exact,
                            "match_type": "exact"
                        })
                
                # Process semantic matches
                query_embedding = self.model.encode([query], convert_to_tensor=True)
                doc_embeddings = self.model.encode(
                    semantic_results["documents"][0],
                    batch_size=512,
                    convert_to_tensor=True
                )
                
                scores = torch.nn.functional.cosine_similarity(
                    query_embedding,
                    doc_embeddings
                )
                
                for idx, (doc, meta) in enumerate(zip(
                    semantic_results["documents"][0],
                    semantic_results["metadatas"][0]
                )):
                    score = scores[idx].item()
                    
                    # Apply boosts
                    if exact_match_value and exact_match_value in doc:
                        score *= boost_contains
                    if (exact_match_field and exact_match_value and 
                        meta.get(exact_match_field) == exact_match_value):
                        score *= boost_exact
                        
                    merged.append({
                        "text": doc,
                        "metadata": meta,
                        "score": score,
                        "match_type": "semantic"
                    })
            
            # Deduplicate and sort
            seen = set()
            final_results = []
            for res in sorted(merged, key=lambda x: x["score"], reverse=True):
                doc_hash = hash(res["text"][:512])  # First 512 chars as hash
                if doc_hash not in seen:
                    seen.add(doc_hash)
                    final_results.append(res)
                    if len(final_results) >= top_k:
                        break
            
            return final_results[:top_k]
            
        except Exception as e:
            self.accelerator.print(f"Retrieval error: {str(e)}")
            return []

# Usage Example
if __name__ == "__main__":
    try:
        # Initialize with your settings
        retriever = AutoMergeRetriever(
            collection_name="pdf_documents3",
            model_path="/domino/edv/modelhub/Model Hub-model-huggingface-BAAI/bge-m3/main"
        )
        
        # French document search
        french_results = retriever.retrieve(
            query="Où puis-je trouver DW016?",
            exact_match_field="doc_id",
            exact_match_value="DW016",
            lang="fr",
            boost_exact=3.0
        )
        
        # English fallback
        english_results = retriever.retrieve(
            query="Technical specifications",
            top_k=3
        )
        
        print("French Results:", [r["metadata"] for r in french_results])
        print("English Results:", [r["text"][:100] for r in english_results])
        
    except Exception as e:
        print(f"Fatal initialization error: {str(e)}")










# Initialize once (typically at service startup)
retriever = AutoMergeRetriever(
    collection_name="your_collection_name",  # Your ChromaDB collection
    model_path="BAAI/bge-m3"  # or your local path
)






from typing import List, Dict, Optional
import chromadb
from sentence_transformers import SentenceTransformer
import torch

class AutoMergeRetriever:
    def __init__(self, collection_name: str, model_path: str):
        self.client = chromadb.PersistentClient(path="chroma_db")
        self.collection = self.client.get_collection(collection_name)
        self.model = SentenceTransformer(model_path).half().cuda()  # GPU acceleration

    def retrieve(
        self,
        query: str,
        exact_match_field: Optional[str] = None,
        exact_match_value: Optional[str] = None,
        top_k: int = 5,
        boost_exact: float = 2.0,
        boost_contains: float = 1.5
    ) -> List[Dict]:
        """
        Auto-merging retrieval with exact match prioritization
        
        Args:
            query: Search query
            exact_match_field: Metadata field for exact matching (e.g., "doc_id")
            exact_match_value: Value to exactly match (e.g., "DW016")
            top_k: Number of results to return
            boost_exact: Score multiplier for exact matches
            boost_contains: Score multiplier for text containing match value
        """
        # Stage 1: Get exact matches if specified
        exact_results = []
        if exact_match_field and exact_match_value:
            exact_results = self.collection.get(
                where={exact_match_field: {"$eq": exact_match_value}},
                include=["documents", "metadatas"]
            )
        
        # Stage 2: Semantic search
        semantic_results = self.collection.query(
            query_texts=[query],
            n_results=top_k*3,  # Over-fetch for merging
            include=["documents", "metadatas", "distances"]
        )
        
        # Merge and re-score results
        merged = []
        
        # Add exact matches with boosted scores
        if exact_results.get("ids"):
            for doc, meta in zip(exact_results["documents"], exact_results["metadatas"]):
                merged.append({
                    "text": doc,
                    "metadata": meta,
                    "score": 1.0 * boost_exact,
                    "match_type": "exact"
                })
        
        # Add semantic results with conditional boosting
        for doc, meta, dist in zip(semantic_results["documents"][0],
                                 semantic_results["metadatas"][0],
                                 semantic_results["distances"][0]):
            score = 1 - dist  # Convert distance to similarity
            
            # Apply boosts
            if exact_match_value and exact_match_value in doc:
                score *= boost_contains
            if (exact_match_field and exact_match_value and 
                meta.get(exact_match_field) == exact_match_value):
                score *= boost_exact
                
            merged.append({
                "text": doc,
                "metadata": meta,
                "score": score,
                "match_type": "semantic"
            })
        
        # De-duplicate and sort
        seen = set()
        final_results = []
        for res in sorted(merged, key=lambda x: x["score"], reverse=True):
            doc_hash = hash(res["text"][:512])  # First 512 chars as hash
            if doc_hash not in seen:
                seen.add(doc_hash)
                final_results.append(res)
                if len(final_results) >= top_k:
                    break
        
        return final_results[:top_k]











def retrieve_relevant_chunks(query, top_k=35):
    """GPU-accelerated retrieval with proper error handling"""
    query_embedding = model.encode([query], convert_to_tensor=True)
    query_embedding = accelerator.gather(query_embedding).cpu().numpy().tolist()
    
    results = collection.query(
        query_embeddings=query_embedding,
        n_results=top_k,
        include=["documents", "metadatas", "distances"]
    )
    
    return [
        {"document": doc, "metadata": meta, "distance": dist}
        for doc, meta, dist in zip(results["documents"][0],
                                  results["metadatas"][0],
                                  results["distances"][0])
    ]




def retrieve_relevant_chunks(query: str, top_k: int = 35) -> list:
    """Retrieve relevant document chunks using multiple generated queries.
    
    Args:
        query: Input query in French
        top_k: Number of results to return after deduplication
    
    Returns:
        List of unique results sorted by relevance
    """
    from accelerate import Accelerator
    import torch

    # Initialize accelerator for multi-GPU consistency
    accelerator = Accelerator()
    
    # Generate multiple query variations
    generated_queries = generate_queries(query)
    
    all_results = []
    
    # Process each query variation
    for q in generated_queries:
        try:
            # Query using accelerator-aware collection
            results = collection.query(
                query_texts=[q],
                n_results=top_k * 2,  # Over-fetch to account for deduplication
                include=["documents", "metadatas", "distances"]
            )
            
            # Process results with GPU-aware tensors
            with torch.no_grad():
                for doc, meta, dist in zip(results["documents"][0],
                                         results["metadatas"][0],
                                         results["distances"][0]):
                    all_results.append({
                        "document": doc,
                        "metadata": meta,
                        "distance": accelerator.gather(torch.tensor(dist, device=device))
                    })
        
        except Exception as e:
            print(f"Error processing query '{q}': {str(e)}")
            continue

    # Deduplicate and sort using accelerator-aware operations
    seen_documents = set()
    unique_results = []
    
    # Sort by distance (lower is better)
    for result in sorted(all_results, key=lambda x: x["distance"].item()):
        doc_content = result["document"]
        if doc_content not in seen_documents:
            seen_documents.add(doc_content)
            
            # Convert tensor to Python native type for serialization
            unique_results.append({
                "document": doc_content,
                "metadata": result["metadata"],
                "score": 1 - result["distance"].cpu().item()  # Convert to similarity score
            })
    
    # Free GPU memory
    torch.cuda.empty_cache()
    
    return unique_results[:top_k]


import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from accelerate import Accelerator
import torch

# 1. Initialize Accelerator for multi-GPU
accelerator = Accelerator()
device = accelerator.device

# 2. Load BGE-M3 model with GPU acceleration
model_path = '/domino/edv/modelhub/Model Hub-model-huggingface-BAAI/bge-m3/main'
model = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name=model_path,
    device=device,
    normalize_embeddings=True
).model

# Prepare model for multi-GPU and FP16
model = model.half().to(device)
model = accelerator.prepare(model)

# 3. Custom Embedding Function with Chroma 0.4.x+ Compatibility
class AcceleratedBGEEmbeddingFunction:
    def __init__(self, model, accelerator):
        self.model = model
        self.accelerator = accelerator
        
    def __call__(self, input):  # <-- Chroma expects parameter named "input"
        with torch.no_grad():
            # Convert input to device and process
            inputs = self.model.tokenize(input)
            inputs = {k: v.to(self.accelerator.device) for k, v in inputs.items()}
            
            # Forward pass with multi-GPU support
            outputs = self.model.model(**inputs)
            
            # Gather embeddings from all GPUs
            embeddings = self.accelerator.gather(outputs.last_hidden_state[:, 0])
            
            # Convert to numpy and normalize
            embeddings = embeddings.cpu().numpy()
            return embeddings.tolist()  # Chroma expects List[List[float]]

# 4. Initialize Chroma Client
chroma_client = chromadb.PersistentClient(
    path="chroma_db",
    settings=Settings(allow_reset=True, anonymized_telemetry=False)
)

# 5. Create/Get Collection with Correct Configuration
collection = chroma_client.get_or_create_collection(
    name="pdf_documents3",
    metadata={
        "hnsw:space": "cosine",
        "hnsw:dimension": 1024  # Must match bge-m3's output size
    },
    embedding_function=AcceleratedBGEEmbeddingFunction(model, accelerator)
)

# Usage example
collection.add(
    documents=["Document en français...", "Autre document..."],
    ids=["1", "2"],
    metadatas=[{"source": "doc1"}, {"source": "doc2"}]
)
\



import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from accelerate import Accelerator
import torch

# Initialize Accelerator for multi-GPU
accelerator = Accelerator()
device = accelerator.device

# 1. Load BGE-M3 model with GPU acceleration
model_path = '/domino/edv/modelhub/Model Hub-model-huggingface-BAAI/bge-m3/main'
model = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name=model_path,
    device=device,
    normalize_embeddings=True
).model

# Prepare model for multi-GPU and FP16
model = model.half().to(device)
model = accelerator.prepare(model)

# 2. Custom Embedding Function with Accelerator
class AcceleratedBGEEmbeddingFunction:
    def __init__(self, model, accelerator):
        self.model = model
        self.accelerator = accelerator
        
    def __call__(self, texts):
        with torch.no_grad():
            inputs = self.model.tokenize(texts)
            inputs = {k:v.to(self.accelerator.device) for k,v in inputs.items()}
            outputs = self.model.model(**inputs)
            embeddings = self.accelerator.gather(outputs.last_hidden_state[:, 0])
            return embeddings.cpu().numpy()

# 3. Initialize Chroma with Accelerated Embeddings
chroma_client = chromadb.PersistentClient(
    path="chroma_db",
    settings=Settings(allow_reset=True, anonymized_telemetry=False)
    
# 4. Create/Get Collection with Proper Configuration
collection = chroma_client.get_or_create_collection(
    name="pdf_documents3",
    metadata={
        "hnsw:space": "cosine",
        "hnsw:dimension": 1024  # Must match bge-m3 output
    },
    embedding_function=AcceleratedBGEEmbeddingFunction(model, accelerator)
)

# 5. Usage Example
documents = ["Document français...", "Another French doc..."]
collection.add(
    documents=documents,
    ids=["1", "2"],
    metadatas=[{"source": "doc1"}, {"source": "doc2"}]
)

# Query with GPU acceleration
results = collection.query(
    query_texts=["Recherche de documents techniques"],
    n_results=2
)








pip install sentence-transformers accelerate torch>=2.0.0


import torch
from sentence_transformers import SentenceTransformer
from accelerate import Accelerator

# Initialize accelerator (auto-distributes across GPUs)
accelerator = Accelerator()
device = accelerator.device

# Load model with FP16 and distribute across GPUs
model = SentenceTransformer("BAAI/bge-m3", device=device)
model = model.half()  # FP16 for speed
model = accelerator.prepare(model)  # Distribute across GPUs

# Example: 1 million documents (adjust to your data)
texts = ["doc1", "doc2", ..., "doc1000000"]

# Batch size per GPU (adjust based on H100 memory)
# H100 has 80GB VRAM → 12 GPUs → ~6.6GB/GPU → batch_size=256 is safe
batch_size = 256 * accelerator.num_processes  # Total batches across GPUs

# Parallel embedding generation
embeddings = model.encode(
    texts,
    batch_size=batch_size,
    convert_to_tensor=True,  # Keep tensors on GPU
    show_progress_bar=True,
    normalize_embeddings=True,
    device=device,
)

# Gather embeddings from all GPUs (if needed)
embeddings = accelerator.gather(embeddings)















def generate_response(query, context):
    # Extract unique sources with page numbers
    sources = {}
    for chunk in context:
        source = chunk["metadata"]["source"]
        page = chunk["metadata"].get("page", "N/A")  # Handle missing page numbers
        if source not in sources:
            sources[source] = set()
        sources[source].add(page)
    
    # Format sources string
    sources_str = "\n".join([
        f"- [Source: {source}, Pages: {', '.join(sorted(pages))}]"
        for source, pages in sources.items()
    ])

    # Improved French prompt with strict formatting
    prompt = f"""Vous êtes un assistant spécialisé en architecture bancaire. Répondez en français avec:
1. Une réponse précise basée exclusivement sur les documents fournis
2. La procédure d'extraction des informations
3. Les références exactes des documents utilisés

Documents disponibles:
{context}

Question: {query}

Exigez le format de réponse suivant:
[Procédure]
1. J'ai identifié les informations pertinentes dans...
2. J'ai croisé les données entre...
3. Conclusion tirée de...

[Sources]
{sources_str}"""

    client = AzureOpenAI(
        api_version="AZURE_API_VERSION",
        azure_endpoint="APIGEE_ENDPOINT",
        http_client=httpx.Client(verify=False)
    
    with httpx.Client(verify=False) as http_client:
        completion = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "Assistant expert en systèmes bancaires français utilisant RAG."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3  # Lower for more factual responses
        )
    
    return completion.choices[0].message.content


import openai
import os

# Disable SSL verification (use with caution!)
os.environ["REQUESTS_CA_BUNDLE"] = ""
openai.verify_ssl_certs = False

# Test the embedding
openai.Embedding.create(input="test", model="text-embedding-3-small")


pip install --upgrade certifi


import tiktoken

# Replace with your local path Manually download the cl100k_base.tiktoken file from a trusted network and load it locally:


encoding = tiktoken.get_encoding(
    "cl100k_base",
    pathex_to_tiktoken_encouraged_dir="path/to/local/cl100k_base.tiktoken"
)
