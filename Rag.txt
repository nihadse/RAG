def retrieve_relevant_chunks(query: str, top_k: int = 35) -> list:
    """Retrieve relevant document chunks using multiple generated queries.
    
    Args:
        query: Input query in French
        top_k: Number of results to return after deduplication
    
    Returns:
        List of unique results sorted by relevance
    """
    from accelerate import Accelerator
    import torch

    # Initialize accelerator for multi-GPU consistency
    accelerator = Accelerator()
    
    # Generate multiple query variations
    generated_queries = generate_queries(query)
    
    all_results = []
    
    # Process each query variation
    for q in generated_queries:
        try:
            # Query using accelerator-aware collection
            results = collection.query(
                query_texts=[q],
                n_results=top_k * 2,  # Over-fetch to account for deduplication
                include=["documents", "metadatas", "distances"]
            )
            
            # Process results with GPU-aware tensors
            with torch.no_grad():
                for doc, meta, dist in zip(results["documents"][0],
                                         results["metadatas"][0],
                                         results["distances"][0]):
                    all_results.append({
                        "document": doc,
                        "metadata": meta,
                        "distance": accelerator.gather(torch.tensor(dist, device=device))
                    })
        
        except Exception as e:
            print(f"Error processing query '{q}': {str(e)}")
            continue

    # Deduplicate and sort using accelerator-aware operations
    seen_documents = set()
    unique_results = []
    
    # Sort by distance (lower is better)
    for result in sorted(all_results, key=lambda x: x["distance"].item()):
        doc_content = result["document"]
        if doc_content not in seen_documents:
            seen_documents.add(doc_content)
            
            # Convert tensor to Python native type for serialization
            unique_results.append({
                "document": doc_content,
                "metadata": result["metadata"],
                "score": 1 - result["distance"].cpu().item()  # Convert to similarity score
            })
    
    # Free GPU memory
    torch.cuda.empty_cache()
    
    return unique_results[:top_k]


import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from accelerate import Accelerator
import torch

# 1. Initialize Accelerator for multi-GPU
accelerator = Accelerator()
device = accelerator.device

# 2. Load BGE-M3 model with GPU acceleration
model_path = '/domino/edv/modelhub/Model Hub-model-huggingface-BAAI/bge-m3/main'
model = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name=model_path,
    device=device,
    normalize_embeddings=True
).model

# Prepare model for multi-GPU and FP16
model = model.half().to(device)
model = accelerator.prepare(model)

# 3. Custom Embedding Function with Chroma 0.4.x+ Compatibility
class AcceleratedBGEEmbeddingFunction:
    def __init__(self, model, accelerator):
        self.model = model
        self.accelerator = accelerator
        
    def __call__(self, input):  # <-- Chroma expects parameter named "input"
        with torch.no_grad():
            # Convert input to device and process
            inputs = self.model.tokenize(input)
            inputs = {k: v.to(self.accelerator.device) for k, v in inputs.items()}
            
            # Forward pass with multi-GPU support
            outputs = self.model.model(**inputs)
            
            # Gather embeddings from all GPUs
            embeddings = self.accelerator.gather(outputs.last_hidden_state[:, 0])
            
            # Convert to numpy and normalize
            embeddings = embeddings.cpu().numpy()
            return embeddings.tolist()  # Chroma expects List[List[float]]

# 4. Initialize Chroma Client
chroma_client = chromadb.PersistentClient(
    path="chroma_db",
    settings=Settings(allow_reset=True, anonymized_telemetry=False)
)

# 5. Create/Get Collection with Correct Configuration
collection = chroma_client.get_or_create_collection(
    name="pdf_documents3",
    metadata={
        "hnsw:space": "cosine",
        "hnsw:dimension": 1024  # Must match bge-m3's output size
    },
    embedding_function=AcceleratedBGEEmbeddingFunction(model, accelerator)
)

# Usage example
collection.add(
    documents=["Document en français...", "Autre document..."],
    ids=["1", "2"],
    metadatas=[{"source": "doc1"}, {"source": "doc2"}]
)
\



import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from accelerate import Accelerator
import torch

# Initialize Accelerator for multi-GPU
accelerator = Accelerator()
device = accelerator.device

# 1. Load BGE-M3 model with GPU acceleration
model_path = '/domino/edv/modelhub/Model Hub-model-huggingface-BAAI/bge-m3/main'
model = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name=model_path,
    device=device,
    normalize_embeddings=True
).model

# Prepare model for multi-GPU and FP16
model = model.half().to(device)
model = accelerator.prepare(model)

# 2. Custom Embedding Function with Accelerator
class AcceleratedBGEEmbeddingFunction:
    def __init__(self, model, accelerator):
        self.model = model
        self.accelerator = accelerator
        
    def __call__(self, texts):
        with torch.no_grad():
            inputs = self.model.tokenize(texts)
            inputs = {k:v.to(self.accelerator.device) for k,v in inputs.items()}
            outputs = self.model.model(**inputs)
            embeddings = self.accelerator.gather(outputs.last_hidden_state[:, 0])
            return embeddings.cpu().numpy()

# 3. Initialize Chroma with Accelerated Embeddings
chroma_client = chromadb.PersistentClient(
    path="chroma_db",
    settings=Settings(allow_reset=True, anonymized_telemetry=False)
    
# 4. Create/Get Collection with Proper Configuration
collection = chroma_client.get_or_create_collection(
    name="pdf_documents3",
    metadata={
        "hnsw:space": "cosine",
        "hnsw:dimension": 1024  # Must match bge-m3 output
    },
    embedding_function=AcceleratedBGEEmbeddingFunction(model, accelerator)
)

# 5. Usage Example
documents = ["Document français...", "Another French doc..."]
collection.add(
    documents=documents,
    ids=["1", "2"],
    metadatas=[{"source": "doc1"}, {"source": "doc2"}]
)

# Query with GPU acceleration
results = collection.query(
    query_texts=["Recherche de documents techniques"],
    n_results=2
)








pip install sentence-transformers accelerate torch>=2.0.0


import torch
from sentence_transformers import SentenceTransformer
from accelerate import Accelerator

# Initialize accelerator (auto-distributes across GPUs)
accelerator = Accelerator()
device = accelerator.device

# Load model with FP16 and distribute across GPUs
model = SentenceTransformer("BAAI/bge-m3", device=device)
model = model.half()  # FP16 for speed
model = accelerator.prepare(model)  # Distribute across GPUs

# Example: 1 million documents (adjust to your data)
texts = ["doc1", "doc2", ..., "doc1000000"]

# Batch size per GPU (adjust based on H100 memory)
# H100 has 80GB VRAM → 12 GPUs → ~6.6GB/GPU → batch_size=256 is safe
batch_size = 256 * accelerator.num_processes  # Total batches across GPUs

# Parallel embedding generation
embeddings = model.encode(
    texts,
    batch_size=batch_size,
    convert_to_tensor=True,  # Keep tensors on GPU
    show_progress_bar=True,
    normalize_embeddings=True,
    device=device,
)

# Gather embeddings from all GPUs (if needed)
embeddings = accelerator.gather(embeddings)















def generate_response(query, context):
    # Extract unique sources with page numbers
    sources = {}
    for chunk in context:
        source = chunk["metadata"]["source"]
        page = chunk["metadata"].get("page", "N/A")  # Handle missing page numbers
        if source not in sources:
            sources[source] = set()
        sources[source].add(page)
    
    # Format sources string
    sources_str = "\n".join([
        f"- [Source: {source}, Pages: {', '.join(sorted(pages))}]"
        for source, pages in sources.items()
    ])

    # Improved French prompt with strict formatting
    prompt = f"""Vous êtes un assistant spécialisé en architecture bancaire. Répondez en français avec:
1. Une réponse précise basée exclusivement sur les documents fournis
2. La procédure d'extraction des informations
3. Les références exactes des documents utilisés

Documents disponibles:
{context}

Question: {query}

Exigez le format de réponse suivant:
[Procédure]
1. J'ai identifié les informations pertinentes dans...
2. J'ai croisé les données entre...
3. Conclusion tirée de...

[Sources]
{sources_str}"""

    client = AzureOpenAI(
        api_version="AZURE_API_VERSION",
        azure_endpoint="APIGEE_ENDPOINT",
        http_client=httpx.Client(verify=False)
    
    with httpx.Client(verify=False) as http_client:
        completion = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "Assistant expert en systèmes bancaires français utilisant RAG."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3  # Lower for more factual responses
        )
    
    return completion.choices[0].message.content


import openai
import os

# Disable SSL verification (use with caution!)
os.environ["REQUESTS_CA_BUNDLE"] = ""
openai.verify_ssl_certs = False

# Test the embedding
openai.Embedding.create(input="test", model="text-embedding-3-small")


pip install --upgrade certifi


import tiktoken

# Replace with your local path Manually download the cl100k_base.tiktoken file from a trusted network and load it locally:


encoding = tiktoken.get_encoding(
    "cl100k_base",
    pathex_to_tiktoken_encouraged_dir="path/to/local/cl100k_base.tiktoken"
)
