# Update your collection creation to include segments
collection = chroma_client.get_or_create_collection(
    name="pdf_documents3",
    embedding_function=embedding_function,
    metadata={
        "hnsw:space": "cosine",
        "dimension": 1024,
        "segments": ["vector", "metadata"],  # REQUIRED FIX
        "hnsw:construction_ef": 200,
        "hnsw:M": 16
    }
)



import streamlit as st
import chromadb
import json
import datetime
from typing import List, Dict, Any, Tuple

# ChromaDB Initialization with Streamlit Session State
@st.cache_resource
def init_chroma():
    client = chromadb.PersistentClient(
        path="chroma_db",
        settings=chromadb.config.Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory="chroma_db",
            allow_reset=True,
            migrations="apply"
        )
    )
    
    # Get or create collection with metadata segments
    collection = client.get_or_create_collection(
        name="pdf_documents3",
        metadata={
            "hnsw:space": "cosine",
            "dimension": 1024,
            "segments": ["vector", "metadata"]
        }
    )
    
    # Initialize with dummy document if empty
    if collection.count() == 0:
        collection.add(
            documents=["Initial system document"],
            metadatas=[{"source": "system"}],
            ids=["init_id"]
        )
    
    return collection

class CustomRetrieverWithHistory:
    def __init__(self, collection, chat_history: List[Tuple[str, str]] = None, k: int = 50, rerank_k: int = 50):
        self.collection = collection
        self.chat_history = chat_history or []
        self.k = k
        self.rerank_k = rerank_k
        self.reranker = BM25Reranker(k=rerank_k)
        
        # Validate collection state
        self._validate_collection()

    def _validate_collection(self):
        """Ensure collection has required metadata segments"""
        try:
            # Force metadata update
            self.collection.update_metadata({
                "hnsw:space": "cosine",
                "dimension": 1024,
                "segments": ["vector", "metadata"]
            })
        except Exception as e:
            st.error(f"Collection validation failed: {str(e)}")
            st.stop()

    def get_relevant_documents(self, query: str) -> List[Dict[str, Any]]:
        try:
            # Step 1: Vector search
            results = self.collection.query(
                query_texts=[query],
                n_results=self.k * 2,
                include=["documents", "metadatas"]
            )
            
            docs = [
                {"text": doc, "metadata": meta}
                for doc, meta in zip(results["documents"][0], results["metadatas"][0])
            ]

            # Step 2: Query expansion with history
            enhanced_docs = []
            if self.chat_history:
                recent_history = self.chat_history[-3:]  # Last 3 exchanges
                history_context = "\n".join(
                    [f"Q: {q}\nA: {a}" for q, a in recent_history]
                )
                enhanced_query = f"{query}\nContext:\n{history_context}"
                
                # Secondary search
                extra_results = self.collection.query(
                    query_texts=[enhanced_query],
                    n_results=100,
                    include=["documents", "metadatas"]
                )
                
                enhanced_docs = [
                    {"text": doc, "metadata": meta}
                    for doc, meta in zip(extra_results["documents"][0], extra_results["metadatas"][0])
                ]

            # Merge and deduplicate
            all_docs = {doc["text"]: doc for doc in docs + enhanced_docs}
            unique_docs = list(all_docs.values())[:self.k*3]

            # Step 3: Rerank
            return self.reranker.rerank(query, unique_docs)
            
        except Exception as e:
            st.error(f"Retrieval error: {str(e)}")
            return []

    def update_history(self, query: str, answer: str):
        self.chat_history.append((query, answer))
        
    def save_history(self, file_path: str = "rag_state.json"):
        state = {
            "chat_history": self.chat_history,
            "timestamp": str(datetime.datetime.now())
        }
        with open(file_path, "w") as f:
            json.dump(state, f)
            
    def load_history(self, file_path: str = "rag_state.json"):
        try:
            with open(file_path, "r") as f:
                state = json.load(f)
                self.chat_history = state.get("chat_history", [])
                st.success(f"Loaded {len(self.chat_history)} previous conversations")
        except FileNotFoundError:
            st.warning("No previous history found")
        except Exception as e:
            st.error(f"Error loading history: {str(e)}")

# Streamlit App
def main():
    st.title("RAG Chatbot")
    
    # Initialize components
    collection = init_chroma()
    
    # Initialize retriever in session state
    if "retriever" not in st.session_state:
        st.session_state.retriever = CustomRetrieverWithHistory(collection)
        st.session_state.retriever.load_history()
    
    # Chat interface
    user_input = st.chat_input("Ask a question:")
    
    if user_input:
        with st.spinner("Searching..."):
            # Retrieve documents
            context = st.session_state.retriever.get_relevant_documents(user_input)
            
            # Generate answer (implement your LLM here)
            answer = f"Found {len(context)} relevant documents"
            
            # Update history
            st.session_state.retriever.update_history(user_input, answer)
            st.session_state.retriever.save_history()
            
            # Display
            st.write(f"**Answer:** {answer}")
            with st.expander("See context"):
                st.json(context)

if __name__ == "__main__":
    main()


import chromadb
from typing import List
from FlagEmbedding import FlagModel

# Initialize Chroma client with proper configuration
chroma_client = chromadb.PersistentClient(
    path="chroma_db",
    settings=chromadb.config.Settings(
        chroma_db_impl="duckdb+parquet",  # More reliable for most use cases
        persist_directory="chroma_db",
        allow_reset=True,
        is_persistent=True,
        migrations="apply"  # Ensure schema migrations are handled
    )
)

# Initialize BGE-M3 model
model_path = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"
model = FlagModel(
    model_path,
    query_instruction_for_retrieval="Generate representation for this sentence to retrieve relevant articles:",
    use_fp16=False  # Disable FP16 if not using compatible GPU
)

# Create compliant embedding function with dimension validation
class BGE_M3_EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        self.dimension = 1024  # BGE-M3 embedding dimension
        
    def __call__(self, input: List[str]) -> List[List[float]]:
        if isinstance(input, str):
            input = [input]
        embeddings = self.model.encode(input).tolist()
        # Validate dimension consistency
        if len(embeddings[0]) != self.dimension:
            raise ValueError(f"Embedding dimension mismatch. Expected {self.dimension}, got {len(embeddings[0])}")
        return embeddings

# Initialize embedding function
embedding_function = BGE_M3_EmbeddingFunction(model)

# Create collection with proper configuration
try:
    # Try to delete existing collection if it has invalid configuration
    chroma_client.delete_collection("pdf_documents3")
except Exception as e:
    print(f"Cleanup warning: {str(e)}")

collection = chroma_client.get_or_create_collection(
    name="pdf_documents3",
    embedding_function=embedding_function,
    metadata={
        "hnsw:space": "cosine",
        "dimension": embedding_function.dimension,  # Direct reference
        "hnsw:construction_ef": 200,
        "hnsw:M": 16,
        "hnsw:max_elements": 1000000  # Adjust based on your needs
    }
)

# Verify collection configuration
print("Collection metadata:", collection.metadata)
print("Collection count:", collection.count())

# Query test with validation
try:
    results = collection.query(
        query_texts=["example query"],
        n_results=20,
        include=["documents", "embeddings"]
    )
    print("Query results:", results)
except Exception as e:
    print("Query failed:", str(e))
    if "embeddings" in locals():
        print("First embedding dimension:", len(results["embeddings"][0][0]))





import chromadb
from typing import List
from FlagEmbedding import FlagModel

# Initialize Chroma client
chroma_client = chromadb.PersistentClient(path="chroma_db")

# Initialize BGE-M3 model
model_path = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"
model = FlagModel(
    model_path,
    query_instruction_for_retrieval="Generate representation for this sentence to retrieve relevant articles:",
    use_fp16=True  # Enable if using GPU
)

# Create compliant embedding function
class BGE_M3_EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        
    def __call__(self, input: List[str]) -> List[List[float]]:
        # Convert single string to list
        if isinstance(input, str):
            input = [input]
            
        # Get embeddings (automatically batches inputs)
        embeddings = self.model.encode(input).tolist()
        return embeddings

# Create embedding function instance
bge_ef = BGE_M3_EmbeddingFunction(model)

# Create collection with dimension metadata
collection = chroma_client.get_or_create_collection(
    name="pdf_documents3",
    embedding_function=bge_ef,
    metadata={"hnsw:space": "cosine", "dimension": 1024}
)

# Query the collection
results = collection.query(
    query_texts=["example query"],
    n_results=20,
    include=["documents"]
)

print(results)








import chromadb
from typing import List
from FlagEmbedding import FlagModel

# Initialize Chroma client
chroma_client = chromadb.PersistentClient(path="chroma_db")

# Initialize BGE-M3 model
model_path = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"
model = FlagModel(
    model_path,
    query_instruction_for_retrieval="Generate representation for this sentence to retrieve relevant articles:",
    use_fp16=True  # Enable if using GPU
)

# Create compliant embedding function
class BGE_M3_EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        
    def __call__(self, input: List[str]) -> List[List[float]]:
        # Convert single string to list
        if isinstance(input, str):
            input = [input]
            
        # Get embeddings (automatically batches inputs)
        embeddings = self.model.encode(input).tolist()
        return embeddings

# Create embedding function instance
bge_ef = BGE_M3_EmbeddingFunction(model)

# Create collection with dimension metadata
collection = chroma_client.get_or_create_collection(
    name="pdf_documents3",
    embedding_function=bge_ef,
    metadata={"hnsw:space": "cosine", "dimension": 1024}
)

# Query the collection
results = collection.query(
    query_texts=["example query"],
    n_results=20,
    include=["documents"]
)

print(results)



import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from FlagEmbedding import FlagModel

# Initialize Chroma client
chroma_client = chromadb.PersistentClient(path="chroma_db")

# Initialize BGE-M3 model
model_path = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"
model = FlagModel(
    model_path,
    query_instruction_for_retrieval="Generate representation for this sentence to retrieve relevant articles:",
    use_fp16=True  # Enable if using GPU
)

# Create custom embedding function
def bge_m3_embedding_function(texts):
    return model.encode(texts).tolist()

# Create collection with custom embedding function
collection = chroma_client.get_or_create_collection(
    name="pdf_documents3",
    embedding_function=bge_m3_embedding_function
)

# Query the collection
results = collection.query(
    query_texts=["example query"],
    n_results=20,
    include=["documents"]
)

print(results)





import streamlit as st
from your_rag_functions import (  # Replace with your actual imports
    generate_alternative_queries,
    generate_response,
    generate_answer_explanation,
    evaluate_rag_response,
    extract_source_files
)

# Initialize session state for chat history
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "current_chat" not in st.session_state:
    st.session_state.current_chat = []

# Initialize chat sessions
if "chat_sessions" not in st.session_state:
    st.session_state.chat_sessions = {"New Chat": []}

def rag_chatbot(query):
    # Your existing RAG processing logic
    all_queries = [query] + generate_alternative_queries(query)
    all_results = []

    for q in all_queries:
        context = retriever.get_relevant_documents(q)
        answer = generate_response(q, context)
        all_results.append({
            "query": q,
            "answer": answer,
            "context": context,
            "num_sources": len(context)
        })

    best_result = max(all_results, key=lambda x: x["num_sources"])
    explanation = generate_answer_explanation(best_result["query"], best_result["answer"], best_result["context"])
    evaluation = evaluate_rag_response(best_result["query"], best_result["answer"], best_result["context"])
    sources = extract_source_files(best_result["context"])

    # Update history
    retriever.update_history(best_result["query"], best_result["answer"])
    retriever.save_history()

    return {
        "original_query": query,
        "alternative_queries": all_queries[1:],
        "final_query": best_result["query"],
        "answer": best_result["answer"],
        "explanation": explanation,
        "evaluation": evaluation,
        "sources": sources
    }

# Sidebar for chat sessions
with st.sidebar:
    st.header("Chat Sessions")
    
    # Create new chat
    if st.button("+ New Chat"):
        new_chat_name = f"Chat {len(st.session_state.chat_sessions) + 1}"
        st.session_state.chat_sessions[new_chat_name] = []
        st.session_state.current_chat = new_chat_name
    
    # Display existing chats
    selected_chat = st.radio(
        "Select Chat",
        options=list(st.session_state.chat_sessions.keys()),
        index=0
    )
    st.session_state.current_chat = selected_chat

# Main chat interface
st.title("RAG Chatbot")

# Display chat history
chat_container = st.container()
with chat_container:
    for message in st.session_state.chat_sessions[st.session_state.current_chat]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if message["role"] == "assistant":
                with st.expander("Details"):
                    st.write("**Sources:**", message["sources"])
                    st.write("**Evaluation:**", message["evaluation"])

# Input area
user_input = st.chat_input("Type your message here...")
if user_input:
    # Add user message to chat history
    st.session_state.chat_sessions[st.session_state.current_chat].append({
        "role": "user",
        "content": user_input
    })
    
    # Process query
    with st.spinner("Thinking..."):
        response = rag_chatbot(user_input)
    
    # Add assistant response to chat history
    st.session_state.chat_sessions[st.session_state.current_chat].append({
        "role": "assistant",
        "content": response["answer"],
        "sources": response["sources"],
        "evaluation": response["evaluation"]
    })
    
    # Rerun to update the chat display
    st.rerun()
