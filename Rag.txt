


import streamlit as st
import datetime

def main():
    st.title("üí¨ Chatbot BNP ED")

    # === Initialize session states
    if "chat_sessions" not in st.session_state:
        st.session_state.chat_sessions = {}

    if "current_chat" not in st.session_state:
        st.session_state.current_chat = str(datetime.datetime.now())

    if "copy_time" not in st.session_state:
        st.session_state.copy_time = datetime.datetime.now()

    if st.session_state.current_chat not in st.session_state.chat_sessions:
        st.session_state.chat_sessions[st.session_state.current_chat] = []

    chat = st.session_state.chat_sessions[st.session_state.current_chat]
    temp_chunks = []

    # === File upload
    uploaded_file = st.sidebar.file_uploader("üìé Upload a document (optional)", type=["pdf", "txt", "docx", "csv", "xlsx"])

    if uploaded_file:
        filename = uploaded_file.name.lower()
        existing = get_existing_sources()

        if filename in existing:
            st.sidebar.warning("‚ö†Ô∏è This file is already indexed in ChromaDB.")
        else:
            with st.spinner("Extracting and cleaning uploaded document..."):
                raw_doc = extract_text_from_uploaded(uploaded_file)

                if raw_doc is None:
                    st.sidebar.error("‚ùå Unsupported file format.")
                    return

                docs = [raw_doc]
                cleaned = clean_documents(docs)
                temp_chunks = split_documents_semantic(cleaned)

                st.sidebar.success(f"‚úÖ File loaded with {len(temp_chunks)} temporary chunks.")

    # === Display previous chat
    if "chat_history" not in st.session_state:
        handle_chat_history()  # Load it

    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            if message["role"] == "user" and "timestamp" in message:
                st.markdown(f"_{message['timestamp']}_", unsafe_allow_html=True)
            st.markdown(message["content"], unsafe_allow_html=True)

    # === Chat input
    user_input = st.chat_input("Type your message here...")

    if user_input:
        timestamp = datetime.datetime.now()

        # Save user message
        handle_chat_history(user_input=user_input)

        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = rag_chatbot(user_input, temp_chunks=temp_chunks)
                answer = response["answer"]

                st.markdown(answer, unsafe_allow_html=True)

                # Save assistant response
                handle_chat_history(response=answer)




import os
import json
import datetime
from langchain.schema import Document

CHAT_HISTORY_FILE = "chat_history.json"

def handle_chat_history(user_input=None, response=None, clear=False):
    """
    Handles and saves chat history to JSON file.
    - Loads existing chat from JSON
    - Appends new messages
    - Writes back to JSON
    - Optionally clears the chat
    """
    # Load or initialize chat
    if "chat_history" not in st.session_state or clear:
        if clear:
            if os.path.exists(CHAT_HISTORY_FILE):
                os.remove(CHAT_HISTORY_FILE)
            st.session_state.chat_history = []
            return

        if os.path.exists(CHAT_HISTORY_FILE):
            with open(CHAT_HISTORY_FILE, "r", encoding="utf-8") as f:
                st.session_state.chat_history = json.load(f)
        else:
            st.session_state.chat_history = []

    # Add new messages
    if user_input:
        st.session_state.chat_history.append({
            "role": "user",
            "content": user_input,
            "timestamp": datetime.datetime.now().isoformat()
        })
    if response:
        st.session_state.chat_history.append({
            "role": "assistant",
            "content": response,
            "timestamp": datetime.datetime.now().isoformat()
        })

    # Save to file
    with open(CHAT_HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(st.session_state.chat_history, f, ensure_ascii=False, indent=2)










import fitz  # For PDF
import docx  # For DOCX
import pandas as pd  # For Excel and CSV
from langchain.schema import Document

def extract_text_from_uploaded(uploaded_file):
    """
    Load uploaded file via Streamlit and return it as a Document object.
    Supports: PDF, TXT, DOCX, CSV, XLSX.
    """
    filename = uploaded_file.name.lower()

    # === PDF ===
    if filename.endswith(".pdf"):
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        text = "\n".join([page.get_text() for page in doc])

    # === TXT ===
    elif filename.endswith(".txt"):
        text = uploaded_file.read().decode("utf-8")

    # === DOCX ===
    elif filename.endswith(".docx"):
        doc = docx.Document(uploaded_file)
        text = "\n".join([p.text for p in doc.paragraphs])

    # === CSV ===
    elif filename.endswith(".csv"):
        df = pd.read_csv(uploaded_file)
        text = df.to_string(index=False)

    # === XLSX / Excel ===
    elif filename.endswith(".xlsx"):
        df = pd.read_excel(uploaded_file)
        text = df.to_string(index=False)

    # ‚ùå Unsupported
    else:
        return None

    return Document(
        page_content=text,
        metadata={
            "source_file": uploaded_file.name,
            "source_type": filename.split(".")[-1]
        }
    )




import fitz  # PyMuPDF
import docx
from langchain.schema import Document

def load_uploaded_file(uploaded_file):
    """
    Load uploaded file via Streamlit and return it as a Document object.
    Supports .pdf, .txt, and .docx formats.
    """
    filename = uploaded_file.name.lower()

    # === PDF ===
    if filename.endswith(".pdf"):
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        text = "\n".join([page.get_text() for page in doc])

    # === TXT ===
    elif filename.endswith(".txt"):
        text = uploaded_file.read().decode("utf-8")

    # === DOCX ===
    elif filename.endswith(".docx"):
        doc = docx.Document(uploaded_file)
        text = "\n".join([p.text for p in doc.paragraphs])

    else:
        return None  # or raise Exception("Unsupported file format")

    return Document(
        page_content=text,
        metadata={"source_file": uploaded_file.name, "source_type": filename.split(".")[-1]}
    )



import streamlit as st

st.title("üìÑ Document Upload")

uploaded_file = st.file_uploader("Upload a file", type=["pdf", "txt", "docx"])

if uploaded_file is not None:
    doc = load_uploaded_file(uploaded_file)

    if doc:
        st.success(f"‚úÖ Loaded file: {uploaded_file.name}")
        st.write(doc.page_content[:500])  # Preview first 500 chars
        # Now you can: clean ‚Üí split ‚Üí embed ‚Üí etc.
    else:
        st.error("‚ùå Unsupported file format.")










from langchain.schema import Document

docs = [Document(page_content=raw_text)]
cleaned_docs = clean_documents(docs, lowercase=True)
chunks = split_documents_semantic(cleaned_docs)



import streamlit as st
import datetime

def main():
    st.set_page_config(page_title="üß† Chatbot BNP ED", layout="centered")
    st.title("üìö Chatbot BNP ED")

    # === Initialize Session State ===
    if "chat_sessions" not in st.session_state:
        st.session_state.chat_sessions = {}
    if "current_chat" not in st.session_state:
        st.session_state.current_chat = str(datetime.datetime.now())
    if st.session_state.current_chat not in st.session_state.chat_sessions:
        st.session_state.chat_sessions[st.session_state.current_chat] = []

    chat = st.session_state.chat_sessions[st.session_state.current_chat]

    # === Optional Document Upload (not stored in ChromaDB) ===
    temp_chunks = []
    uploaded_file = st.sidebar.file_uploader("üì§ Upload a document (optional)", type=["pdf", "txt", "docx"])
    
    if uploaded_file:
        filename = uploaded_file.name.lower()
        existing = get_existing_sources()

        if filename in existing:
            st.sidebar.warning("‚ö†Ô∏è This file is already indexed in ChromaDB.")
        else:
            with st.spinner("Extracting and cleaning uploaded document..."):
                raw_text = extract_text_from_uploaded(uploaded_file)
                if raw_text.startswith("Unsupported"):
                    st.sidebar.error("‚ùå Unsupported file format.")
                    return
                cleaned = clean_text(raw_text)
                temp_chunks = split_into_chunks(cleaned)
            st.sidebar.success(f"‚úÖ File loaded with {len(temp_chunks)} temporary chunks.")

    # === Render Previous Messages ===
    if chat:
        for message in chat:
            with st.chat_message(message["role"]):
                if message["role"] == "user" and "timestamp" in message:
                    st.markdown(f"*{message['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}*")
                st.markdown(message["content"], unsafe_allow_html=True)

    # === Chat Input ===
    user_input = st.chat_input("Type your message here...")

    if user_input:
        timestamp = datetime.datetime.now()
        chat.append({
            "role": "user",
            "content": user_input,
            "timestamp": timestamp
        })

        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = rag_chatbot(user_input, temp_chunks=temp_chunks)
                answer = response["answer"]

            st.markdown(answer, unsafe_allow_html=True)

            chat.append({
                "role": "assistant",
                "content": answer,
                "timestamp": datetime.datetime.now()
            })




import streamlit as st
import datetime
import re
import fitz
import docx
from io import BytesIO
from uuid import uuid4
from chromadb import PersistentClient
from sentence_transformers import SentenceTransformer
from langchain.schema import Document

# === Chroma Setup ===
chroma_client = PersistentClient(path="/domino/datasets/local/vect-pro-base/")
model_path = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"
model = SentenceTransformer(model_path)


class BGEM3EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        self.dimension = 1024

    def __call__(self, input_texts):
        if isinstance(input_texts, str):
            input_texts = [input_texts]
        return self.model.encode(input_texts).tolist()


embedding_function = BGEM3EmbeddingFunction(model)

collection = chroma_client.get_or_create_collection(
    name="my_documents",
    embedding_function=embedding_function,
    metadata={"hnsw:space": "cosine", "dimension": embedding_function.dimension}
)


# === Text Processing Utilities ===
def extract_text_from_uploaded(uploaded_file):
    filename = uploaded_file.name.lower()

    if filename.endswith(".pdf"):
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        return "\n".join(page.get_text() for page in doc)

    elif filename.endswith(".txt"):
        return uploaded_file.read().decode("utf-8")

    elif filename.endswith(".docx"):
        doc = docx.Document(BytesIO(uploaded_file.read()))
        return "\n".join([p.text for p in doc.paragraphs])

    else:
        return "Unsupported file format."


def clean_text(text, lowercase=False):
    text = re.sub(r'\.{2,}', '.', text)
    text = re.sub(r'\t+', ' ', text)
    text = "\n".join(line.strip() for line in text.splitlines())
    text = "\n".join([line for line in text.splitlines() if line.strip() != ""])
    return text.lower() if lowercase else text


def split_into_chunks(text, chunk_size=1000, overlap=200):
    start = 0
    chunks = []
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks


def get_existing_sources():
    try:
        results = collection.query(query_texts=["."], n_results=1000, include=["metadatas"])
        metadatas = results.get("metadatas", [[]])[0]
        return set(meta["source"] for meta in metadatas if "source" in meta)
    except:
        return set()


# === RAG Chatbot Pipeline ===
def rag_chatbot(query, temp_chunks=None):
    st.markdown(f"üïê {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    alternative_queries = generate_alternative_queries(query)
    all_queries = [query] + alternative_queries
    all_results = []

    retriever = CustomRetrieverWithHistory(collection=collection, k=50, rerank_k=30)

    for q in all_queries:
        context = retriever.get_relevant_documents(q)

        if temp_chunks:
            context += [{"text": chunk, "metadata": {"source": "TEMP_UPLOAD"}} for chunk in temp_chunks]

        answer = generate_response(q, context)

        all_results.append({
            "query": q,
            "answer": answer,
            "context": context,
            "num_sources": len(context)
        })

    best_result = max(all_results, key=lambda x: x["num_sources"])
    final_query = best_result["query"]
    final_answer = best_result["answer"]
    final_context = best_result["context"]

    explanation = generate_answer_explanation(final_query, final_answer, final_context)
    evaluation = evaluate_rag_response(final_query, final_answer, final_context)
    sources = extract_source_files(final_context)

    retriever.update_history(final_query, final_answer)
    retriever.save_history()

    return {
        "query": final_query,
        "answer": final_answer,
        "context": final_context,
        "evaluation": evaluation,
        "sources": sources,
        "explanation": explanation,
        "time": datetime.datetime.now()
    }


# === Streamlit UI ===
def main():
    st.set_page_config(page_title="üß† RAG Chatbot", layout="centered")
    st.title("üìö Ask Your Documents (Preloaded + Temporary)")

    temp_chunks = []
    uploaded_file = st.sidebar.file_uploader("üì§ Upload a new document (optional)", type=["pdf", "txt", "docx"])

    if uploaded_file:
        filename = uploaded_file.name.lower()
        existing = get_existing_sources()

        if filename in existing:
            st.sidebar.warning("‚ö†Ô∏è This file is already indexed. It will not be used again.")
        else:
            with st.spinner("Extracting text..."):
                raw_text = extract_text_from_uploaded(uploaded_file)

            if raw_text.startswith("Unsupported"):
                st.error("‚ùå Unsupported format.")
                return

            cleaned = clean_text(raw_text)
            temp_chunks = split_into_chunks(cleaned)
            st.sidebar.success(f"‚úÖ File processed. {len(temp_chunks)} temporary chunks ready.")

    query = st.text_input("üí¨ Ask a question about the documents:")

    if query:
        with st.spinner("üí° Running RAG..."):
            result = rag_chatbot(query, temp_chunks=temp_chunks)

            st.subheader("ü§ñ Answer")
            st.write(result["answer"])

            st.subheader("üìÑ Source Chunks")
            for i, doc in enumerate(result["context"]):
                st.markdown(f"**{i+1}.** {doc['text'][:500]}...\n---")

            st.subheader("üìä Evaluation")
            st.json(result["evaluation"])

            st.subheader("üìå Explanation")
            st.write(result["explanation"])

            st.subheader("üìÅ Sources")
            st.write(result["sources"])


if __name__ == "__main__":
    main()



import os
import re
import fitz
import docx
import streamlit as st
from io import BytesIO
from uuid import uuid4
from chromadb import PersistentClient
from sentence_transformers import SentenceTransformer
from langchain.schema import Document


# ========== SETUP ==========
chroma_client = PersistentClient(path="/domino/datasets/local/vect-pro-base/")

model_path = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"
model = SentenceTransformer(model_path)


class BGEM3EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        self.dimension = 1024

    def __call__(self, input_texts):
        if isinstance(input_texts, str):
            input_texts = [input_texts]
        return self.model.encode(input_texts).tolist()


embedding_function = BGEM3EmbeddingFunction(model)

collection = chroma_client.get_or_create_collection(
    name="my_documents",
    embedding_function=embedding_function,
    metadata={"hnsw:space": "cosine", "dimension": embedding_function.dimension}
)


# ========== TEXT PROCESSING UTILS ==========
def extract_text_from_uploaded(uploaded_file):
    filename = uploaded_file.name.lower()

    if filename.endswith(".pdf"):
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        return "\n".join(page.get_text() for page in doc)

    elif filename.endswith(".txt"):
        return uploaded_file.read().decode("utf-8")

    elif filename.endswith(".docx"):
        doc = docx.Document(BytesIO(uploaded_file.read()))
        return "\n".join([p.text for p in doc.paragraphs])

    else:
        return "Unsupported file format."


def clean_text(text, lowercase=False):
    text = re.sub(r'\.{2,}', '.', text)
    text = re.sub(r'\t+', ' ', text)
    text = "\n".join(line.strip() for line in text.splitlines())
    text = "\n".join([line for line in text.splitlines() if line.strip() != ""])
    return text.lower() if lowercase else text


def split_into_chunks(text, chunk_size=1000, overlap=200):
    start = 0
    chunks = []
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks


# ========== HELPER ==========
def get_existing_sources():
    """Retrieve all existing source file names from Chroma."""
    try:
        # We'll query with a dummy term just to extract metadatas
        dummy_query = collection.query(query_texts=["dummy"], n_results=1000, include=["metadatas"])
        metadatas = dummy_query.get("metadatas", [[]])[0]
        return set(meta["source"] for meta in metadatas if "source" in meta)
    except:
        return set()


# ========== STREAMLIT APP ==========
def main():
    st.set_page_config(page_title="üß† RAG Chatbot", layout="centered")
    st.title("üìö Ask Your Documents")

    new_chunks = []
    uploaded_file = st.sidebar.file_uploader("üì§ Upload a new document (not stored)", type=["pdf", "txt", "docx"])

    if uploaded_file:
        filename = uploaded_file.name.lower()
        existing_sources = get_existing_sources()

        if filename in existing_sources:
            st.warning("‚ö†Ô∏è This document has already been embedded. It will be ignored.")
        else:
            with st.spinner("Processing document..."):
                raw_text = extract_text_from_uploaded(uploaded_file)
                if raw_text.startswith("Unsupported"):
                    st.error("‚ùå Unsupported file format.")
                    return

                cleaned = clean_text(raw_text)
                new_chunks = split_into_chunks(cleaned)
                st.success(f"‚úÖ Document processed into {len(new_chunks)} temporary chunks (not stored).")

    query = st.text_input("üí¨ Ask a question about the documents:")

    if query:
        with st.spinner("Searching ChromaDB..."):
            results = collection.query(
                query_texts=[query],
                n_results=5,
                include=["documents", "metadatas"]
            )
            base_docs = results["documents"][0]

        # Optional: Embed new chunks and rerank manually
        if new_chunks:
            new_embeddings = embedding_function(new_chunks)
            new_scores = model.similarity_faiss([query], new_embeddings)[0]  # You can use cosine if needed
            top_k = 3
            top_indices = sorted(range(len(new_scores)), key=lambda i: -new_scores[i])[:top_k]
            extra_docs = [new_chunks[i] for i in top_indices]
        else:
            extra_docs = []

        all_docs = base_docs + extra_docs

        if all_docs:
            st.subheader("üìÑ Retrieved Chunks:")
            for i, doc in enumerate(all_docs):
                st.markdown(f"**{i+1}.** {doc[:500]}...\n---")
        else:
            st.warning("No relevant documents found.")

        st.subheader("ü§ñ Assistant Answer (Placeholder)")
        st.success("Answer simulated from retrieved content.")


if __name__ == "__main__":
    main()




from uuid import uuid4
from typing import List
from io import BytesIO
import fitz  # PyMuPDF
import docx

def extract_text_from_uploaded(uploaded_file) -> str:
    """Extracts raw text from PDF, DOCX, or TXT."""
    filename = uploaded_file.name.lower()

    if filename.endswith(".pdf"):
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        return "\n".join(page.get_text() for page in doc)

    elif filename.endswith(".txt"):
        return uploaded_file.read().decode("utf-8")

    elif filename.endswith(".docx"):
        doc = docx.Document(BytesIO(uploaded_file.read()))
        return "\n".join([p.text for p in doc.paragraphs])

    else:
        return "Unsupported file format."


def chunk_text(text: str, chunk_size=1000, overlap=200) -> List[str]:
    """Splits text into overlapping chunks."""
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks


def embed_and_store_text(chroma_collection, text: str, file_name: str):
    """Chunks, embeds, and adds the text to an existing ChromaDB collection."""
    chunks = chunk_text(text)
    for i, chunk in enumerate(chunks):
        chroma_collection.add(
            documents=[chunk],
            metadatas=[{"source": file_name, "chunk": i}],
            ids=[str(uuid4())]
        )


Bonjour,
Tout fonctionne correctement, merci pour l'acc√®s.






Objet : Remboursement frais de visa Capago

Bonjour,

Veuillez trouver ci-joint la capture d‚Äô√©cran de mes frais de visa Capago pour le remboursement.

Cordialement,
[Votre pr√©nom et nom]



.
