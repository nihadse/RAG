def evaluate_rag_response(query, answer, retrieved_docs, feedback=None):
    """
    Evaluate the quality of a RAG (Retrieval-Augmented Generation) response based on various metrics.
    """
    evaluation = {}

    # 1. Document relevance score
    if isinstance(query, list):
        query = " ".join(str(q) for q in query)
    query_tokens = set(query.lower().split())

    relevant_docs = 0
    for doc in retrieved_docs:
        text = doc.get('content') or doc.get('page_content') or ''
        doc_tokens = set(text.lower().split())
        overlap = len(query_tokens.intersection(doc_tokens)) / len(query_tokens) if query_tokens else 0
        if overlap > 0.2:  # Simple relevance threshold
            relevant_docs += 1

    doc_relevance = relevant_docs / len(retrieved_docs) if retrieved_docs else 0
    evaluation["document_relevance"] = round(doc_relevance * 10, 2)  # Score out of 10

    # 2. Source diversity
    sources = set()
    for doc in retrieved_docs:
        if "metadata" in doc and ("source_file" in doc["metadata"] or "source" in doc["metadata"]):
            sources.add(doc["metadata"].get("source_file") or doc["metadata"].get("source"))

    source_diversity = min(len(sources) / 3, 1.0)  # Normalize to maximum of 1
    evaluation["source_diversity"] = round(source_diversity * 10, 2)  # Score out of 10

    # 3. Answer relevance
    if isinstance(answer, list):
        answer = " ".join(str(a) for a in answer)
    answer_tokens = set(answer.lower().split())
    query_answer_overlap = len(query_tokens.intersection(answer_tokens)) / len(query_tokens) if query_tokens else 0
    evaluation["answer_query_alignment"] = round(query_answer_overlap * 10, 2)  # Score out of 10

    # 4. User feedback score
    if feedback is not None:
        evaluation["user_feedback"] = feedback  # Expected to be between 1-5

    # 5. Hallucination risk
    contains_source_citation = any(keyword in answer.lower() for keyword in ["source", "doc", "provient", "extrait"])
    evaluation["hallucination_risk"] = "Low" if contains_source_citation else "Medium"

    # Overall score (weighted average)
    weights = {
        "document_relevance": 0.4,
        "source_diversity": 0.1,
        "answer_query_alignment": 0.3,
    }
    overall_score = sum(evaluation[key] * weights[key] for key in weights.keys() if key in evaluation)
    evaluation["overall_score"] = round(overall_score, 2)  # Score out of 10

    return evaluation



# Prepare the documents summary safely
    sources_summary = "\n".join([
        (doc.get('content') or doc.get('page_content') or "")[:300] 
        for doc in documents[:5]
    ])


# --- FUNCTIONS YOU ADDED ---

def extract_source_files(documents):
    """
    Extract source files from document metadata.
    """
    sources = []
    for doc in documents:
        if "source_file" in doc.get("metadata", {}):
            sources.append(doc["metadata"]["source_file"])
        elif "source" in doc.get("metadata", {}):
            sources.append(doc["metadata"]["source"])
    return list(set(sources))


def save_chat_history(chat_history, filename="chat_history.json"):
    """
    Save the chat history to a JSON file.
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(chat_history, f, indent=2, ensure_ascii=False)
        print(f"Chat history saved to {filename}")
    except Exception as e:
        print(f"Error saving chat history: {e}")

# --- RAG WORKFLOW ---

query = "Explique-moi l'architecture du syst√®me bancaire DARA."

# 1. Generate alternative queries (optional)
alternatives = generate_alternative_queries(query)

# 2. Retrieve documents
context = retriever.get_relevant_documents(query)

# 3. Generate final answer
answer = generate_response(query, context)

# 4. Generate explanation of the answer
explanation = generate_answer_explanation(query, answer, context)

# 5. Evaluate the RAG response
evaluation = evaluate_rag_response(query, answer, context)

# 6. Extract source files
sources = extract_source_files(context)

# 7. Update and save retriever history
retriever.update_history(query, answer)
retriever.save_history()

# 8. (Optional) Save full chat history
chat_history = {
    "query": query,
    "alternative_queries": alternatives,
    "answer": answer,
    "explanation": explanation,
    "evaluation": evaluation,
    "sources": sources
}
save_chat_history(chat_history)

# --- OUTPUT ---
print("Answer:", answer)
print("Explanation:", explanation)
print("Evaluation:", evaluation)
print("Sources:", sources)
