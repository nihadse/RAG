import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("✓ Model loaded.\n")

# --------------------------
# Field Schemas
# --------------------------
PAGE_SCHEMAS = {
    "ordre_de_virement": [
        "Monnaie", "Montant en chiffres", "Montant en lettres", "Agence",
        "Nom/Prénom", "Adresse complète", "Numéro de compte", "Date de valeur",
        "bank code", "Siège", "Racine", "Ordinal", "Devise", "Clé",
        "Adresse SWIFT", "Autre (Transfert en EURO)",
    ],
    "annexe_1": ["Nom", "Date de signature"],
    "annexe_2": ["Mois", "Part transférable", "Nom et prénom"],
    "domiciliation_import": ["Nom"],
    "bulletin_de_paie": ["Nom"],
    "gestion_de_blocages": [],
}

ID_FIELDS = [
    "اللقب", "الاسم", "تاريخ الميلاد", "الجنس",
    "رقم الوطني", "تاريخ الإصدار", "تاريخ الانتهاء",
    "سلطة الإصدار", "Nom", "Prénom"
]

# --------------------------
# Prompt Builder (YOUR CUSTOM VERSION)
# --------------------------
def build_prompt(text: str, fields: list, is_bank: bool = False) -> str:
    """Build CPU-friendly minimal prompt"""
    order = " | ".join(fields)
    
    # Base prompt
    prompt = f"""You are an expert document information extraction system.
Extract the following fields in EXACT order separated by "|":

{order}
"""
    
    # Add bank-specific rules
    if is_bank:
        prompt += """
Note for bank documents:
- bank code: 3 numbers
- Siège: 5 numbers
- Racine: 6 numbers
- Ordinal: 3 numbers
- Devise: letters
- Clé: 2 numbers

"""
    
    # Instructions
    prompt += f"""Return ONLY values separated by "|".
If a value is missing, return null.
No explanations, no field names.

OCR:
{text}
"""
    
    return prompt.strip()

# --------------------------
# Helper: Detect page type
# --------------------------
def detect_page_type(text: str) -> str:
    text_lower = text.lower()
    if "ordre de virement" in text_lower or "virement a l'etranger" in text_lower:
        return "ordre_de_virement"
    elif "annexe 1" in text_lower or "annexe i" in text_lower:
        return "annexe_1"
    elif "annexe 2" in text_lower or "annexe ii" in text_lower:
        return "annexe_2"
    elif "domiciliation" in text_lower and "import" in text_lower:
        return "domiciliation_import"
    elif "bulletin de paie" in text_lower or "bulletin de salaire" in text_lower:
        return "bulletin_de_paie"
    elif "gestion de blocages" in text_lower or "blocage" in text_lower:
        return "gestion_de_blocages"
    return "unknown"

# --------------------------
# Helper: LLM extraction (UPDATED TO USE build_prompt)
# --------------------------
def llm_extract(text: str, fields: list, is_bank: bool = False) -> dict:
    """Call LLM to extract fields from text - ALWAYS returns dict"""
    if not fields:
        return {}
    
    # Use your custom prompt builder
    prompt = build_prompt(text[:800], fields, is_bank)
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs, max_new_tokens=180, do_sample=False,
            use_cache=True, num_beams=1, pad_token_id=tokenizer.eos_token_id
        )
    
    result = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()
    
    # Parse pipe-separated values
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    extracted = {}
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            extracted[field] = None if val.lower() == "null" or val == "" else val
        else:
            extracted[field] = None
    
    return extracted

# --------------------------
# UNIFIED EXTRACTION FUNCTION
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    """
    Single function to extract from both ID cards and multi-page bank documents
    
    Input: ocr_pages = [{"page": 1, "text": "..."}, ...]
    Output: structured dict with all extracted fields
    """
    if not ocr_pages:
        raise ValueError("No OCR pages provided")
    
    # Detect document type from first page
    first_page_text = ocr_pages[0].get("text", "").lower()
    
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # ID CARD PATH
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    if any(k in first_page_text for k in ["بطاقة", "cni", "carte", "identité"]):
        print("Document type: ID Card\n")
        
        full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
        
        extracted = llm_extract(full_text, ID_FIELDS, is_bank=False)
        extracted["document_type"] = "بطاقة التعريف الوطنية"
        
        print("✓ ID card extraction complete\n")
        return extracted
    
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # BANK DOCUMENT PATH
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    elif any(k in first_page_text for k in ["ordre de virement", "virement", "banque", "swift"]):
        print(f"Document type: Bank Transfer ({len(ocr_pages)} pages)\n")
        
        all_data = {"document_type": "ORDRE DE VIREMENT A L'ETRANGER"}
        
        # Process each page
        for i, page in enumerate(ocr_pages):
            page_text = page.get("text", "")
            if not page_text:
                continue
            
            print(f"Page {i+1}:", end=" ")
            page_type = detect_page_type(page_text)
            
            if page_type == "unknown":
                print("⚠ Unknown page type, skipping")
                continue
            
            if page_type == "gestion_de_blocages":
                print(f"{page_type} (skipped)")
                continue
            
            # Get fields for this page type
            fields = PAGE_SCHEMAS.get(page_type, [])
            if not fields:
                print(f"{page_type} (no fields defined)")
                continue
            
            # Extract (pass is_bank=True for bank-specific rules)
            is_bank_page = (page_type == "ordre_de_virement")
            page_data = llm_extract(page_text, fields, is_bank=is_bank_page)
            
            # Verify it's a dict
            if not isinstance(page_data, dict):
                print(f"⚠ Error: expected dict, got {type(page_data)}")
                continue
            
            # Merge into main dict (don't overwrite existing values)
            for key, value in page_data.items():
                if key not in all_data or all_data[key] is None:
                    all_data[key] = value
            
            print(f"{page_type} → extracted {len(page_data)} fields")
        
        print("\n✓ Bank document extraction complete\n")
        return all_data
    
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # UNKNOWN DOCUMENT TYPE
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    else:
        raise ValueError("Unknown document type - could not detect ID card or bank document")

# --------------------------
# Run
# --------------------------
if __name__ == "__main__":
    # Load OCR output
    with open("extracted.json", "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)
    
    # Extract
    structured = extract_structured_data(ocr_pages)
    
    # Save JSON
    with open("structured.json", "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)
    
    print(json.dumps(structured, indent=2, ensure_ascii=False))















import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("✓ Model loaded.\n")

# --------------------------
# Field Schemas
# --------------------------
PAGE_SCHEMAS = {
    "ordre_de_virement": [
        "Monnaie", "Montant en chiffres", "Montant en lettres", "Agence",
        "Nom/Prénom", "Adresse complète", "Numéro de compte", "Date de valeur",
        "bank code", "Siège", "Racine", "Ordinal", "Devise", "Clé",
        "Adresse SWIFT", "Autre (Transfert en EURO)",
    ],
    "annexe_1": ["Nom", "Date de signature"],
    "annexe_2": ["Mois", "Part transférable", "Nom et prénom"],
    "domiciliation_import": ["Nom"],
    "bulletin_de_paie": ["Nom"],
    "gestion_de_blocages": [],
}

ID_FIELDS = [
    "اللقب", "الاسم", "تاريخ الميلاد", "الجنس",
    "رقم الوطني", "تاريخ الإصدار", "تاريخ الانتهاء",
    "سلطة الإصدار", "Nom", "Prénom"
]

# --------------------------
# Helper: Detect page type
# --------------------------
def detect_page_type(text: str) -> str:
    text_lower = text.lower()
    if "ordre de virement" in text_lower or "virement a l'etranger" in text_lower:
        return "ordre_de_virement"
    elif "annexe 1" in text_lower or "annexe i" in text_lower:
        return "annexe_1"
    elif "annexe 2" in text_lower or "annexe ii" in text_lower:
        return "annexe_2"
    elif "domiciliation" in text_lower and "import" in text_lower:
        return "domiciliation_import"
    elif "bulletin de paie" in text_lower or "bulletin de salaire" in text_lower:
        return "bulletin_de_paie"
    elif "gestion de blocages" in text_lower or "blocage" in text_lower:
        return "gestion_de_blocages"
    return "unknown"

# --------------------------
# Helper: LLM extraction
# --------------------------
def llm_extract(text: str, fields: list) -> dict:
    """Call LLM to extract fields from text"""
    if not fields:
        return {}
    
    messages = [
        {"role": "system", "content": "Extract document fields. Output ONLY pipe-separated values, no field names."},
        {"role": "user", "content": f"""Extract these fields in order:
{' | '.join(fields)}

Rules:
- Return ONLY values like: value1 | value2 | value3 | ...
- Use "null" for missing fields
- No explanations, no field names

Document:
{text[:800]}"""}
    ]
    
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs, max_new_tokens=180, do_sample=False,
            use_cache=True, num_beams=1, pad_token_id=tokenizer.eos_token_id
        )
    
    result = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    extracted = {}
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            extracted[field] = None if val.lower() == "null" or val == "" else val
        else:
            extracted[field] = None
    
    return extracted

# --------------------------
# UNIFIED EXTRACTION FUNCTION
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    """
    Single function to extract from both ID cards and multi-page bank documents
    
    Input: ocr_pages = [{"page": 1, "text": "..."}, ...]
    Output: structured dict with all extracted fields
    """
    if not ocr_pages:
        raise ValueError("No OCR pages provided")
    
    # Detect document type from first page
    first_page_text = ocr_pages[0].get("text", "").lower()
    
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # ID CARD PATH (single page or simple multi-page)
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    if any(k in first_page_text for k in ["بطاقة", "cni", "carte", "identité"]):
        print("Document type: ID Card\n")
        
        # Merge all pages text
        full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
        
        # Extract
        extracted = llm_extract(full_text, ID_FIELDS)
        extracted["document_type"] = "بطاقة التعريف الوطنية"
        
        print("✓ ID card extraction complete\n")
        return extracted
    
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # BANK DOCUMENT PATH (multi-page with page-specific fields)
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    elif any(k in first_page_text for k in ["ordre de virement", "virement", "banque", "swift"]):
        print(f"Document type: Bank Transfer ({len(ocr_pages)} pages)\n")
        
        all_data = {"document_type": "ORDRE DE VIREMENT A L'ETRANGER"}
        
        # Process each page
        for i, page in enumerate(ocr_pages):
            page_text = page.get("text", "")
            if not page_text:
                continue
            
            print(f"Page {i+1}:", end=" ")
            page_type = detect_page_type(page_text)
            
            if page_type == "unknown":
                print("⚠ Unknown page type, skipping")
                continue
            
            if page_type == "gestion_de_blocages":
                print(f"{page_type} (skipped)")
                continue
            
            # Get fields for this page type
            fields = PAGE_SCHEMAS.get(page_type, [])
            if not fields:
                print(f"{page_type} (no fields defined)")
                continue
            
            # Extract
            page_data = llm_extract(page_text, fields)
            
            # Merge into main dict (don't overwrite existing values)
            for key, value in page_data.items():
                if key not in all_data or all_data[key] is None:
                    all_data[key] = value
            
            print(f"{page_type} → extracted {len(page_data)} fields")
        
        print("\n✓ Bank document extraction complete\n")
        return all_data
    
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # UNKNOWN DOCUMENT TYPE
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    else:
        raise ValueError("Unknown document type - could not detect ID card or bank document")

# --------------------------
# Run
# --------------------------
if __name__ == "__main__":
    # Load OCR output
    with open("extracted.json", "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)
    
    # Extract (single function handles everything)
    structured = extract_structured_data(ocr_pages)
    
    # Save JSON
    with open("structured.json", "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)
    
    print(json.dumps(structured, indent=2, ensure_ascii=False))















import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("✓ Model loaded.\n")

# --------------------------
# Page-specific field schemas
# --------------------------
PAGE_SCHEMAS = {
    "ordre_de_virement": [
        "Monnaie",
        "Montant en chiffres",
        "Montant en lettres",
        "Agence",
        "Nom/Prénom",
        "Adresse complète",
        "Numéro de compte",
        "Date de valeur",
        "bank code",
        "Siège",
        "Racine",
        "Ordinal",
        "Devise",
        "Clé",
        "Adresse SWIFT",
        "Autre (Transfert en EURO)",
    ],
    "annexe_1": [
        "Nom",
        "Date de signature",
    ],
    "annexe_2": [
        "Mois",
        "Part transférable",
        "Nom et prénom",
    ],
    "domiciliation_import": [
        "Nom",
        # Add other fields you need from this page
    ],
    "bulletin_de_paie": [
        "Nom",
        # Add other fields you need from this page
    ],
    "gestion_de_blocages": [
        # No extraction needed
    ],
}

ID_FIELDS = [
    "اللقب", "الاسم", "تاريخ الميلاد", "الجنس",
    "رقم الوطني", "تاريخ الإصدار", "تاريخ الانتهاء",
    "سلطة الإصدار", "Nom", "Prénom"
]

# --------------------------
# Detect page type by content
# --------------------------
def detect_page_type(text: str) -> str:
    """Detect what type of page this is based on text content"""
    text_lower = text.lower()
    
    # Check for specific page identifiers
    if "ordre de virement" in text_lower or "virement a l'etranger" in text_lower:
        return "ordre_de_virement"
    elif "annexe 1" in text_lower or "annexe i" in text_lower:
        return "annexe_1"
    elif "annexe 2" in text_lower or "annexe ii" in text_lower:
        return "annexe_2"
    elif "domiciliation" in text_lower and "import" in text_lower:
        return "domiciliation_import"
    elif "bulletin de paie" in text_lower or "bulletin de salaire" in text_lower:
        return "bulletin_de_paie"
    elif "gestion de blocages" in text_lower or "blocage" in text_lower:
        return "gestion_de_blocages"
    
    return "unknown"

# --------------------------
# Detect document type (bank vs ID)
# --------------------------
def detect_document_type(ocr_pages: list) -> str:
    """Detect if this is a bank document or ID card"""
    # Check first page
    first_page_text = ocr_pages[0].get("text", "").lower() if ocr_pages else ""
    
    if any(k in first_page_text for k in ["بطاقة", "cni", "carte", "identité"]):
        return "id_card"
    elif any(k in first_page_text for k in ["ordre de virement", "virement", "banque", "swift"]):
        return "bank"
    
    return "unknown"

# --------------------------
# Extract from a single page
# --------------------------
def extract_page(page_text: str, page_type: str) -> dict:
    """Extract fields from a single page"""
    if page_type == "gestion_de_blocages" or page_type == "unknown":
        return {}
    
    fields = PAGE_SCHEMAS.get(page_type, [])
    if not fields:
        return {}
    
    # Build prompt
    messages = [
        {
            "role": "system",
            "content": "Extract document fields. Output ONLY pipe-separated values, no field names."
        },
        {
            "role": "user",
            "content": f"""Extract these fields in order:
{' | '.join(fields)}

Rules:
- Return ONLY values like: value1 | value2 | value3 | ...
- Use "null" for missing fields
- No explanations, no field names

Document:
{page_text[:800]}"""
        }
    ]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    print(f"  Page type: {page_type}")
    print(f"  Raw output: {result[:100]}...")
    
    # Parse pipe-separated values
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    page_data = {}
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            page_data[field] = None if val.lower() == "null" or val == "" else val
        else:
            page_data[field] = None
    
    return page_data

# --------------------------
# Extract ID card
# --------------------------
def extract_id_card(ocr_pages: list) -> dict:
    """Extract ID card (single page or multi-page)"""
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
    
    messages = [
        {
            "role": "system",
            "content": "Extract document fields. Output ONLY pipe-separated values, no field names."
        },
        {
            "role": "user",
            "content": f"""Extract these fields in order:
{' | '.join(ID_FIELDS)}

Rules:
- Return ONLY values like: value1 | value2 | value3 | ...
- Use "null" for missing fields
- No explanations, no field names

Document:
{full_text[:800]}"""
        }
    ]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    values = [v.strip() for v in result.split("|")]
    
    structured = {"document_type": "بطاقة التعريف الوطنية"}
    for i, field in enumerate(ID_FIELDS):
        if i < len(values):
            val = values[i]
            structured[field] = None if val.lower() == "null" or val == "" else val
        else:
            structured[field] = None
    
    return structured

# --------------------------
# Extract multi-page bank document
# --------------------------
def extract_bank_document(ocr_pages: list) -> dict:
    """Extract bank document with page-specific fields"""
    all_data = {"document_type": "ORDRE DE VIREMENT A L'ETRANGER"}
    
    print(f"Processing {len(ocr_pages)} pages...\n")
    
    for i, page in enumerate(ocr_pages):
        page_text = page.get("text", "")
        if not page_text:
            continue
        
        print(f"Page {i+1}:")
        page_type = detect_page_type(page_text)
        
        if page_type == "unknown":
            print(f"  ⚠ Could not detect page type, skipping\n")
            continue
        
        page_data = extract_page(page_text, page_type)
        
        # Merge into main dict (avoid overwriting)
        for key, value in page_data.items():
            if key not in all_data or all_data[key] is None:
                all_data[key] = value
        
        print(f"  ✓ Extracted {len(page_data)} fields\n")
    
    return all_data

# --------------------------
# Main extraction function
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    """Main entry point: detect document type and extract accordingly"""
    doc_type = detect_document_type(ocr_pages)
    
    if doc_type == "id_card":
        print("Document type: ID Card\n")
        return extract_id_card(ocr_pages)
    elif doc_type == "bank":
        print("Document type: Bank Transfer (Multi-page)\n")
        return extract_bank_document(ocr_pages)
    else:
        raise ValueError("Unknown document type")

# --------------------------
# Run
# --------------------------
if __name__ == "__main__":
    # Load OCR output
    with open("extracted.json", "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)
    
    # Extract
    structured = extract_structured_data(ocr_pages)
    
    # Save JSON
    with open("structured.json", "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)
    
    print("\n✓ Extraction complete!\n")
    print(json.dumps(structured, indent=2, ensure_ascii=False))


















from concurrent.futures import ProcessPoolExecutor
import os
import json

INPUT_FOLDER = "/mnt/ocr_outputs"

def process_file(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    structured = extract_structured_data(ocr_pages)
    return {
        "file": os.path.basename(file_path),
        "extracted": structured
    }

if __name__ == "__main__":

    json_files = [
        os.path.join(INPUT_FOLDER, f)
        for f in os.listdir(INPUT_FOLDER)
        if f.endswith(".json")
    ]

    with ProcessPoolExecutor() as executor:
        results = list(executor.map(process_file, json_files))

    print(json.dumps(results, indent=2, ensure_ascii=False))



















# Split the string into lines
lines = structured.split("\n")

# Create dictionary
data_dict = {}
for line in lines:
    if ":" in line:
        key, value = line.split(":", 1)  # split at first colon
        data_dict[key.strip()] = value.strip()

print(data_dict)
import pandas as pd

# If you have multiple pages, you can wrap them in a list
df = pd.DataFrame([data_dict])  # wrap in list for single row

# Save to Excel
df.to_excel("payment.xlsx", index=False, encoding="utf-8")








import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"
torch.set_num_threads(8)

print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("✓ Model loaded.\n")

# --------------------------
# Schemas
# --------------------------
BANK_FIELDS = [
    "Monnaie",
    "Montant en chiffres",
    "Montant en lettres",
    "Agence",
    "Nom/Prénom",
    "Nature du paiement",
    "Adresse complète",
    "Numéro de compte",
    "Date de valeur",
    "Siège",
    "Racine",
    "Ordinal",
    "Devise",
    "Clé",
    "Adresse SWIFT",
    "Autre (Transfert en EURO)",
]

ID_FIELDS = [
    "اللقب", "الاسم", "تاريخ الميلاد", "الجنس",
    "رقم الوطني", "تاريخ الإصدار", "تاريخ الانتهاء",
    "سلطة الإصدار", "Nom", "Prénom"
]

def detect_document_type(text: str) -> str:
    if any(k in text for k in ["ORDRE", "virement", "Banque", "SWIFT"]):
        return "bank"
    elif any(k in text for k in ["بطاقة", "CNI", "Carte"]):
        return "id"
    return "unknown"

# --------------------------
# FIXED PROMPT (uses chat template properly)
# --------------------------
def build_prompt(text: str, doc_type: str) -> str:
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    
    # Qwen chat format
    messages = [
        {
            "role": "system",
            "content": "Extract document fields. Output ONLY pipe-separated values, no field names."
        },
        {
            "role": "user",
            "content": f"""Extract these fields in order:
{' | '.join(fields)}

Rules:
- Return ONLY values like: value1 | value2 | value3 | ...
- Use "null" for missing fields
- No explanations, no field names

Document:
{text[:800]}"""
        }
    ]
    
    return tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

# --------------------------
# Extract
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))
    doc_type = detect_document_type(full_text)
    
    if doc_type == "unknown":
        raise ValueError("Unknown document type")
    
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    prompt = build_prompt(full_text, doc_type)
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1200).to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            use_cache=True,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(
        outputs[0, inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    print(f"Raw output:\n{result}\n")
    
    # Parse pipe-separated values
    values = [v.strip() for v in result.split("|")]
    
    # Build dict
    doc_type_name = "ORDRE DE VIREMENT A L'ETRANGER" if doc_type == "bank" else "بطاقة التعريف الوطنية"
    structured = {"document_type": doc_type_name}
    
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            structured[field] = None if val.lower() == "null" or val == "" else val
        else:
            structured[field] = None
    
    return structured

# --------------------------
# Run
# --------------------------
if __name__ == "__main__":
    # Load your extracted.json
    with open("extracted.json", "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)
    
    structured = extract_structured_data(ocr_pages)
    
    # Save
    with open("structured.json", "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)
    
    print(json.dumps(structured, indent=2, ensure_ascii=False))












import zipfile
import os

folder_path = "dossier"
zip_path = "excel_files.zip"

with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for file in os.listdir(folder_path):
        if file.endswith(".xlsx"):
            zipf.write(os.path.join(folder_path, file),
                       arcname=file)

print("ZIP file created successfully!")







structured = {}

for i in range(len(fields)):
    if i < len(values):
        val = values[i].strip()
        structured[fields[i]] = None if val.lower() == "null" else val
    else:
        structured[fields[i]] = None







structured = {}

for i, field in enumerate(fields):
    if i < len(values):
        structured[field] = values[i]
    else:
        structured[field] = None




import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --------------------------
# Config
# --------------------------
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen3-4B-Instruct-2507/main"
DEVICE = "cpu"  # CPU only
torch.set_num_threads(8)  # adjust to your CPU cores

# Load model
print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map=None,
    trust_remote_code=True,
)
model.eval()
print("✓ Model loaded.\n")


# --------------------------
# Document type detection
# --------------------------
def detect_document_type(text: str) -> str:
    """Detect document type from OCR text"""
    if any(k in text for k in ["ORDRE DE VIREMENT", "virement", "Banque"]):
        return "bank"
    elif any(k in text for k in ["بطاقة", "CNI", "Carte"]):
        return "id"
    else:
        return "unknown"


# --------------------------
# Field Schemas
# --------------------------
BANK_FIELDS = [
    "Normale",
    "Montant en chiffres",
    "Montant en lettres",
    "Agence",
    "Nom/Prénom",
    "Nature du paiement",
    "Adresse complète",
    "Numéro de compte",
    "Date de valeur",
    "Siège",
    "Racine",
    "Ordinal",
    "Devise",
    "Clé",
    "Adresse SWIFT",
    "Autre (Transfert en EURO)",
]

ID_FIELDS = [
    "اللقب",
    "الاسم",
    "التاريخ الميلاد",
    "الجنس",
    "رقم الوطني",
    "تاريخ الإصدار",
    "تاريخ الانتهاء",
    "سلطة الإصدار",
    "Prénom",
]


# --------------------------
# Prompt Builder
# --------------------------
def build_prompt(text: str, doc_type: str) -> str:
    """Build CPU-friendly minimal prompt"""
    if doc_type == "bank":
        order = " | ".join(BANK_FIELDS)
    else:
        order = " | ".join(ID_FIELDS)

    prompt = f"""
You are an expert document information extraction system.
Extract the following fields in EXACT order separated by "|":

{order}

Return ONLY values separated by "|".
If a value is missing, return null.
No explanations.

OCR:
{text}
"""
    return prompt.strip()


# --------------------------
# Extraction Function
# --------------------------
def extract_structured_data(ocr_pages: list) -> dict:
    """
    Input: ocr_pages = [{"page": 1, "text": "..."}, ...]
    Output: structured dict with all fields
    """
    # Merge OCR text
    full_text = "\n".join(p.get("text", "") for p in ocr_pages if p.get("text"))

    # Detect document type
    doc_type = detect_document_type(full_text)
    if doc_type == "unknown":
        raise ValueError("Unknown document type.")

    # Build prompt
    prompt = build_prompt(full_text, doc_type)

    # Tokenize & generate
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=800).to(DEVICE)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=120,  # CPU-friendly
            do_sample=False,
            use_cache=True,
            num_beams=1,
        )

    # Decode output
    generated = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True).strip()

    # Split pipe-separated values
    values = [v.strip() if v.strip().lower() != "null" else None for v in generated.split("|")]

    # Map to fields
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    structured = dict(zip(fields, values))

    # Add document type
    structured["document_type"] = doc_type

    return structured


# --------------------------
# Example Usage
# --------------------------
if __name__ == "__main__":
    # Example OCR input
    ocr_pages = [
        {"page": 1, "text": "ORDRE DE VIREMENT A L'ETRANGER Date: 05/02/2026 Beneficiaire: John Doe Amount: 1000 EUR Account: FR761234567890 Swift: ABCDFRPP Bank: BNP Paribas Agency: Algiers Nature: Salary"},
    ]

    structured = extract_structured_data(ocr_pages)
    print(json.dumps(structured, indent=2, ensure_ascii=False))

















import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# ── Config ────────────────────────────────────────────────────────────────────
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

# ── Load model ────────────────────────────────────────────────────────────────
print("Loading model...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if device == "cuda" else torch.float32

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=dtype,
    device_map="auto" if device == "cuda" else None,
    trust_remote_code=True,
)
if device == "cpu":
    model = model.to("cpu")
    torch.set_num_threads(8)

model.eval()
print(f"✓ Loaded on {device} in {time()-t0:.1f}s\n")

# ── Schema per document type ──────────────────────────────────────────────────
def get_schema(document_type: str) -> dict:
    if document_type == "bank":
        return {
            "document_type": "ORDRE DE VIREMENT A L'ETRANGER",
            "Monnaie": None,
            "Montant en chiffres": None,
            "Montant en lettres": None,
            "Agence": None,
            "Nom/Prénom": None,
            "Nature du paiement": None,
            "Adresse complète": None,
            "Numéro de compte": None,
            "Date de valeur": None,
            "Siège": None,      # 5 digits
            "Racine": None,     # 6 digits
            "Ordinal": None,    # 3 digits
            "Devise": None,     # letters
            "Clé": None,        # 2 digits
            "Adresse SWIFT": None,
            "Autre (Transfert en EURO)": None,
        }
    elif document_type == "id_card":
        return {
            "document_type": "بطاقة التعريف الوطنية",
            "اللقب": None,
            "الاسم": None,
            "تاريخ الميلاد": None,
            "الجنس": None,
            "رقم الوطني": None,
            "تاريخ الإصدار": None,
            "تاريخ الانتهاء": None,
            "سلطة الإصدار": None,
            "Nom": None,
            "Prénom": None,
        }
    else:
        raise ValueError(f"Unknown document type: {document_type}")

# ── Detect document type ──────────────────────────────────────────────────────
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["بطاقة", "التعريف", "الوطنية", "carte nationale", "cni", "identité"]):
        return "id_card"
    return "unknown"

# ── Parse JSON from output ────────────────────────────────────────────────────
def parse_json_from_output(text: str) -> dict:
    if "<|im_start|>assistant" in text:
        text = text.split("<|im_start|>assistant")[-1]
    
    # Try ```json block
    json_match = re.search(r'```json\s*(\{.*?)```', text, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(1))
        except json.JSONDecodeError:
            pass
    
    # Try incomplete ```json block
    json_match = re.search(r'```json\s*(\{.*)', text, re.DOTALL)
    if json_match:
        json_str = json_match.group(1).strip()
        if json_str.count('{') > json_str.count('}'):
            json_str += '}'
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass
    
    # Fallback: find { ... }
    start = text.find('{')
    end = text.rfind('}') + 1
    if start != -1 and end > start:
        json_str = text[start:end]
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            if json_str.count('{') > json_str.count('}'):
                json_str += '}'
            return json.loads(json_str)
    
    raise ValueError(f"No valid JSON found in model output:\n{text[:500]}")

# ── Extract structured data ───────────────────────────────────────────────────
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))[:1000]  # truncate
    document_type = detect_document_type(full_text)
    schema = get_schema(document_type)
    
    # Extract only field names (keys), not the full schema with None values
    field_names = [k for k in schema.keys() if k != "document_type"]
    
    # OPTIMIZED PROMPT - Much shorter
    prompt = f"""<|im_start|>system
Extract fields to JSON. Return only values, no field names repeated.
<|im_end|>
<|im_start|>user
Fields: {', '.join(field_names)}

Text:
{full_text}

Return JSON with these fields. Use null if missing.
<|im_end|>
<|im_start|>assistant
```json
{{
  "document_type": "{schema['document_type']}"
"""

    print(f"Document type: {document_type}")
    print(f"Prompt tokens: ~{len(tokenizer.encode(prompt))}\n")
    
    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1500).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=250,  # reduced from 350-500
            do_sample=False,
            num_beams=1,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    gen_time = time() - t0
    
    print(f"⏱ Generation: {gen_time:.2f}s")
    print(f"Raw output:\n{result[:200]}...\n")
    
    # Complete the JSON if needed
    complete_json = '{\n  "document_type": "' + schema['document_type'] + '"\n' + result
    
    return parse_json_from_output(complete_json)

# ── Run ───────────────────────────────────────────────────────────────────────
if __name__ == "__main__":
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages\n")

    t_start = time()
    structured = extract_structured_data(ocr_pages)
    
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"\n✓ Done in {time()-t_start:.1f}s!\n")
    print(json.dumps(structured, indent=2, ensure_ascii=False))












import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# ── Config ────────────────────────────────────────────────────────────────────
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

# ── Load model ────────────────────────────────────────────────────────────────
print("Loading model...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if device == "cuda" else torch.float32

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=dtype,
    device_map="auto" if device == "cuda" else None,
    trust_remote_code=True,
)
if device == "cpu":
    model = model.to("cpu")
    torch.set_num_threads(8)

model.eval()
print(f"✓ Loaded on {device} in {time()-t0:.1f}s\n")

# ── Field order (important!) ──────────────────────────────────────────────────
BANK_FIELDS = [
    "Monnaie",
    "Montant en chiffres", 
    "Montant en lettres",
    "Agence",
    "Nom/Prénom",
    "Nature du paiement",
    "Adresse complète",
    "Numéro de compte",
    "Date de valeur",
    "Siège",
    "Racine",
    "Ordinal",
    "Devise",
    "Clé",
    "Adresse SWIFT",
    "Autre (Transfert en EURO)"
]

ID_FIELDS = [
    "اللقب",
    "الاسم",
    "تاريخ الميلاد",
    "الجنس",
    "رقم الوطني",
    "تاريخ الإصدار",
    "تاريخ الانتهاء",
    "سلطة الإصدار",
    "Nom",
    "Prénom"
]

# ── Detect document type ──────────────────────────────────────────────────────
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["بطاقة", "التعريف", "الوطنية", "carte nationale", "cni", "identité"]):
        return "id_card"
    return "unknown"

# ── Extract with values-only prompt ───────────────────────────────────────────
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))[:1200]
    doc_type = detect_document_type(full_text)
    
    if doc_type == "unknown":
        raise ValueError("Could not detect document type")
    
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    field_list = "\n".join([f"{i+1}. {field}" for i, field in enumerate(fields)])
    
    if doc_type == "bank":
        example = '"020", "318060.13", "trois cent dix-huit mille soixante virgule treize Dinars", "MOSTAGANEM", "ALI SAYED HASSNAT", "virement de trésorerie", "HARATI, PATS, PASCHIM BARDHAMAN, DURGAPUR, WEST BENGAL PIN 713102", "50100692", null, "97000", "675432", "052", "070", "92", "ABCDINDR", null'
    else:
        example = '"شيال", "محمد", "1956", "ذكر", "100581262011435400", "20180121", "20280121", null, "CHIAL", "M HAMED"'
    
    prompt = f"""<|im_start|>system
Extract document fields. Return ONLY values as a comma-separated list in quotes. No field names.
<|im_end|>
<|im_start|>user
Extract these fields IN ORDER:
{field_list}

Rules:
- Return ONLY values in quotes: "value1", "value2", "value3", ...
- If field is missing or empty, write null (no quotes)
- Do NOT include field names
- Do NOT explain anything

Example output format:
{example}

Document text:
{full_text}
<|im_end|>
<|im_start|>assistant
"""

    print(f"Document type: {doc_type}")
    
    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1800).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=False,
            num_beams=1,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    gen_time = time() - t0
    
    print(f"⏱ Generation: {gen_time:.2f}s")
    print(f"Raw output:\n{result}\n")
    
    # Parse comma-separated values
    result = result.strip()
    if result.startswith('"') or result.startswith("'"):
        # Use regex to split by commas outside quotes
        values = re.findall(r'"([^"]*)"|\bnull\b', result)
        values = [None if v == '' and 'null' in result[i:i+10] else v for i, v in enumerate(values)]
    else:
        # Fallback: simple split
        values = [v.strip().strip('"').strip("'") for v in result.split(",")]
        values = [None if v.lower() == "null" or v == "" else v for v in values]
    
    # Build result dict
    doc_type_name = "ORDRE DE VIREMENT A L'ETRANGER" if doc_type == "bank" else "بطاقة التعريف الوطنية"
    structured = {"document_type": doc_type_name}
    
    for i, field in enumerate(fields):
        structured[field] = values[i] if i < len(values) else None
    
    return structured

# ── Run ───────────────────────────────────────────────────────────────────────
if __name__ == "__main__":
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages\n")

    t_start = time()
    structured = extract_structured_data(ocr_pages)
    
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"\n✓ Done in {time()-t_start:.1f}s!\n")
    print(json.dumps(structured, indent=2, ensure_ascii=False))








import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# ── Config ────────────────────────────────────────────────────────────────────
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

# ── Load model (optimized for CPU if needed) ─────────────────────────────────
print("Loading model...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

# Check if GPU is available
device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if device == "cuda" else torch.float32

if device == "cpu":
    torch.set_num_threads(8)  # adjust to your CPU cores

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=dtype,
    device_map="auto" if device == "cuda" else None,
    trust_remote_code=True,
)
if device == "cpu":
    model = model.to("cpu")
    
model.config.use_cache = True
model.eval()
print(f"✓ Loaded on {device} in {time()-t0:.1f}s\n")

# ── Field definitions (order matters!) ───────────────────────────────────────
BANK_FIELDS = [
    "Monnaie",
    "Montant en chiffres",
    "Montant en lettres",
    "Agence",
    "Nom/Prénom",
    "Nature du paiement",
    "Adresse complète",
    "Numéro de compte",
    "Date de valeur",
    "Siège",      # 5 digits
    "Racine",     # 6 digits
    "Ordinal",    # 3 digits
    "Devise",     # letters
    "Clé",        # 2 digits
    "Adresse SWIFT",
    "Autre (Transfert en EURO)"
]

ID_FIELDS = [
    "اللقب",
    "الاسم",
    "تاريخ الميلاد",
    "الجنس",
    "رقم الوطني",
    "تاريخ الإصدار",
    "تاريخ الانتهاء",
    "سلطة الإصدار",
    "Nom",
    "Prénom"
]

# ── Detect document type ──────────────────────────────────────────────────────
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["بطاقة", "التعريف", "الوطنية", "carte nationale", "cni", "identité"]):
        return "id_card"
    return "unknown"

# ── Build minimal prompt ──────────────────────────────────────────────────────
def build_prompt(text: str, fields: list, doc_type: str) -> str:
    field_list = " | ".join(fields)
    
    if doc_type == "bank":
        notes = "Note: Siège=5 digits, Racine=6 digits, Ordinal=3 digits, Devise=letters, Clé=2 digits."
    else:
        notes = ""
    
    prompt = f"""<|im_start|>system
Extract document fields.
<|im_end|>
<|im_start|>user
Extract these fields in EXACT order, separated by "|":

{field_list}

Return ONLY values separated by "|". If missing, write "null". No explanation.
{notes}

Text:
{text[:1000]}
<|im_end|>
<|im_start|>assistant
"""
    return prompt

# ── Parse pipe-separated output ───────────────────────────────────────────────
def parse_output(text: str, fields: list, doc_type: str) -> dict:
    # Clean output
    text = text.strip()
    if "<|im_start|>assistant" in text:
        text = text.split("<|im_start|>assistant")[-1].strip()
    
    # Split by pipe
    values = [v.strip() for v in text.split("|")]
    
    # Build dict
    result = {"document_type": "ORDRE DE VIREMENT A L'ETRANGER" if doc_type == "bank" else "بطاقة التعريف الوطنية"}
    
    for i, field in enumerate(fields):
        if i < len(values):
            val = values[i]
            result[field] = None if val.lower() == "null" or val == "" else val
        else:
            result[field] = None
    
    return result

# ── Extract ───────────────────────────────────────────────────────────────────
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))
    doc_type = detect_document_type(full_text)
    
    if doc_type == "unknown":
        raise ValueError("Could not detect document type")
    
    fields = BANK_FIELDS if doc_type == "bank" else ID_FIELDS
    prompt = build_prompt(full_text, fields, doc_type)
    
    print(f"Document: {doc_type}")
    print(f"Prompt length: {len(tokenizer.encode(prompt))} tokens\n")
    
    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1536).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=120,  # ↓ from 350 — enough for pipe-separated
            do_sample=False,
            num_beams=1,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    result = tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    gen_time = time() - t0
    
    print(f"⏱ Generation: {gen_time:.2f}s")
    print(f"Raw output: {result}\n")
    
    return parse_output(result, fields, doc_type)

# ── Run ───────────────────────────────────────────────────────────────────────
if __name__ == "__main__":
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages\n")

    t_start = time()
    structured = extract_structured_data(ocr_pages)
    
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"✓ Done in {time()-t_start:.1f}s!\n")
    print(json.dumps(structured, indent=2, ensure_ascii=False))













import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# ── Use 0.5B model (4x faster than 1.5B) ─────────────────────────────────────
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-0.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

print("Loading 0.5B model (fast)...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
)
model.eval()
print(f"✓ Loaded in {time()-t0:.1f}s\n")

def detect_document_type(text: str) -> str:
    return "id_card" if any(k in text for k in ["بطاقة", "CNI", "Carte"]) else "bank"

SCHEMAS = {
    "bank": {"Montant": None, "Nom": None, "Compte": None},
    "id_card": {"اللقب": None, "الاسم": None, "رقم الوطني": None, "Nom": None, "Prénom": None}
}

def parse_json(text: str) -> dict:
    match = re.search(r'\{[^{}]*\}', text)
    return json.loads(match.group(0)) if match else {}

def extract_structured_data(ocr_pages: list) -> dict:
    text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))[:600]
    doc_type = detect_document_type(text)
    schema = SCHEMAS[doc_type]

    prompt = f"""Extract to JSON:
{json.dumps(schema, ensure_ascii=False)}

{text}

{{"""

    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt", max_length=800, truncation=True).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=150, do_sample=False, use_cache=True)
    
    result = "{" + tokenizer.decode(outputs[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    print(f"⏱ {time()-t0:.2f}s | {result[:100]}...")
    
    return parse_json(result)

if __name__ == "__main__":
    with open(INPUT_JSON, "r") as f:
        ocr_pages = json.load(f)

    structured = extract_structured_data(ocr_pages)
    
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"\n✓ Done!\n{json.dumps(structured, indent=2, ensure_ascii=False)}")






import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from time import time

# ── Config ────────────────────────────────────────────────────────────────────
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"
OUTPUT_JSON = "structured.json"

# ── Load model ────────────────────────────────────────────────────────────────
print("Loading tokenizer and model...")
t0 = time()
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map="auto",
    trust_remote_code=True,
)
model.eval()
print(f"Model loaded in {time()-t0:.1f}s\n")

# ── Detect document type ──────────────────────────────────────────────────────
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["بطاقة", "التعريف", "الوطنية", "carte nationale", "cni", "identité"]):
        return "id_card"
    return "unknown"

# ── Schema per document type ──────────────────────────────────────────────────
def get_schema(document_type: str) -> dict:
    if document_type == "bank":
        return {
            "document_type": "ORDRE DE VIREMENT A L'ETRANGER",
            "Monnaie": None,
            "Montant en chiffres": None,
            "Montant en lettres": None,
            "Agence": None,
            "Nom/Prénom": None,
            "Nature du paiement": None,
            "Adresse complète": None,
            "Numéro de compte": None,
            "Date de valeur": None,
            "Siège": None,
            "Racine": None,
            "Ordinal": None,
            "Devise": None,
            "Clé": None,
            "Adresse SWIFT": None,
            "Autre (Transfert en EURO)": None,
        }
    elif document_type == "id_card":
        return {
            "document_type": "بطاقة التعريف الوطنية",
            "اللقب": None,
            "الاسم": None,
            "تاريخ الميلاد": None,
            "الجنس": None,
            "رقم الوطني": None,
            "تاريخ الإصدار": None,
            "تاريخ الانتهاء": None,
            "سلطة الإصدار": None,
            "Nom": None,
            "Prénom": None,
        }
    else:
        raise ValueError(f"Unknown document type: {document_type}")

# ── Clean and parse JSON from model output ────────────────────────────────────
def parse_json_from_output(text: str) -> dict:
    if "<|im_start|>assistant" in text:
        text = text.split("<|im_start|>assistant")[-1]

    json_match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
    if json_match:
        return json.loads(json_match.group(1))

    start = text.find("{")
    end   = text.rfind("}") + 1
    if start != -1 and end > start:
        return json.loads(text[start:end])

    raise ValueError(f"No valid JSON found in model output:\n{text[:500]}")

# ── Main extraction function ──────────────────────────────────────────────────
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))
    document_type = detect_document_type(full_text)
    schema = get_schema(document_type)

    # Shorter, more direct prompt
    prompt = f"""<|im_start|>system
Extract document fields into JSON. Return only valid JSON, no explanation.
<|im_end|>
<|im_start|>user
Schema:
{json.dumps(schema, ensure_ascii=False, indent=2)}

Text:
{full_text[:2000]}
<|im_end|>
<|im_start|>assistant
"""

    print(f"Document type: {document_type}")
    print(f"Prompt length: {len(prompt)} chars\n")
    
    t0 = time()
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    prep_time = time() - t0
    print(f"⏱ Tokenization: {prep_time:.2f}s")

    t0 = time()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=400,        # ↓ from 600
            do_sample=False,
            num_beams=1,               # explicit greedy decode
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    gen_time = time() - t0
    print(f"⏱ Generation: {gen_time:.2f}s")

    generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    print(f"⏱ Total: {prep_time + gen_time:.2f}s\n")
    print(f"── Raw output ──\n{result[:300]}...\n────────────────\n")

    return parse_json_from_output(result)

# ── Run ───────────────────────────────────────────────────────────────────────
if __name__ == "__main__":
    t_start = time()
    
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages from {INPUT_JSON}\n")

    structured = extract_structured_data(ocr_pages)

    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    t_total = time() - t_start
    print(f"✓ Done in {t_total:.1f}s! Structured data saved to {OUTPUT_JSON}\n")
    print(json.dumps(structured, indent=4, ensure_ascii=False))










import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ── Config ────────────────────────────────────────────────────────────────────
MODEL_NAME = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-1.5B-Instruct/main"
INPUT_JSON = "extracted.json"   # output from the OCR step
OUTPUT_JSON = "structured.json"

# ── Load model ────────────────────────────────────────────────────────────────
print("Loading tokenizer and model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map="auto",
    trust_remote_code=True,
)
model.eval()
print("Model loaded.")

# ── Detect document type ──────────────────────────────────────────────────────
def detect_document_type(text: str) -> str:
    text_lower = text.lower()
    if any(kw in text_lower for kw in ["virement", "swift", "montant", "agence", "ordre"]):
        return "bank"
    if any(kw in text for kw in ["بطاقة", "التعريف", "الوطنية", "carte nationale", "cni", "identité"]):
        return "id_card"
    return "unknown"

# ── Schema per document type ──────────────────────────────────────────────────
def get_schema(document_type: str) -> dict:
    if document_type == "bank":
        return {
            "document_type": "ORDRE DE VIREMENT A L'ETRANGER",
            "Monnaie": None,
            "Montant en chiffres": None,
            "Montant en lettres": None,
            "Agence": None,
            "Nom/Prénom": None,
            "Nature du paiement": None,
            "Adresse complète": None,
            "Numéro de compte": None,
            "Date de valeur": None,
            "Siège": None,
            "Racine": None,
            "Ordinal": None,
            "Devise": None,
            "Clé": None,
            "Adresse SWIFT": None,
            "Autre (Transfert en EURO)": None,
        }
    elif document_type == "id_card":
        return {
            "document_type": "بطاقة التعريف الوطنية",
            "اللقب": None,
            "الاسم": None,
            "تاريخ الميلاد": None,
            "الجنس": None,
            "رقم الوطني": None,
            "تاريخ الإصدار": None,
            "تاريخ الانتهاء": None,
            "سلطة الإصدار": None,
            "Nom": None,
            "Prénom": None,
        }
    else:
        raise ValueError(f"Unknown document type: {document_type}")

# ── Clean and parse JSON from model output ────────────────────────────────────
def parse_json_from_output(text: str) -> dict:
    # Strip the prompt echo if present (keep only after last assistant turn)
    if "<|im_start|>assistant" in text:
        text = text.split("<|im_start|>assistant")[-1]

    # Try to find a JSON block (handles ```json ... ``` or raw {...})
    json_match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
    if json_match:
        return json.loads(json_match.group(1))

    # Fallback: find first { ... } block
    start = text.find("{")
    end   = text.rfind("}") + 1
    if start != -1 and end > start:
        return json.loads(text[start:end])

    raise ValueError(f"No valid JSON found in model output:\n{text[:500]}")

# ── Main extraction function ──────────────────────────────────────────────────
def extract_structured_data(ocr_pages: list) -> dict:
    full_text = "\n".join(p["text"] for p in ocr_pages if p.get("text"))
    document_type = detect_document_type(full_text)
    schema = get_schema(document_type)

    prompt = f"""<|im_start|>system
You are an expert document information extraction system.
The document language may be French, Arabic, or both.
The OCR text may contain noise or formatting errors — correct them carefully.
Return ONLY valid JSON matching the schema exactly.
Do not explain anything. If a field is missing, use null.
<|im_end|>
<|im_start|>user
Schema:
{json.dumps(schema, ensure_ascii=False, indent=2)}

OCR TEXT:
{full_text}
<|im_end|>
<|im_start|>assistant
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=600,  # increased slightly for 1.5B model
            do_sample=False,
            temperature=None,
            top_p=None,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Decode only newly generated tokens
    generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
    result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    print(f"\n── Raw model output ──\n{result}\n─────────────────────\n")

    return parse_json_from_output(result)

# ── Run ───────────────────────────────────────────────────────────────────────
if __name__ == "__main__":
    with open(INPUT_JSON, "r", encoding="utf-8") as f:
        ocr_pages = json.load(f)

    print(f"Loaded {len(ocr_pages)} pages from {INPUT_JSON}")

    structured = extract_structured_data(ocr_pages)

    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(structured, f, indent=4, ensure_ascii=False)

    print(f"\n✓ Done! Structured data saved to {OUTPUT_JSON}")
    print(json.dumps(structured, indent=4, ensure_ascii=False))







import torch
import json
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM

# =========================================
# 1️⃣ LOAD MODEL
# =========================================

model_name = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-32B-Instruct/main"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# =========================================
# 2️⃣ MERGE OCR PAGES
# =========================================

def merge_ocr_pages(ocr_output):
    full_text = ""
    for page in ocr_output:
        full_text += page["text"] + "\n"
    return full_text


# =========================================
# 3️⃣ DOCUMENT TYPE DETECTION
# =========================================

def detect_document_type(text):

    text_lower = text.lower()

    if "virement" in text_lower and "etranger" in text_lower:
        return "bank"

    elif "التعريف" in text and "الوطنية" in text:
        return "id"

    else:
        return "unknown"


# =========================================
# 4️⃣ STRUCTURED EXTRACTION
# =========================================

def extract_structured_data(ocr_output):

    full_text = merge_ocr_pages(ocr_output)

    document_type = detect_document_type(full_text)

    if document_type == "bank":
        schema = """
{
  "document_type": "foreign_transfer_order",
  "date": "",
  "amount": "",
  "currency": "",
  "beneficiary_name": "",
  "account_number": "",
  "swift_code": "",
  "bank_name": "",
  "agency": "",
  "nature_of_payment": ""
}
"""
    elif document_type == "id":
        schema = """
{
  "document_type": "algerian_id_card",
  "full_name": "",
  "date_of_birth": "",
  "place_of_birth": "",
  "id_number": "",
  "expiration_date": "",
  "nationality": "",
  "blood_group": ""
}
"""
    else:
        raise ValueError("❌ Unknown document type")

    prompt = f"""
You are an expert document information extraction system.

The OCR text may contain noise or formatting errors.
Extract the correct structured data carefully.

Return ONLY valid JSON.
Do not explain anything.

If a field is missing, return null.

Schema:
{schema}

OCR TEXT:
{full_text}
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=500,
        temperature=0.1
    )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

    json_start = result.find("{")
    json_end = result.rfind("}") + 1
    clean_json = result[json_start:json_end]

    return json.loads(clean_json)


# =========================================
# 5️⃣ EXECUTION
# =========================================

# Example OCR output
ocr_output = [
    {"page": 1, "text": "ORDRE DE VIREMENT A L'ETRANGER ..."},
    {"page": 2, "text": "Beneficiaire ..."}
]

structured_data = extract_structured_data(ocr_output)

df = pd.DataFrame([structured_data])

print(df)

df.to_excel("structured_output.xlsx", index=False)

print("✅ Structured extraction completed")




















OCR text → Structured JSON (via Qwen2.5) → Pandas Table → Excel export




import torch
import json
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from pdf2image import convert_from_path
import pytesseract

# =========================================
# 1️⃣ LOAD MODEL
# =========================================

model_name = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-Coder-32B-Instruct/main"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# =========================================
# 2️⃣ OCR FUNCTION
# =========================================

def extract_text_from_scanned_pdf(pdf_path, lang="eng"):
    pages = convert_from_path(pdf_path)
    full_text = ""

    for page in pages:
        text = pytesseract.image_to_string(page, lang=lang)
        full_text += text + "\n"

    return full_text


# =========================================
# 3️⃣ AUTO DOCUMENT TYPE DETECTION
# =========================================

def detect_document_type(ocr_text):

    text_lower = ocr_text.lower()

    if "ordre de virement" in text_lower:
        return "bank"

    elif "بطاقة التعريف الوطنية" in ocr_text:
        return "id"

    else:
        return "unknown"


# =========================================
# 4️⃣ STRUCTURED EXTRACTION
# =========================================

def extract_structured_data(ocr_text):

    document_type = detect_document_type(ocr_text)

    if document_type == "bank":
        schema = """
{
  "document_type": "foreign_transfer_order",
  "date": "",
  "amount": "",
  "currency": "",
  "beneficiary_name": "",
  "account_number": "",
  "swift_code": "",
  "bank_name": "",
  "agency": "",
  "nature_of_payment": ""
}
"""

    elif document_type == "id":
        schema = """
{
  "document_type": "algerian_id_card",
  "full_name": "",
  "date_of_birth": "",
  "place_of_birth": "",
  "id_number": "",
  "expiration_date": "",
  "nationality": "",
  "blood_group": ""
}
"""
    else:
        raise ValueError("❌ Unknown document type")

    prompt = f"""
You are an expert information extraction system.

The OCR text may contain noise.
Extract the correct structured data.

Return ONLY valid JSON.
Do not explain anything.

If a field is missing, return null.

Schema:
{schema}

OCR TEXT:
{ocr_text}
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=500,
        temperature=0.1
    )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

    json_start = result.find("{")
    json_end = result.rfind("}") + 1
    clean_json = result[json_start:json_end]

    return json.loads(clean_json)


# =========================================
# 5️⃣ MAIN
# =========================================

pdf_path = "your_scanned_document.pdf"

# OCR
ocr_text = extract_text_from_scanned_pdf(pdf_path, lang="fra")  # change if needed

# Auto extraction
structured_data = extract_structured_data(ocr_text)

# Convert to table
df = pd.DataFrame([structured_data])

print(df)

df.to_excel("structured_output.xlsx", index=False)

print("✅ Auto detection + extraction completed")






from pypdf import PdfReader, PdfWriter

input_pdf = "input.pdf"        # your original PDF
output_pdf = "first_page.pdf"  # new PDF with only page 1

reader = PdfReader(input_pdf)
writer = PdfWriter()

# Add first page (index 0)
writer.add_page(reader.pages[0])

# Save as new PDF
with open(output_pdf, "wb") as f:
    writer.write(f)

print("First page saved as PDF successfully!")






print(type(image))
print(image.size)
image.save("/mnt/test_page.png")

def extract_text(image):

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": "Extract all visible text from this French document page. Return plain text only."}
            ],
        }
    ]

    # Build proper multimodal prompt
    prompt = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = processor(
        text=[prompt],        # MUST be list
        images=[image],       # MUST be list
        padding=True,
        return_tensors="pt"
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048
        )

    result = processor.batch_decode(
        output, skip_special_tokens=True
    )[0]

    return result









import os
import json
import torch
import fitz  # PyMuPDF
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq

# Paths
MODEL_PATH = "/domina/cdv/modelhub/Modelhub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"

# Load model and processor
processor = AutoProcessor.from_pretrained(
    MODEL_PATH, 
    trust_remote_code=True
)

model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,
    device_map="cpu",
    trust_remote_code=True
)

model.eval()

def pdf_to_images(pdf_path, save_dir="/mnt/pdf_pages", dpi=300):
    """Convert PDF pages to images"""
    os.makedirs(save_dir, exist_ok=True)
    
    doc = fitz.open(pdf_path)
    images = []
    
    for i, page in enumerate(doc):
        # Render page to pixmap
        pix = page.get_pixmap(dpi=dpi)
        
        # Convert to PIL Image
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Save image
        img_path = f"{save_dir}/page_{i+1}.png"
        img.save(img_path)
        print(f"Saved page {i+1}: {img_path} ({img.width}x{img.height})")
        
        images.append(img)
    
    doc.close()
    return images

def extract_text(image):
    """Extract text from image using Qwen2.5-VL"""
    
    # Qwen2.5-VL specific prompt format
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text", 
                    "text": "Extract ALL visible text from this document page. Preserve the structure and formatting. Return plain text only."
                }
            ],
        }
    ]
    
    # Apply chat template
    text = processor.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    # Process inputs
    inputs = processor(
        text=[text],
        images=[image],
        padding=True,
        return_tensors="pt"
    )
    
    # Generate
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=4096,
            temperature=0.1,
            do_sample=False
        )
    
    # Decode output
    generated_ids = [
        output_ids[len(input_ids):]
        for input_ids, output_ids in zip(inputs.input_ids, output_ids)
    ]
    
    result = processor.batch_decode(
        generated_ids, 
        skip_special_tokens=True, 
        clean_up_tokenization_spaces=False
    )[0]
    
    return result

# Main execution
def main():
    OUTPUT_IMG_DIR = "/mnt/pdf_pages"
    
    # Create output directory
    if not os.path.exists(OUTPUT_IMG_DIR):
        os.makedirs(OUTPUT_IMG_DIR)
    
    # Convert PDF to images
    print(f"Converting PDF: {PDF_PATH}")
    images = pdf_to_images(PDF_PATH, save_dir=OUTPUT_IMG_DIR, dpi=300)
    
    # Extract text from each page
    all_text = []
    for i, img in enumerate(images):
        print(f"\nProcessing page {i+1}/{len(images)}")
        try:
            page_text = extract_text(img)
            all_text.append({
                "page": i+1,
                "text": page_text
            })
            print(f"✓ Page {i+1} completed")
        except Exception as e:
            print(f"✗ Error on page {i+1}: {str(e)}")
            all_text.append({
                "page": i+1,
                "text": f"ERROR: {str(e)}"
            })
    
    # Save results
    output_file = "extracted.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_text, f, indent=4, ensure_ascii=False)
    
    print(f"\n✓ All text saved to {output_file}")

if __name__ == "__main__":
    main()








import os
import json
import torch
import fitz  # PyMuPDF
from PIL import Image
from transformers import AutoProcessor, Qwen2VLForConditionalGeneration

# Paths
MODEL_PATH = "/domina/cdv/modelhub/Modelhub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"

# Load model and processor
processor = AutoProcessor.from_pretrained(
    MODEL_PATH, 
    trust_remote_code=True
)

model = Qwen2VLForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,  # Use float16 instead of float32 for better performance
    device_map="auto",  # Auto device mapping
    trust_remote_code=True
)

def pdf_to_images(pdf_path, save_dir="/mnt/pdf_pages", dpi=300):
    """Convert PDF pages to images"""
    os.makedirs(save_dir, exist_ok=True)
    
    doc = fitz.open(pdf_path)
    images = []
    
    for i, page in enumerate(doc):
        # Render page to pixmap
        pix = page.get_pixmap(dpi=dpi)
        
        # Convert to PIL Image
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Save image
        img_path = f"{save_dir}/page_{i+1}.png"
        img.save(img_path)
        print(f"Saved page {i+1}: {img_path} ({img.width}x{img.height})")
        
        images.append(img)
    
    doc.close()
    return images

def extract_text(image):
    """Extract text from image using Qwen2.5-VL"""
    
    # Qwen2.5-VL specific prompt format
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text", 
                    "text": "Extract ALL visible text from this document page. Preserve the structure and formatting. Return plain text only."
                }
            ],
        }
    ]
    
    # Apply chat template
    text = processor.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    # Process inputs
    inputs = processor(
        text=[text],
        images=[image],
        padding=True,
        return_tensors="pt"
    )
    
    # Move inputs to same device as model
    inputs = inputs.to(model.device)
    
    # Generate
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=4096,
            temperature=0.1,
            do_sample=False
        )
    
    # Decode output
    generated_ids = [
        output_ids[len(input_ids):]
        for input_ids, output_ids in zip(inputs.input_ids, output_ids)
    ]
    
    result = processor.batch_decode(
        generated_ids, 
        skip_special_tokens=True, 
        clean_up_tokenization_spaces=False
    )[0]
    
    return result

# Main execution
def main():
    OUTPUT_IMG_DIR = "/mnt/pdf_pages"
    
    # Create output directory
    if not os.path.exists(OUTPUT_IMG_DIR):
        os.makedirs(OUTPUT_IMG_DIR)
    
    # Convert PDF to images
    print(f"Converting PDF: {PDF_PATH}")
    images = pdf_to_images(PDF_PATH, save_dir=OUTPUT_IMG_DIR, dpi=300)
    
    # Extract text from each page
    all_text = []
    for i, img in enumerate(images):
        print(f"\nProcessing page {i+1}/{len(images)}")
        try:
            page_text = extract_text(img)
            all_text.append({
                "page": i+1,
                "text": page_text
            })
            print(f"✓ Page {i+1} completed")
        except Exception as e:
            print(f"✗ Error on page {i+1}: {str(e)}")
            all_text.append({
                "page": i+1,
                "text": f"ERROR: {str(e)}"
            })
    
    # Save results
    output_file = "extracted.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_text, f, indent=4, ensure_ascii=False)
    
    print(f"\n✓ All text saved to {output_file}")

if __name__ == "__main__":
    main()




import fitz  # PyMuPDF
from PIL import Image
import torch
from transformers import DonutProcessor, VisionEncoderDecoderModel
import json
import os

# =====================
# CONFIG
# =====================
MODEL_NAME = "naver-clova-ix/donut-base-finetuned-docvqa"  # Donut model
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"
OUTPUT_DIR = "/mnt/pdf_images"  # where we save images to check readability
os.makedirs(OUTPUT_DIR, exist_ok=True)

# =====================
# Load Donut Model
# =====================
processor = DonutProcessor.from_pretrained(MODEL_NAME)
model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)
model.eval()

# =====================
# Convert PDF → Images
# =====================
def pdf_to_images(pdf_path, dpi=400):
    doc = fitz.open(pdf_path)
    images = []
    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        image_path = os.path.join(OUTPUT_DIR, f"page_{i+1}.png")
        img.save(image_path)  # Save image so you can check
        images.append(img)
        print(f"Saved page {i+1} image at: {image_path}")
    return images

# =====================
# Extract text from image
# =====================
def extract_text(image):
    # Donut expects a "task prompt" to describe what you want to extract
    prompt = "<s_docvqa> Please extract all text from this page. </s_docvqa>"
    pixel_values = processor(image, return_tensors="pt").pixel_values

    with torch.no_grad():
        output_ids = model.generate(pixel_values, max_length=4096)
    
    result = processor.batch_decode(output_ids, skip_special_tokens=True)[0]
    return result

# =====================
# MAIN
# =====================
images = pdf_to_images(PDF_PATH, dpi=400)

all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}...")
    page_text = extract_text(img)
    all_text.append({
        "page": i+1,
        "text": page_text
    })

# Save results
with open("/mnt/extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("Done! Text extracted to /mnt/extracted.json")















def pdf_to_model_images(pdf_path, size=1024):
    import os
    import fitz
    from PIL import Image

    save_dir = "/mnt/pdf_pages"
    os.makedirs(save_dir, exist_ok=True)
    doc = fitz.open(pdf_path)
    images = []

    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)  # keep 300 DPI, not too big
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        # Resize to model-friendly square
        img.thumbnail((size, size), Image.LANCZOS)
        square_img = Image.new("RGB", (size, size), "white")
        square_img.paste(img, (0, 0))

        img_path = f"{save_dir}/page_{i+1}.png"
        square_img.save(img_path)
        print(f"Saved {img_path} ({square_img.width}x{square_img.height})")
        images.append(square_img)

    return images





def pdf_to_images_readable(pdf_path, save_dir="/mnt/pdf_pages", dpi=400):
    os.makedirs(save_dir, exist_ok=True)
    doc = fitz.open(pdf_path)
    images = []

    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        # Pad to square without resizing smaller
        max_side = max(img.width, img.height)
        img_square = Image.new("RGB", (max_side, max_side), "white")
        img_square.paste(img, (0, 0))

        img_path = f"{save_dir}/page_{i+1}.png"
        img_square.save(img_path)
        print(f"Saved readable page {i+1} image: {img_path} ({img_square.width}x{img_square.height})")
        images.append(img_square)

    return images




def pdf_to_images(pdf_path, save_dir="/mnt/pdf_pages"):
    doc = fitz.open(pdf_path)
    images = []
    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        # --- Resize and convert to RGB for Qwen ---
        img = img.convert("RGB")
        img = img.resize((512, 512))  # adjust size if needed

        images.append(img)

        # Save the image to check
        img_path = f"{save_dir}/page_{i+1}.png"
        img.save(img_path)
        print(f"✓ Saved page {i+1} as image: {img_path}")

    return images






OUTPUT_IMG_DIR = "/mnt/pdf_pages"

# Make sure output folder exists
if not os.path.exists(OUTPUT_IMG_DIR):
    os.makedirs(OUTPUT_IMG_DIR)

img_path = f"{OUTPUT_IMG_DIR}/page_{i+1}.png"





OUTPUT_IMG_DIR = "/mnt/user-data/pdf_pages"  # folder to save images
Path(OUTPUT_IMG_DIR).mkdir(parents=True, exist_ok=True)


def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)

        # Save the image to disk for inspection
        img_path = Path(OUTPUT_IMG_DIR) / f"page_{i+1}.png"
        img.save(img_path)
        print(f"✓ Saved page {i+1} as image: {img_path}")

    return images











def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.

<image>
"""  # <-- crucial marker

    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result





import fitz  # PyMuPDF
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq
import torch
import json

# =====================
# CONFIG
# =====================
MODEL_PATH = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"

# =====================
# LOAD MODEL
# =====================
print("Loading model...")
processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,  # CPU safe
    trust_remote_code=True
)
model.eval()
print("Model loaded.")

# =====================
# PDF → Images
# =====================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page in doc:
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# =====================
# Extract text from an image
# =====================
def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.
"""

    # Prepare input
    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    )

    # Forward pass
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0
        )

    # Decode result
    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# =====================
# MAIN
# =====================
images = pdf_to_images(PDF_PATH)

all_text = []
for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}")
    page_text = extract_text(img)
    all_text.append({
        "page": i+1,
        "text": page_text
    })

# Save results
with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("✅ Done! Text saved to extracted.json")













import fitz  # PyMuPDF
from PIL import Image
import torch
from transformers import AutoProcessor, AutoModelForVision2Seq
import json

# =====================
# CONFIG
# =====================
MODEL_PATH = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"
DEVICE = "cpu"  # force CPU

# =====================
# Load Qwen Model
# =====================
print("Loading model...")
processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,  # CPU safe
    trust_remote_code=True
).to(DEVICE)
model.eval()
print("Model loaded.")

# =====================
# PDF → Images
# =====================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page in doc:
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# =====================
# Extract Text per Page
# =====================
def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.
"""
    # Prepare inputs
    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    ).to(DEVICE)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# =====================
# Main
# =====================
images = pdf_to_images(PDF_PATH)
all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}")
    page_text = extract_text(img)
    all_text.append({
        "page": i+1,
        "text": page_text
    })

# Save JSON
with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("✅ Done! Saved to extracted.json")









import fitz  # PyMuPDF
from PIL import Image
import torch
from transformers import AutoProcessor, AutoModelForVision2Seq
import json

# =====================
# CONFIG
# =====================
MODEL_PATH = "/domino/edv/modelhub/ModelHub-model-huggingface-Qwen/Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"
DEVICE = "cpu"  # force CPU

# =====================
# Load Qwen Model
# =====================
print("Loading model...")
processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,  # CPU safe
    trust_remote_code=True
).to(DEVICE)
model.eval()
print("Model loaded.")

# =====================
# PDF → Images
# =====================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page in doc:
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# =====================
# Extract Text per Page
# =====================
def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.
"""
    # Prepare inputs
    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    ).to(DEVICE)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# =====================
# Main
# =====================
images = pdf_to_images(PDF_PATH)
all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)}")
    page_text = extract_text(img)
    all_text.append({
        "page": i+1,
        "text": page_text
    })

# Save JSON
with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("✅ Done! Saved to extracted.json")







import fitz  # PyMuPDF
from PIL import Image
import easyocr
import json

PDF_PATH = "/mnt/bareme-de-remboursement-cardif.pdf"

# Initialize EasyOCR reader (French + English)
reader = easyocr.Reader(['fr', 'en'], gpu=False)

# Convert PDF → Images
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page in doc:
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# OCR each image
def extract_text_from_images(images):
    results = []
    for i, img in enumerate(images):
        print(f"Processing page {i+1}/{len(images)}")
        ocr_result = reader.readtext(np.array(img), detail=0)  # detail=0 → just text
        page_text = "\n".join(ocr_result)
        results.append({"page": i+1, "text": page_text})
    return results

# Main
images = pdf_to_images(PDF_PATH)
all_text = extract_text_from_images(images)

# Save JSON
with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("✅ Done! Saved to extracted.json")






pip install --upgrade --no-cache-dir torch

pip install --upgrade --no-cache-dir transformers>=4.40.0



import torch
import fitz
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq
import json

# =====================
# CONFIG
# =====================
MODEL_PATH = "/domino/edv/modelhub/.../Qwen2.5-VL-7B-Instruct/main"
PDF_PATH = "/mnt/your_file.pdf"

# =====================
# Load Model (CPU ONLY)
# =====================
print("Loading model on CPU...")

processor = AutoProcessor.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True
)

model = AutoModelForVision2Seq.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float32,   # CPU safe
    trust_remote_code=True
)

model.eval()

print("Model loaded.")

# =====================
# Convert PDF → Images
# =====================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []

    for page in doc:
        pix = page.get_pixmap(dpi=200)  # lower DPI for CPU
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)

    return images

# =====================
# OCR Function
# =====================
def extract_text(image):
    prompt = """
Extract ALL visible text from this document page.
Preserve structure and formatting.
Return plain text only.
"""

    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=1024,   # smaller for CPU
            temperature=0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# =====================
# MAIN
# =====================
images = pdf_to_images(PDF_PATH)

all_text = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}/{len(images)} (CPU mode, may be slow)")
    page_text = extract_text(img)

    all_text.append({
        "page": i + 1,
        "text": page_text
    })

with open("extracted.json", "w", encoding="utf-8") as f:
    json.dump(all_text, f, indent=4, ensure_ascii=False)

print("Done. Saved to extracted.json")











from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "Qwen/Qwen2.5-3B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

inputs = tokenizer("Explain transformers briefly:", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))






from transformers import AutoProcessor, AutoModelForVision2Seq

model = AutoModelForVision2Seq.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    trust_remote_code=True
)

processor = AutoProcessor.from_pretrained(
    "Qwen/Qwen-VL-Chat",
    trust_remote_code=True
)

pip uninstall qwen -y
pip uninstall zeta -y

pip install transformers accelerate torch




Bonjour,

Veuillez trouver en pièce jointe les résultats de Sentinella pour le trimestre arrêté à fin février 2026.

Les données couvrent la période du 01/01/2024 au 28/02/2026 et présentent la répartition des communautés selon le nombre de clients ainsi que les montants par pays.

En résumé :

- 64 communautés au total
- Montant global : 5 346 762 €

La ventilation détaillée par rubrique et par pays est disponible dans le fichier joint.

Pour toute question ou besoin de complément d'information, Nihad prendra en charge la suite.

Cordialement,
Omar



#!/usr/bin/env python3
"""
Extract text from multi-page scanned PDFs using OpenAI GPT-4V (Vision).
Each PDF page is converted to an image and processed individually.
"""

import base64
from pathlib import Path
from typing import List, Dict
import fitz  # PyMuPDF
from openai import OpenAI


# -----------------------------
# 1️⃣ Convert PDF pages to images
# -----------------------------
def pdf_to_images(pdf_path: str) -> List[str]:
    """
    Convert each page of PDF to a PNG image and return list of image file paths.
    """
    pdf_file = Path(pdf_path)
    if not pdf_file.exists():
        raise FileNotFoundError(f"PDF not found: {pdf_path}")
    
    doc = fitz.open(pdf_file)
    images = []

    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)
        img_path = f"{pdf_file.stem}_page_{i+1}.png"
        pix.save(img_path)
        images.append(img_path)
    
    print(f"✓ Converted {len(images)} pages to images.")
    return images


# -----------------------------
# 2️⃣ Encode image to base64
# -----------------------------
def encode_image_to_base64(image_path: str) -> str:
    """
    Encode image to base64 for GPT-4V input.
    """
    path = Path(image_path)
    if not path.exists():
        raise FileNotFoundError(f"Image not found: {image_path}")

    with open(path, "rb") as f:
        encoded = base64.b64encode(f.read()).decode("utf-8")
    
    return encoded


# -----------------------------
# 3️⃣ Extract text from image using GPT-4V
# -----------------------------
def extract_text_gpt4v(api_key: str, base64_image: str, model: str = "gpt-4o") -> Dict:
    """
    Send image to GPT-4V and return extracted text.
    """
    client = OpenAI(api_key=api_key)

    # Default to JPEG MIME
    mime_type = "image/png"  # since we converted pages to PNG

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": """Extract ALL text from this image with maximum accuracy.
Preserve structure, tables, line breaks, and French text exactly.
Output markdown for tables."""
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:{mime_type};base64,{base64_image}"}
                    }
                ]
            }],
            max_tokens=4096,
            temperature=0
        )
        extracted = response.choices[0].message.content
        return {"success": True, "text": extracted, "tokens_used": response.usage.total_tokens}
    except Exception as e:
        return {"success": False, "error": str(e)}


# -----------------------------
# 4️⃣ Process multi-page PDF
# -----------------------------
def extract_text_from_pdf(api_key: str, pdf_path: str) -> str:
    """
    Convert PDF to images, extract text for each page, and combine results.
    """
    images = pdf_to_images(pdf_path)
    full_text = ""

    for i, img_path in enumerate(images):
        print(f"🔄 Processing page {i+1}/{len(images)}: {img_path}")
        b64 = encode_image_to_base64(img_path)
        result = extract_text_gpt4v(api_key, b64)

        if result["success"]:
            full_text += f"\n\n---\n\n## Page {i+1}\n\n"
            full_text += result["text"]
        else:
            full_text += f"\n\n---\n\n## Page {i+1}\n\n❌ Extraction failed: {result['error']}\n"
    
    return full_text


# -----------------------------
# 5️⃣ Save extracted text
# -----------------------------
def save_text(text: str, output_path: str):
    """
    Save extracted text to a file.
    """
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(text)
    print(f"✓ Extracted text saved to {output_path}")


# -----------------------------
# 6️⃣ Main function
# -----------------------------
def main():
    PDF_PATH = "/mnt/user-data/uploads/scanned_document.pdf"
    OUTPUT_PATH = "/mnt/user-data/outputs/extracted_text_gpt4v.md"
    OPENAI_API_KEY = input("Enter your OpenAI API key: ").strip()

    print("🔹 Starting PDF text extraction using GPT-4V...")
    extracted_text = extract_text_from_pdf(OPENAI_API_KEY, PDF_PATH)
    save_text(extracted_text, OUTPUT_PATH)
    print("✅ Extraction complete!")


if __name__ == "__main__":
    main()




#!/usr/bin/env python3
"""
Text extraction from images using Vision-Language Models (VLMs)
Supports: OpenAI GPT-4V, Anthropic Claude, Google Gemini
No OCR preprocessing required
"""

import os
import base64
from pathlib import Path
from typing import Optional, Dict, Any
import json

# Installation instructions:
# pip install openai anthropic google-generativeai pillow --break-system-packages

class VLMTextExtractor:
    """Extract text from images using various VLM models"""
    
    def __init__(self):
        self.image_path = None
        self.base64_image = None
    
    def load_image(self, image_path: str) -> None:
        """Load and encode image to base64"""
        self.image_path = Path(image_path)
        if not self.image_path.exists():
            raise FileNotFoundError(f"Image not found: {image_path}")
        
        with open(self.image_path, "rb") as image_file:
            self.base64_image = base64.b64encode(image_file.read()).decode('utf-8')
        
        print(f"✓ Loaded image: {self.image_path.name}")
    
    def extract_with_gpt4v(self, api_key: str, model: str = "gpt-4o") -> Dict[str, Any]:
        """
        Extract text using OpenAI GPT-4 Vision
        Models: gpt-4o, gpt-4o-mini, gpt-4-turbo
        """
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=api_key)
            
            # Determine image format
            ext = self.image_path.suffix.lower()
            mime_type = {
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.png': 'image/png',
                '.webp': 'image/webp',
                '.gif': 'image/gif'
            }.get(ext, 'image/jpeg')
            
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": """Extract ALL text from this image with maximum accuracy. 
                                
Instructions:
1. Preserve the exact structure and layout
2. Maintain all tables, headers, and formatting
3. Include ALL numerical values, percentages, and currency amounts
4. Keep French text exactly as shown
5. Preserve line breaks and spacing where meaningful
6. Output in markdown format for tables

Please extract the complete text."""
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:{mime_type};base64,{self.base64_image}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=4096,
                temperature=0
            )
            
            extracted_text = response.choices[0].message.content
            
            return {
                "success": True,
                "model": model,
                "text": extracted_text,
                "tokens_used": response.usage.total_tokens
            }
            
        except Exception as e:
            return {
                "success": False,
                "model": model,
                "error": str(e)
            }
    
    def extract_with_claude(self, api_key: str, model: str = "claude-3-5-sonnet-20241022") -> Dict[str, Any]:
        """
        Extract text using Anthropic Claude Vision
        Models: claude-3-5-sonnet-20241022, claude-3-opus-20240229, claude-3-haiku-20240307
        """
        try:
            from anthropic import Anthropic
            
            client = Anthropic(api_key=api_key)
            
            # Determine media type
            ext = self.image_path.suffix.lower()
            media_type = {
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.png': 'image/png',
                '.webp': 'image/webp',
                '.gif': 'image/gif'
            }.get(ext, 'image/jpeg')
            
            message = client.messages.create(
                model=model,
                max_tokens=4096,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": media_type,
                                    "data": self.base64_image,
                                },
                            },
                            {
                                "type": "text",
                                "text": """Extract ALL text from this image with maximum accuracy.

Instructions:
1. Preserve the exact structure and layout
2. Maintain all tables, headers, and formatting
3. Include ALL numerical values, percentages, and currency amounts
4. Keep French text exactly as shown
5. Preserve line breaks and spacing where meaningful
6. Output in markdown format for tables

Please extract the complete text."""
                            }
                        ],
                    }
                ],
            )
            
            extracted_text = message.content[0].text
            
            return {
                "success": True,
                "model": model,
                "text": extracted_text,
                "tokens_used": message.usage.input_tokens + message.usage.output_tokens
            }
            
        except Exception as e:
            return {
                "success": False,
                "model": model,
                "error": str(e)
            }
    
    def extract_with_gemini(self, api_key: str, model: str = "gemini-1.5-pro") -> Dict[str, Any]:
        """
        Extract text using Google Gemini Vision
        Models: gemini-1.5-pro, gemini-1.5-flash, gemini-pro-vision
        """
        try:
            import google.generativeai as genai
            from PIL import Image
            
            genai.configure(api_key=api_key)
            
            model_instance = genai.GenerativeModel(model)
            
            # Load image with PIL
            img = Image.open(self.image_path)
            
            prompt = """Extract ALL text from this image with maximum accuracy.

Instructions:
1. Preserve the exact structure and layout
2. Maintain all tables, headers, and formatting
3. Include ALL numerical values, percentages, and currency amounts
4. Keep French text exactly as shown
5. Preserve line breaks and spacing where meaningful
6. Output in markdown format for tables

Please extract the complete text."""
            
            response = model_instance.generate_content([prompt, img])
            
            return {
                "success": True,
                "model": model,
                "text": response.text,
                "tokens_used": "N/A (not provided by Gemini API)"
            }
            
        except Exception as e:
            return {
                "success": False,
                "model": model,
                "error": str(e)
            }
    
    def save_results(self, results: Dict[str, Any], output_path: str) -> None:
        """Save extraction results to file"""
        output_file = Path(output_path)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"# Text Extraction Results\n\n")
            f.write(f"**Source Image:** {self.image_path.name}\n")
            f.write(f"**Model:** {results['model']}\n")
            f.write(f"**Success:** {results['success']}\n")
            
            if results['success']:
                f.write(f"**Tokens Used:** {results.get('tokens_used', 'N/A')}\n\n")
                f.write("---\n\n")
                f.write("## Extracted Text\n\n")
                f.write(results['text'])
            else:
                f.write(f"**Error:** {results['error']}\n")
        
        print(f"✓ Results saved to: {output_file}")


def main():
    """Main execution function"""
    print("=" * 70)
    print("VLM Text Extraction Tool")
    print("=" * 70)
    print("\nSupported Models:")
    print("  1. OpenAI GPT-4V (gpt-4o, gpt-4o-mini)")
    print("  2. Anthropic Claude (claude-3-5-sonnet, claude-3-opus)")
    print("  3. Google Gemini (gemini-1.5-pro, gemini-1.5-flash)")
    print("\n")
    
    # Configuration
    IMAGE_PATH = "/mnt/user-data/uploads/1000024457.webp"
    
    # Initialize extractor
    extractor = VLMTextExtractor()
    extractor.load_image(IMAGE_PATH)
    
    print("\n" + "=" * 70)
    print("Select extraction method:")
    print("=" * 70)
    print("1. OpenAI GPT-4V")
    print("2. Anthropic Claude")
    print("3. Google Gemini")
    print("4. Extract with all models (requires all API keys)")
    print("\n")
    
    choice = input("Enter your choice (1-4): ").strip()
    
    results = []
    
    if choice == "1":
        api_key = input("\nEnter your OpenAI API key: ").strip()
        model = input("Enter model name (default: gpt-4o): ").strip() or "gpt-4o"
        print(f"\n🔄 Extracting text using {model}...")
        result = extractor.extract_with_gpt4v(api_key, model)
        results.append(("gpt4v", result))
        
    elif choice == "2":
        api_key = input("\nEnter your Anthropic API key: ").strip()
        model = input("Enter model name (default: claude-3-5-sonnet-20241022): ").strip() or "claude-3-5-sonnet-20241022"
        print(f"\n🔄 Extracting text using {model}...")
        result = extractor.extract_with_claude(api_key, model)
        results.append(("claude", result))
        
    elif choice == "3":
        api_key = input("\nEnter your Google API key: ").strip()
        model = input("Enter model name (default: gemini-1.5-pro): ").strip() or "gemini-1.5-pro"
        print(f"\n🔄 Extracting text using {model}...")
        result = extractor.extract_with_gemini(api_key, model)
        results.append(("gemini", result))
        
    elif choice == "4":
        openai_key = input("\nEnter your OpenAI API key: ").strip()
        claude_key = input("Enter your Anthropic API key: ").strip()
        gemini_key = input("Enter your Google API key: ").strip()
        
        print("\n🔄 Extracting with GPT-4V...")
        results.append(("gpt4v", extractor.extract_with_gpt4v(openai_key)))
        
        print("🔄 Extracting with Claude...")
        results.append(("claude", extractor.extract_with_claude(claude_key)))
        
        print("🔄 Extracting with Gemini...")
        results.append(("gemini", extractor.extract_with_gemini(gemini_key)))
    
    else:
        print("❌ Invalid choice!")
        return
    
    # Save results
    print("\n" + "=" * 70)
    print("Saving Results")
    print("=" * 70)
    
    for name, result in results:
        if result['success']:
            output_path = f"/mnt/user-data/outputs/extracted_text_{name}.md"
            extractor.save_results(result, output_path)
            print(f"✓ {name.upper()}: Extraction successful")
            print(f"  Preview: {result['text'][:100]}...")
        else:
            print(f"❌ {name.upper()}: Extraction failed - {result['error']}")
    
    print("\n✅ Process complete!")


if __name__ == "__main__":
    main()






import fitz  # PyMuPDF
from PIL import Image
from transformers import DonutProcessor, VisionEncoderDecoderModel

# -----------------------------
# 🔥 CHANGE THIS
# -----------------------------
MODEL_PATH = "/path_to_your_local_donut_model"
PDF_PATH = "your_scanned_file.pdf"

# -----------------------------
# 1️⃣ Convert PDF → Images
# -----------------------------
def pdf_to_images(pdf_path, dpi=200):
    doc = fitz.open(pdf_path)
    images = []

    for page in doc:
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)

    return images

# -----------------------------
# 2️⃣ Load Model (CPU Only)
# -----------------------------
print("Loading local Donut model (CPU mode)...")

processor = DonutProcessor.from_pretrained(MODEL_PATH, local_files_only=True)
model = VisionEncoderDecoderModel.from_pretrained(MODEL_PATH, local_files_only=True)
model.eval()

print("Model loaded successfully ✅")

# -----------------------------
# 3️⃣ Donut Extraction Function
# -----------------------------
def run_donut(image):
    # Resize for CPU stability
    image = image.resize((1280, 1280))

    prompt = (
        "<s_docvqa>"
        "<s_question>Extract all information from this document</s_question>"
        "<s_answer>"
    )

    pixel_values = processor(image, return_tensors="pt").pixel_values
    decoder_input_ids = processor.tokenizer(
        prompt,
        add_special_tokens=False,
        return_tensors="pt"
    ).input_ids

    outputs = model.generate(
        pixel_values,
        decoder_input_ids=decoder_input_ids,
        max_length=512,  # lower for CPU
        early_stopping=True,
        pad_token_id=processor.tokenizer.pad_token_id,
        eos_token_id=processor.tokenizer.eos_token_id,
    )

    result = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return result

# -----------------------------
# 4️⃣ Process PDF
# -----------------------------
pages = pdf_to_images(PDF_PATH)
results = []

for i, page in enumerate(pages):
    print(f"Processing page {i+1}/{len(pages)}")
    text = run_donut(page)
    results.append(text)

final_text = "\n\n".join(results)
print("\n========== FINAL RESULT ==========\n")
print(final_text)






import torch
import fitz  # PyMuPDF
from PIL import Image
from transformers import DonutProcessor, VisionEncoderDecoderModel

# -----------------------------
# 1️⃣ Convert PDF to Images
# -----------------------------
def pdf_to_images(pdf_path, dpi=300):
    doc = fitz.open(pdf_path)
    images = []

    for page in doc:
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)

    return images


# -----------------------------
# 2️⃣ Load Donut Model
# -----------------------------
model_name = "naver-clova-ix/donut-base"

processor = DonutProcessor.from_pretrained(model_name)
model = VisionEncoderDecoderModel.from_pretrained(model_name)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)


# -----------------------------
# 3️⃣ Run Donut on Image
# -----------------------------
def run_donut(image, task_prompt="<s_docvqa><s_question>Extract all information</s_question><s_answer>"):
    
    pixel_values = processor(image, return_tensors="pt").pixel_values.to(device)

    decoder_input_ids = processor.tokenizer(
        task_prompt,
        add_special_tokens=False,
        return_tensors="pt"
    ).input_ids.to(device)

    outputs = model.generate(
        pixel_values,
        decoder_input_ids=decoder_input_ids,
        max_length=1024,
        early_stopping=True,
        pad_token_id=processor.tokenizer.pad_token_id,
        eos_token_id=processor.tokenizer.eos_token_id,
    )

    result = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return result


# -----------------------------
# 4️⃣ Process Full PDF
# -----------------------------
pdf_path = "your_scanned.pdf"

images = pdf_to_images(pdf_path)

all_results = []

for i, img in enumerate(images):
    print(f"Processing page {i+1}...")
    result = run_donut(img)
    all_results.append(result)

# Combine all pages
final_output = "\n\n".join(all_results)

print("\n===== FINAL OUTPUT =====\n")
print(final_output)







pip install transformers torch torchvision pillow pymupdf sentencepiece



task_prompt = """
<s_docvqa>
<s_question>
Extract all structured information and return valid JSON.
</s_question>
<s_answer>
"""






pip install paddlepaddle paddleocr


import fitz
from PIL import Image

doc = fitz.open("your_scanned.pdf")
page = doc[0]

pix = page.get_pixmap(dpi=300)
img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
img.save("page.png")


from paddleocr import PaddleOCR

ocr = PaddleOCR(lang='en')
result = ocr.ocr("page.png")

text = "\n".join([line[1][0] for line in result[0]])
print(text)





import fitz
from PIL import Image

def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for i, page in enumerate(doc):
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

images = pdf_to_images("scanned.pdf")





pip uninstall qwen zeta transformers torch triton -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install qwen==0.1.1 qwen-agent qwen-vl-utils
pip install transformers==4.39.1 triton==2.2.0


# Uninstall potentially conflicting packages
pip uninstall qwen zeta -y

# Install PyTorch first (if not already installed)
pip install torch torchvision --break-system-packages

# Install the correct Qwen package
pip install transformers --break-system-packages
pip install qwen-vl-utils --break-system-packages




pip install torch torchvision torchaudio  # Core PyTorch
pip install pillow                         # Image handling
pip install pymupdf                        # PDF → images
pip install qwen                           # Official Qwen VL package




pip install tqdm                            # Progress bars
pip install numpy                           # Array operations (often required by Qwen)
pip install transformers                     # Only if you need extra transformer utils


pip install tqdm                            # Progress bars
pip install numpy                           # Array operations (often required by Qwen)
pip install transformers                     # Only if you need extra transformer utils






import fitz  # PyMuPDF
import json
from PIL import Image
from qwen import QwenVL, QwenProcessor  # official Qwen VL loader

# ===============================
# CONFIG
# ===============================
MODEL_PATH = "/path/to/Qwen2.5-VL-32B-Instruct"  # your local folder
PDF_PATH = "scanned.pdf"
OUTPUT_FILE = "output.json"

# ===============================
# LOAD MODEL
# ===============================
print("Loading Qwen2.5-VL-32B-Instruct...")
processor = QwenProcessor.from_pretrained(MODEL_PATH)
model = QwenVL.from_pretrained(MODEL_PATH)
model.eval()  # CPU by default
print("Model loaded successfully.")

# ===============================
# CONVERT PDF TO IMAGES
# ===============================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page_number in range(len(doc)):
        page = doc[page_number]
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# ===============================
# EXTRACT JSON FROM IMAGE
# ===============================
def extract_json_from_image(image):
    prompt = """
You are a document extraction engine.

Extract ALL visible text and structure from this scanned document.

Return strictly valid JSON only.
Do not add explanations or summaries.
"""

    # Preprocess the image + prompt
    inputs = processor(prompt, image, return_tensors="pt")

    # Generate the model output
    with torch.no_grad():
        output = model.generate(**inputs, max_new_tokens=2048, temperature=0.0)

    # Decode text
    text_output = processor.decode(output[0])
    return text_output

# ===============================
# MAIN PIPELINE
# ===============================
def main():
    images = pdf_to_images(PDF_PATH)
    all_pages = []

    for i, img in enumerate(images):
        print(f"Processing page {i+1}...")
        result_text = extract_json_from_image(img)

        # Attempt to parse JSON, fallback to raw output
        try:
            parsed = json.loads(result_text)
        except json.JSONDecodeError:
            parsed = {"raw_output": result_text}

        all_pages.append({
            "page_number": i + 1,
            "content": parsed
        })

    final_output = {
        "document_name": PDF_PATH,
        "total_pages": len(all_pages),
        "pages": all_pages
    }

    # Save to JSON file
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)

    print(f"Extraction complete. Saved to {OUTPUT_FILE}")

# ===============================
# ENTRY POINT
# ===============================
if __name__ == "__main__":
    main()




import fitz  # PyMuPDF
import json
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq

# ===============================
# CONFIG
# ===============================
MODEL_PATH = "/path/to/Qwen2.5-VL-32B-Instruct"  # Your local folder
PDF_PATH = "scanned.pdf"
OUTPUT_FILE = "output.json"

# ===============================
# Load Model
# ===============================
print("Loading model...")
processor = AutoProcessor.from_pretrained(MODEL_PATH)
model = AutoModelForVision2Seq.from_pretrained(MODEL_PATH)
model.eval()
print("Model loaded.")

# ===============================
# Convert PDF to Images
# ===============================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page_number in range(len(doc)):
        page = doc[page_number]
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# ===============================
# Extract JSON from Image
# ===============================
def extract_json_from_image(image):
    prompt = """
You are a document extraction engine.

Extract ALL visible text and structure from this scanned document.

Return strictly valid JSON.
Do not add explanations.
"""

    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0.0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# ===============================
# MAIN
# ===============================
def main():
    images = pdf_to_images(PDF_PATH)
    all_pages = []

    for i, img in enumerate(images):
        print(f"Processing page {i+1}...")
        result = extract_json_from_image(img)

        try:
            parsed = json.loads(result)
        except:
            parsed = {"raw_output": result}

        all_pages.append({
            "page_number": i + 1,
            "content": parsed
        })

    final_output = {
        "document_name": PDF_PATH,
        "total_pages": len(all_pages),
        "pages": all_pages
    }

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)

    print(f"Extraction complete. Saved to {OUTPUT_FILE}")

if __name__ == "__main__":
    main()





pip install torch torchvision transformers pymupdf pillow




if "chat_sessions" not in st.session_state:
    st.session_state.chat_sessions = load_chat_sessions()

import json
import os

CHAT_STORAGE_PATH = "chat_sessions.json"


def load_chat_sessions():
    if os.path.exists(CHAT_STORAGE_PATH):
        with open(CHAT_STORAGE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_chat_sessions(chat_sessions):
    with open(CHAT_STORAGE_PATH, "w", encoding="utf-8") as f:
        json.dump(chat_sessions, f, indent=2, ensure_ascii=False)









if st.sidebar.button("➕ Start New Chat"):
    chat_id = str(datetime.datetime.now())
    st.session_state.current_chat = chat_id
    st.session_state.chat_sessions[chat_id] = {
        "title": "New Chat",
        "messages": []
    }
    save_chat_sessions(st.session_state.chat_sessions)  # ✅ ADD THIS





chat_data["messages"].append({
    "role": "user",
    "content": user_input,
    "timestamp": timestamp
})

save_chat_sessions(st.session_state.chat_sessions)  # ✅ ADD THIS






import os
import glob

from langchain.document_loaders import (
    PyPDFLoader,
    CSVLoader,
    UnstructuredExcelLoader,
    UnstructuredWordDocumentLoader
)

# Your real documents directory
DOCS_DIR = "ant/"

def load_documents():
    documents = []

    procedures_dir = os.path.join(DOCS_DIR, "Procedures")

    # ---------- Load PDFs ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.pdf")):
            try:
                loader = PyPDFLoader(file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "pdf"

                documents.extend(docs)
                print(f"Loaded PDF: {file_name}")

            except Exception as e:
                print(f"Error loading PDF {file}: {e}")

    # ---------- Load CSVs ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.csv")):
            try:
                loader = CSVLoader(file_path=file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "csv"

                documents.extend(docs)
                print(f"Loaded CSV: {file_name}")

            except Exception as e:
                print(f"Error loading CSV {file}: {e}")

    # ---------- Load Excel ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.xlsx")):
            try:
                loader = UnstructuredExcelLoader(file_path=file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "excel"

                documents.extend(docs)
                print(f"Loaded Excel: {file_name}")

            except Exception as e:
                print(f"Error loading Excel {file}: {e}")

    # ---------- Load Word (.docx) ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.docx")):
            try:
                loader = UnstructuredWordDocumentLoader(file_path=file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "word"

                documents.extend(docs)
                print(f"Loaded Word: {file_name}")

            except Exception as e:
                print(f"Error loading Word {file}: {e}")

    print(f"\nLoaded {len(documents)} documents in total.")
    return documents













def main():
    st.title("💬 Chatbot BNP ED")

    # -----------------------------
    # 1️⃣ Session initialization
    # -----------------------------
    if "chat_sessions" not in st.session_state:
        st.session_state.chat_sessions = {}

    if "current_chat" not in st.session_state:
        chat_id = str(datetime.datetime.now())
        st.session_state.current_chat = chat_id
        st.session_state.chat_sessions[chat_id] = {
            "title": "New Chat",
            "messages": []
        }

    # -----------------------------
    # 2️⃣ Start new chat
    # -----------------------------
    if st.sidebar.button("➕ Start New Chat"):
        chat_id = str(datetime.datetime.now())
        st.session_state.current_chat = chat_id
        st.session_state.chat_sessions[chat_id] = {
            "title": "New Chat",
            "messages": []
        }

    # -----------------------------
    # 3️⃣ Chat selector
    # -----------------------------
    chat_ids = list(st.session_state.chat_sessions.keys())

    selected_chat = st.sidebar.radio(
        "Chat Sessions",
        chat_ids,
        format_func=lambda k: st.session_state.chat_sessions[k]["title"],
        index=chat_ids.index(st.session_state.current_chat)
        if st.session_state.current_chat in chat_ids else 0
    )

    st.session_state.current_chat = selected_chat
    chat_data = st.session_state.chat_sessions[selected_chat]

    # -----------------------------
    # 4️⃣ Display chat history
    # -----------------------------
    for message in chat_data["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"], unsafe_allow_html=True)

    # -----------------------------
    # 5️⃣ User input
    # -----------------------------
    user_input = st.chat_input("Type your message...")

    if user_input:
        timestamp = datetime.datetime.now().isoformat()

        # Rename chat on first message
        if chat_data["title"] == "New Chat":
            chat_data["title"] = user_input[:30] + "..."

        # Save user message
        chat_data["messages"].append({
            "role": "user",
            "content": user_input,
            "timestamp": timestamp
        })

        with st.chat_message("user"):
            st.markdown(user_input)

        # Generate assistant response
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = rag_chatbot(user_input)
                answer = response["answer"]
                st.markdown(answer, unsafe_allow_html=True)

        # Save assistant message
        chat_data["messages"].append({
            "role": "assistant",
            "content": answer,
            "timestamp": timestamp
        })










def get_chat_history(max_turns=6):
    messages = st.session_state.chat_sessions[
        st.session_state.current_chat
    ]["messages"]

    history = []
    for msg in messages[-max_turns:]:
        role = msg["role"].capitalize()
        history.append(f"{role}: {msg['content']}")

    return history

from openai import AzureOpenAI
import httpx


def generate_response(query, docs):
    context = build_context(docs)
    chat_history = get_chat_history()

    history_text = "\n".join(chat_history)

    prompt = f"""
Vous êtes un assistant RAG spécialisé dans les systèmes bancaires,
l’architecture de données et les produits financiers.

[Historique]
{history_text}

[Contexte]
{context}

[Question]
{query}

INSTRUCTIONS:
- Utilisez exclusivement le contexte
- Citez les sources: [Source: fichier.pdf, Page X]
- Si l'information n'existe pas, dites-le clairement
"""

    client = AzureOpenAI(
        api_key=AZURE_OPENAI_API_KEY,
        api_version=AZURE_OPENAI_API_VERSION,
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        http_client=httpx.Client(verify=False)
    )

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "Assistant RAG bancaire"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3
    )

    return response.choices[0].message.content


def rag_chatbot(query):
    chat_history = get_chat_history()

    retriever = CustomRetrieverWithHistory(
        collection=collection,
        chat_history=chat_history,
        k=2500,
        rerank_k=50
    )

    docs = retriever.get_relevant_documents(query)
    answer = generate_response(query, docs)

    return {
        "answer": answer,
        "documents": docs
    }














def generate_response(query, context):
    # Collect sources
    sources = list(set([chunk["metadata"]["source"] for chunk in context]))

    # Build prompt
    context_text = build_context(context)
    prompt = f"""
Vous êtes un assistant RAG spécialisé dans les systèmes bancaires,
l'architecture de données, et les produits financiers. Votre rôle est de répondre et d'expliquer clairement les
informations issues des documents, en les rendant compréhensibles pour tout type d'utilisateur, du plus général au plus technique.

[Contexte]
{context_text}

[Requête]
{query}

[INSTRUCTIONS GENERALES]
1. Utilisez exclusivement les informations présentes dans le contexte ci-dessus.
2. Fournissez une réponse claire, structurée et détaillée, adaptée à la compréhension de tout utilisateur.
3. Citez immédiatement la source sous le format [Source: <nom du fichier.pdf, Page <numéro>].
4. Si l'information n'existe pas dans les documents, indiquez-le clairement.
5. Structure attendue:
   a. Réponse détaillée
   b. Synthèse (si utile)
   c. Sources
6. Objectif: rendre la réponse informative, explicative et vérifiable.
"""

    # Initialize Azure/OpenAI client
    # Make sure the API key, endpoint, and version are correct
    import httpx
    from openai import OpenAI  # or your wrapper

    client = OpenAI(
        api_key="YOUR_API_KEY",
        api_base="YOUR_AZURE_ENDPOINT",
        api_type="azure",
        api_version="2023-03-15-preview"  # replace with your version
    )

    # Call the model
    completion = client.chat.completions.create(
        model="gpt4o",
        messages=[
            {"role": "system", "content": "You are an assistant based on RAG for banking system architecture."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.5  # 0.5 is reasonable; 8.5 is invalid
    )

    # Handle return type (Azure wrapper may return string or dict)
    if isinstance(completion, str):
        response = completion
    elif hasattr(completion, "choices"):
        response = completion.choices[0].message["content"]
    else:
        raise ValueError(f"Unexpected completion type: {type(completion)}")

    # Clean up whitespace for readability
    lines = [line.strip() for line in response.split("\n") if line.strip()]
    readable_response = "\n".join(lines)

    return readable_response










Here’s a polished French version of your email:


---

Objet : Mise à jour de la documentation « Usine de Fiable »

Bonjour [Nom du collègue],

Veuillez trouver ci-joint la version mise à jour de la documentation « Usine de Fiable », à laquelle j’ai ajouté le point [ou “cette section” selon ce que vous avez ajouté].

Merci de bien vouloir confirmer la réception et n’hésitez pas à m’ajouter tout élément que vous souhaitez discuter ou revoir.

Cordialement,
[Votre nom]


---

If you want, I can also make a slightly more friendly and collaborative version that encourages discussion without sounding too formal. Do you want me to do that?






appliquées après la production de ce mois.


Ajout d'une nouvelle donnée / nouveau contrôle « DCC » :
Ainsi, si vous avez reçu un nouveau contrôle de donnée, effectué une modification sur un contrôle existant, ou réactivé un ancien contrôle de donnée dans le périmètre, ou effectué toute autre forme de modification, merci de ne pas appliquer ces modifications après le 20 de chaque mois.




# Initialize retriever
retriever = CustomRetrieverWithHistory(
    collection=collection,      # your Chroma collection
    query_model=query_model,    # BGE-M3 with instruction
    doc_model=doc_model,        # BGE-M3 without instruction
    chat_history=[],            # optional
    k=100,                      # fetch top 100 dense docs
    rerank_k=50                 # rerank top 50 with BM25
)

query = "Which table stores AML risk classification for retail clients?"

docs = retriever.get_relevant_documents(query)

# Output results
for d in docs:
    print(d["metadata"], "\n", d["text"])




import string
import re
import numpy as np
from rank_bm25 import BM25Okapi

# ---------- BM25 Reranker ----------
class BM25Reranker:
    def __init__(self, k=50):
        self.k = k
        self.bm25 = None
        self.document_map = {}
        self.tokenizer = str.maketrans("", "", string.punctuation)

    def _tokenize(self, text):
        if not isinstance(text, str):
            return []
        cleaned = text.translate(self.tokenizer).lower()
        return [t for t in cleaned.split() if t.strip()]

    def fit(self, documents):
        if not documents:
            raise ValueError("Empty document list given to BM25Reranker")
        tokenized_corpus = []
        valid_docs = []
        for i, doc in enumerate(documents):
            text = doc.get("text") or doc.get("page content") or doc.get("content")
            tokens = self._tokenize(text)
            if tokens:
                tokenized_corpus.append(tokens)
                valid_docs.append((i, doc))
        if not tokenized_corpus:
            raise ValueError("No valid documents to fit BM25 model")
        self.document_map = {i: doc for i, doc in valid_docs}
        self.bm25 = BM25Okapi(tokenized_corpus)

    def rerank(self, query, documents):
        if not documents:
            return []
        self.fit(documents)
        tokens = self._tokenize(query)
        if not tokens:
            return documents[:self.k]
        scores = self.bm25.get_scores(tokens)
        ranked = sorted(
            zip(scores, self.document_map.values()),
            key=lambda x: x[0],
            reverse=True
        )
        return [doc for _, doc in ranked[:self.k]]



# ---------- Custom Retriever with Query & Document Embeddings ----------
class CustomRetrieverWithHistory:
    def __init__(self, collection, query_model, doc_model, chat_history=None, k=50, rerank_k=50):
        """
        collection: Chroma collection
        query_model: BGE-M3 model with query_instruction
        doc_model: BGE-M3 model without instruction
        chat_history: optional chat context
        k: number of dense results to fetch
        rerank_k: number of results for BM25 reranking
        """
        self.collection = collection
        self.chat_history = chat_history or []
        self.k = k
        self.rerank_k = rerank_k
        self.query_model = query_model
        self.doc_model = doc_model
        self.reranker = BM25Reranker(k=rerank_k)

    def get_relevant_documents(self, query):
        # --- combine chat history if exists ---
        if self.chat_history:
            full_query = " ".join(self.chat_history) + " " + query
        else:
            full_query = query

        # --- encode the query using the correct query embedding model ---
        query_embedding = self.query_model.encode([full_query])[0].tolist()

        # --- dense retrieval using query embedding ---
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=self.k,
            include=["documents", "metadatas"]
        )

        # --- collect dense docs ---
        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(results["documents"][0], results["metadatas"][0])
        ]

        # --- BM25 rerank over dense docs as hybrid step ---
        reranked_docs = self.reranker.rerank(full_query, dense_docs)

        return reranked_docs


query = "Which table stores AML risk classification for retail clients?"

query_embedding = query_model.encode([query])[0].tolist()

results = collection.query(
    query_embeddings=[query_embedding],
    n_results=10,
    include=["documents", "metadatas"]
)




MODEL_PATH = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"

# ---- Document embedding model (NO instruction) ----
doc_model = FlagModel(
    MODEL_PATH,
    use_fp16=True
)

# ---- Query embedding model (WITH instruction) ----
query_model = FlagModel(
    MODEL_PATH,
    query_instruction_for_retrieval=(
        "Generate representation for this sentence to retrieve relevant articles:"
    ),
    use_fp16=True
)












class BGE_M3_EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        self.dimension = 1024  # BGE-M3 dimension

    def __call__(self, inputs: List[str]) -> List[List[float]]:
        if isinstance(inputs, str):
            inputs = [inputs]

        embeddings = self.model.encode(inputs)

        if len(embeddings[0]) != self.dimension:
            raise ValueError(
                f"Embedding dimension mismatch: expected {self.dimension}, "
                f"got {len(embeddings[0])}"
            )

        return embeddings.tolist()collection = chroma_client.get_or_create_collection(
    name="my_documents",
    embedding_function=BGE_M3_EmbeddingFunction(doc_model),
    metadata={
        "hnsw:space": "cosine",
        "hnsw:construction_ef": 200,
        "hnsw:M": 16
    }
)




query = "CLIENT_ACCOUNT risk_level AML"

dense = chroma_collection.query(
    query_texts=[query],
    n_results=1000,   # very large on purpose
    include=["documents"]
)

dense_docs = dense["documents"][0]

found = any(
    any(chunk_id in d for chunk_id in ["CLIENT_ACCOUNT", "risk_level"])
    for d in dense_docs
)

print("Found in dense@1000:", found)




class BM25Reranker:
    def __init__(self, documents, k=50):
        """
        documents: list of {"text": ..., "metadata": ...}
        """
        self.k = k
        self.documents = documents
        self.tokenized_corpus = [self._tokenize(d["text"]) for d in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)

    def _tokenize(self, text: str):
        # preserves table names, columns, acronyms
        return re.findall(r"[A-Za-z0-9_\.]+", text.lower())

    def retrieve(self, query: str):
        tokens = self._tokenize(query)
        scores = self.bm25.get_scores(tokens)

        ranked = sorted(
            zip(scores, self.documents),
            key=lambda x: x[0],
            reverse=True
        )

        return [doc for _, doc in ranked[:self.k]]


class HybridRetriever:
    def __init__(
        self,
        chroma_collection,
        bm25_reranker,
        k_dense=200,
        k_final=20
    ):
        self.collection = chroma_collection
        self.bm25 = bm25_reranker
        self.k_dense = k_dense
        self.k_final = k_final

    def retrieve(self, query: str):
        # ---- 1. Query expansion (light, safe) ----
        expanded_query = (
            query
            + " table database schema column risk AML architecture"
        )

        # ---- 2. Dense retrieval (high recall) ----
        dense = self.collection.query(
            query_texts=[expanded_query],
            n_results=self.k_dense,
            include=["documents", "metadatas"]
        )

        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(dense["documents"][0], dense["metadatas"][0])
        ]

        # ---- 3. Sparse retrieval (BM25 rescue) ----
        sparse_docs = self.bm25.retrieve(query)

        # ---- 4. Union + dedup ----
        merged = {}
        for d in dense_docs + sparse_docs:
            merged[hash(d["text"])] = d

        merged_docs = list(merged.values())

        return merged_docs[:self.k_final]

def output_matching_chunks(collection, keyword: str):
    data = collection.get(include=["documents", "metadatas"])

    documents = data["documents"]
    metadatas = data["metadatas"]

    # handle list-of-lists
    if documents and isinstance(documents[0], list):
        documents = documents[0]
        metadatas = metadatas[0]

    hits = 0
    for i, (doc, meta) in enumerate(zip(documents, metadatas)):
        if keyword.lower() in doc.lower():
            print(f"\n--- MATCHING CHUNK {i} ---")
            print("Metadata:", meta)
            print(doc)
            hits += 1

    print("\nTotal matching chunks:", hits)

def build_context(docs):
    blocks = []
    for i, d in enumerate(docs):
        meta = d.get("metadata", {})
        header = f"""
[Chunk {i+1}]
Database: {meta.get('db', 'N/A')}
Schema: {meta.get('schema', 'N/A')}
Table: {meta.get('table', 'N/A')}
"""
        blocks.append(header + d["text"])
    return "\n\n".join(blocks)

# documents = same docs used for Chroma ingestion
bm25 = BM25Reranker(documents, k=100)

retriever = HybridRetriever(
    chroma_collection=chroma_collection,
    bm25_reranker=bm25,
    k_dense=250,
    k_final=20
)

query = "Which table stores AML risk classification for retail clients?"

docs = retriever.retrieve(query)

for d in docs:
    print(d["metadata"])query = "Which table stores AML risk classification for retail clients?"

docs = retriever.retrieve(query)

for d in docs:
    print(d["metadata"])

context = build_context(docs)

prompt = f"""
You are a banking information system architect.

Answer ONLY using the context.
If not found, say "Information not found".

Context:
{context}

Question:
{query}
"""








keyword = "CLIENT_ACCOUNT"

data = chroma_collection.get(
    include=["documents", "metadatas"]
)

hits = 0

documents = data["documents"]
metadatas = data["metadatas"]

# Handle both flat list and list-of-lists
if documents and isinstance(documents[0], list):
    documents = documents[0]
    metadatas = metadatas[0]

for i, (doc, meta) in enumerate(zip(documents, metadatas)):
    if keyword.lower() in doc.lower():
        print(f"\n--- MATCHING CHUNK {i} ---")
        print("Metadata:", meta)
        print(doc)
        hits += 1

print("\nTotal matching chunks:", hits)











data = chroma_collection.get(
    where={"source": "core_banking_architecture.pdf"},
    include=["documents", "metadatas"]
)

for i, doc in enumerate(data["documents"]):
    print(f"\n--- CHUNK {i} ---\n")
    print(doc)





bm25_reranker = BM25Reranker(
    documents=documents,
    k=50   # how many BM25 docs you want
)


hybrid_retriever = HybridRetriever(
    collection=chroma_collection,
    bm25_reranker=bm25_reranker,
    k_dense=120,   # large recall
    k_final=20     # final docs sent to LLM
)

query = "Which database stores AML risk classification for retail clients?"

retrieved_docs = hybrid_retriever.retrieve(query)



from rank_bm25 import BM25Okapi
import re

class BM25Reranker:
    def __init__(self, documents, k=20):
        self.k = k
        self.documents = documents
        self.tokenized_corpus = [self._tokenize(d["text"]) for d in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)

    def _tokenize(self, text):
        return re.findall(r"[A-Za-z0-9_\.]+", text.lower())

    def rerank(self, query):
        query_tokens = self._tokenize(query)
        scores = self.bm25.get_scores(query_tokens)

        ranked = sorted(
            zip(scores, self.documents),
            key=lambda x: x[0],
            reverse=True
        )
        return [doc for _, doc in ranked[:self.k]]class HybridRetriever:
    def __init__(self, collection, bm25_reranker, k_dense=100, k_final=20):
        self.collection = collection
        self.bm25 = bm25_reranker
        self.k_dense = k_dense
        self.k_final = k_final

    def retrieve(self, query):
        dense = self.collection.query(
            query_texts=[query],
            n_results=self.k_dense,
            include=["documents", "metadatas"]
        )

        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(dense["documents"][0], dense["metadatas"][0])
        ]

        # BM25 rescue
        sparse_docs = self.bm25.rerank(query)

        # Union + dedup
        seen = set()
        merged = []
        for d in dense_docs + sparse_docs:
            h = hash(d["text"])
            if h not in seen:
                seen.add(h)
                merged.append(d)

        return merged[:self.k_final]from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_community.document_compressors.rankllm_rerank import RankLLMRerank
from typing import List

def hybrid_retrieve_langchain(
    collection,           # Your vector DB collection (Chroma/FAISS)
    llm_client,           # Azure/OpenAI client (LLM)
    query: str,
    k_dense: int = 50,    # Number of top docs from embeddings
    k_final: int = 20     # Number of final docs after reranking
) -> List[dict]:
    """
    Hybrid Retrieval using:
    Step 1: Dense retrieval from vector DB
    Step 2: RankLLM reranking (via LangChain)
    Returns top documents (text + metadata)
    """

    # --------------------------
    # Step 1: Dense retrieval
    # --------------------------
    dense_results = collection.query(
        query_texts=[query],
        n_results=k_dense,
        include=["documents", "metadatas"]
    )

    candidates = []
    for text, meta in zip(dense_results["documents"][0], dense_results["metadatas"][0]):
        candidates.append({"text": text, "metadata": meta})

    if not candidates:
        return []

    # --------------------------
    # Step 2: RankLLM reranker
    # --------------------------
    reranker = RankLLMRerank.from_llm(
        llm=llm_client,  # your Azure/OpenAI client
        top_n=k_final,
        verbose=True
    )

    # Optional: wrap with ContextualCompressionRetriever if you want
    # to compress context before passing to reranker (useful for long chunks)
    retriever = ContextualCompressionRetriever(
        base_retriever=candidates,  # here just the list of candidates
        compressor=reranker
    )

    # Run reranking
    reranked_docs = reranker.rerank(query=query, documents=candidates)

    return reranked_docs[:k_final]






query = "How to configure user permissions in the system"

top_docs = hybrid_retrieve_langchain(
    collection=chroma_collection,
    llm_client=azure_openai_client,
    query=query,
    k_dense=80,
    k_final=20
)

for doc in top_docs:
    print(doc["text"][:300], "...\n")







import streamlit as st
import chromadb
from chromadb.utils import embedding_functions
import fitz  # PyMuPDF
from docx import Document
import pandas as pd
from datetime import datetime
import json
import os
from typing import List, Dict, Tuple, Optional
import re
from collections import defaultdict
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
import hashlib

# Azure OpenAI imports
from openai import AzureOpenAI
import httpx

# ==============================================================================
# CONFIGURATION
# ==============================================================================

class Config:
    """Centralized configuration management"""
    
    # Azure OpenAI Configuration
    OIDC_CLIENT_ID = os.getenv("OIDC_CLIENT_ID", "your_client_id")
    OIDC_CLIENT_SECRET = os.getenv("OIDC_CLIENT_SECRET", "your_client_secret")
    OIDC_ENDPOINT = os.getenv("OIDC_ENDPOINT", "https://alfactory.api.staging.schonet/auth/oauth2/v2/token")
    OIDC_SCOPE = os.getenv("OIDC_SCOPE", "genai-model")
    
    AZURE_ENDPOINT = os.getenv("AZURE_ENDPOINT", "https://alfactory.api.staging.schonet/genai-model/v1")
    AZURE_API_VERSION = os.getenv("AZURE_API_VERSION", "2024-05-01-preview")
    AZURE_API_KEY = os.getenv("AZURE_API_KEY", "FAKE_KEY")
    AZURE_MODEL_DEPLOYMENT = os.getenv("AZURE_MODEL_DEPLOYMENT", "gpt-4o")
    
    # Embedding Configuration
    EMBEDDING_MODEL_PATH = "/domino/datasets/local/test-prd-base"
    EMBEDDING_DIMENSION = 1024
    
    # ChromaDB Configuration
    CHROMA_PERSIST_PATH = "/domino/datasets/local/chroma_persistent_db"
    COLLECTION_NAME = "banking_documents"
    
    # Chunking Configuration for Banking Documents
    CHUNK_SIZES = {
        'header': 300,      # For sections with headers
        'table': 500,       # For tables and structured data
        'paragraph': 400,   # For regular paragraphs
        'list': 350,        # For lists and enumerations
    }
    CHUNK_OVERLAP = 100
    
    # BM25 Configuration
    BM25_INDEX_PATH = "/domino/datasets/local/bm25_index.pkl"
    BM25_K1 = 1.5
    BM25_B = 0.75
    
    # Retrieval Configuration
    INITIAL_RETRIEVAL_K = 20  # Retrieve more for reranking
    FINAL_RESULTS_K = 5       # Final results after reranking
    HYBRID_ALPHA = 0.5        # Balance between semantic (0) and keyword (1)
    
    # Batch Processing Configuration
    BATCH_SIZE = 100
    MAX_WORKERS = 4

# ==============================================================================
# DOCUMENT PROCESSING & CHUNKING
# ==============================================================================

class BankingDocumentProcessor:
    """Advanced document processor optimized for banking/technical documents"""
    
    def __init__(self):
        self.chunk_patterns = {
            'section_header': re.compile(r'^(Sect\.|Section|Chapitre|Article|§)\s*[\dIVX]+[\.\s]', re.IGNORECASE),
            'subsection': re.compile(r'^[A-Z]{1,3}\.\d+(\.\d+)*\s+', re.MULTILINE),
            'table_marker': re.compile(r'(Description Rubrique|Num\s+donn\.|Val\s+init)', re.IGNORECASE),
            'code_block': re.compile(r'(Code|Référence|Type de contrat):\s*\d+', re.IGNORECASE),
            'list_item': re.compile(r'^\s*[-•*]\s+|\d+\.\s+', re.MULTILINE),
        }
    
    def extract_text_from_file(self, uploaded_file) -> Optional[Dict]:
        """Extract text with metadata from uploaded files"""
        filename = uploaded_file.name.lower()
        
        try:
            if filename.endswith('.pdf'):
                return self._extract_from_pdf(uploaded_file)
            elif filename.endswith('.txt'):
                text = uploaded_file.read().decode('utf-8')
                return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
            elif filename.endswith('.docx'):
                return self._extract_from_docx(uploaded_file)
            elif filename.endswith(('.csv', '.xlsx')):
                return self._extract_from_spreadsheet(uploaded_file, filename)
            else:
                return None
        except Exception as e:
            st.error(f"Error extracting from {uploaded_file.name}: {str(e)}")
            return None
    
    def _extract_from_pdf(self, uploaded_file) -> Dict:
        """Extract text from PDF with page-level granularity"""
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        pages = []
        
        for page_num, page in enumerate(doc, start=1):
            text = page.get_text()
            pages.append({
                'page_num': page_num,
                'text': text
            })
        
        full_text = "\n".join([p['text'] for p in pages])
        return {'text': full_text, 'pages': pages}
    
    def _extract_from_docx(self, uploaded_file) -> Dict:
        """Extract text from DOCX"""
        doc = Document(uploaded_file)
        text = "\n".join([p.text for p in doc.paragraphs])
        return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
    
    def _extract_from_spreadsheet(self, uploaded_file, filename: str) -> Dict:
        """Extract text from CSV/Excel"""
        if filename.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)
        
        text = df.to_string(index=False)
        return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
    
    def clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove excessive whitespace
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        
        # Normalize line breaks
        text = text.replace('\r\n', '\n')
        
        # Remove page headers/footers (common patterns)
        text = re.sub(r'Page \d+( of \d+)?', '', text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def smart_chunk_document(self, doc_data: Dict, source_file: str) -> List[Dict]:
        """
        Intelligent chunking that preserves document structure
        Optimized for banking/technical documents
        """
        chunks = []
        pages = doc_data.get('pages', [])
        
        for page_info in pages:
            page_num = page_info['page_num']
            text = self.clean_text(page_info['text'])
            
            # Detect document structure
            if self._is_table_content(text):
                page_chunks = self._chunk_table_content(text, source_file, page_num)
            elif self._has_clear_sections(text):
                page_chunks = self._chunk_by_sections(text, source_file, page_num)
            else:
                page_chunks = self._chunk_semantic(text, source_file, page_num)
            
            chunks.extend(page_chunks)
        
        return chunks
    
    def _is_table_content(self, text: str) -> bool:
        """Detect if content is primarily tabular"""
        return bool(self.chunk_patterns['table_marker'].search(text))
    
    def _has_clear_sections(self, text: str) -> bool:
        """Detect if content has clear section structure"""
        headers = self.chunk_patterns['section_header'].findall(text)
        return len(headers) >= 2
    
    def _chunk_table_content(self, text: str, source: str, page: int) -> List[Dict]:
        """Chunk table content preserving structure"""
        chunks = []
        lines = text.split('\n')
        current_chunk = []
        current_size = 0
        
        for line in lines:
            line_size = len(line)
            
            if current_size + line_size > Config.CHUNK_SIZES['table']:
                if current_chunk:
                    chunks.append(self._create_chunk(
                        '\n'.join(current_chunk), source, page, 'table'
                    ))
                    # Overlap: keep last few lines
                    overlap_lines = current_chunk[-3:] if len(current_chunk) > 3 else current_chunk
                    current_chunk = overlap_lines
                    current_size = sum(len(l) for l in current_chunk)
            
            current_chunk.append(line)
            current_size += line_size
        
        if current_chunk:
            chunks.append(self._create_chunk(
                '\n'.join(current_chunk), source, page, 'table'
            ))
        
        return chunks
    
    def _chunk_by_sections(self, text: str, source: str, page: int) -> List[Dict]:
        """Chunk by document sections"""
        chunks = []
        sections = re.split(r'(\n(?:Sect\.|Section|Chapitre|Article)\s+[\dIVX]+)', text, flags=re.IGNORECASE)
        
        current_section = ""
        
        for i, section in enumerate(sections):
            if i % 2 == 0:  # Content
                current_section += section
            else:  # Header
                if current_section.strip():
                    chunks.append(self._create_chunk(
                        current_section.strip(), source, page, 'section'
                    ))
                current_section = section
        
        if current_section.strip():
            chunks.append(self._create_chunk(
                current_section.strip(), source, page, 'section'
            ))
        
        return chunks
    
    def _chunk_semantic(self, text: str, source: str, page: int) -> List[Dict]:
        """Semantic chunking with overlap for regular content"""
        chunks = []
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            if current_size + sentence_size > Config.CHUNK_SIZES['paragraph']:
                if current_chunk:
                    chunks.append(self._create_chunk(
                        ' '.join(current_chunk), source, page, 'paragraph'
                    ))
                    # Overlap: keep last 2 sentences
                    overlap = current_chunk[-2:] if len(current_chunk) > 2 else current_chunk
                    current_chunk = overlap
                    current_size = sum(len(s) for s in current_chunk)
            
            current_chunk.append(sentence)
            current_size += sentence_size
        
        if current_chunk:
            chunks.append(self._create_chunk(
                ' '.join(current_chunk), source, page, 'paragraph'
            ))
        
        return chunks
    
    def _create_chunk(self, text: str, source: str, page: int, chunk_type: str) -> Dict:
        """Create a standardized chunk object"""
        chunk_id = hashlib.md5(f"{source}_{page}_{text[:50]}".encode()).hexdigest()
        
        return {
            'id': chunk_id,
            'text': text,
            'metadata': {
                'source_file': source,
                'page': page,
                'chunk_type': chunk_type,
                'char_count': len(text),
                'timestamp': str(datetime.now())
            }
        }

# ==============================================================================
# BM25 IMPLEMENTATION WITH PERSISTENT INDEX
# ==============================================================================

class PersistentBM25:
    """BM25 implementation with disk persistence for large-scale retrieval"""
    
    def __init__(self, k1: float = Config.BM25_K1, b: float = Config.BM25_B):
        self.k1 = k1
        self.b = b
        self.corpus_size = 0
        self.avgdl = 0
        self.doc_freqs = defaultdict(int)
        self.idf = {}
        self.doc_len = []
        self.doc_ids = []
        self.tokenized_corpus = []
        
    def tokenize(self, text: str) -> List[str]:
        """Tokenize with banking-specific preservation"""
        # Preserve codes and references
        text = re.sub(r'([A-Z]{2,}\d+)', r' \1 ', text)
        
        # Remove punctuation but keep hyphens in codes
        text = re.sub(r'[^\w\s-]', ' ', text.lower())
        
        # Tokenize
        tokens = text.split()
        
        # Filter stopwords (French banking context)
        stopwords = {'le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'ou', 'dans', 'pour', 'sur', 'avec'}
        tokens = [t for t in tokens if t and t not in stopwords]
        
        return tokens
    
    def fit(self, documents: List[Dict]):
        """Build BM25 index from documents"""
        self.corpus_size = len(documents)
        self.doc_ids = [doc['id'] for doc in documents]
        self.tokenized_corpus = []
        
        # Tokenize all documents
        for doc in documents:
            tokens = self.tokenize(doc['text'])
            self.tokenized_corpus.append(tokens)
            self.doc_len.append(len(tokens))
            
            # Count document frequencies
            unique_tokens = set(tokens)
            for token in unique_tokens:
                self.doc_freqs[token] += 1
        
        # Calculate average document length
        self.avgdl = sum(self.doc_len) / self.corpus_size if self.corpus_size > 0 else 0
        
        # Calculate IDF values
        for token, freq in self.doc_freqs.items():
            self.idf[token] = np.log((self.corpus_size - freq + 0.5) / (freq + 0.5) + 1)
    
    def get_scores(self, query: str) -> Dict[str, float]:
        """Calculate BM25 scores for a query"""
        query_tokens = self.tokenize(query)
        scores = {}
        
        for idx, (doc_id, doc_tokens, doc_length) in enumerate(
            zip(self.doc_ids, self.tokenized_corpus, self.doc_len)
        ):
            score = 0
            for token in query_tokens:
                if token not in self.idf:
                    continue
                
                # Calculate term frequency in document
                tf = doc_tokens.count(token)
                
                # BM25 formula
                numerator = tf * (self.k1 + 1)
                denominator = tf + self.k1 * (1 - self.b + self.b * (doc_length / self.avgdl))
                score += self.idf[token] * (numerator / denominator)
            
            scores[doc_id] = score
        
        return scores
    
    def save(self, filepath: str):
        """Save BM25 index to disk"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'k1': self.k1,
                'b': self.b,
                'corpus_size': self.corpus_size,
                'avgdl': self.avgdl,
                'doc_freqs': dict(self.doc_freqs),
                'idf': self.idf,
                'doc_len': self.doc_len,
                'doc_ids': self.doc_ids,
                'tokenized_corpus': self.tokenized_corpus
            }, f)
    
    @classmethod
    def load(cls, filepath: str) -> 'PersistentBM25':
        """Load BM25 index from disk"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        
        bm25 = cls(k1=data['k1'], b=data['b'])
        bm25.corpus_size = data['corpus_size']
        bm25.avgdl = data['avgdl']
        bm25.doc_freqs = defaultdict(int, data['doc_freqs'])
        bm25.idf = data['idf']
        bm25.doc_len = data['doc_len']
        bm25.doc_ids = data['doc_ids']
        bm25.tokenized_corpus = data['tokenized_corpus']
        
        return bm25

# ==============================================================================
# QUERY EXPANSION FOR BANKING TERMINOLOGY
# ==============================================================================

class BankingQueryExpander:
    """Query expansion specialized for banking/financial terminology"""
    
    def __init__(self):
        # Banking-specific synonym mappings
        self.synonyms = {
            'compte': ['account', 'compte support', 'compte client'],
            'contrat': ['contract', 'agreement', 'convention'],
            'dépôt': ['deposit', 'versement', 'apport'],
            'garantie': ['guarantee', 'caution', 'warranty'],
            'échéance': ['maturity', 'deadline', 'due date', 'expiration'],
            'nantissement': ['pledge', 'collateral', 'guarantee'],
            'saisie': ['entry', 'input', 'capture'],
            'grille': ['grid', 'screen', 'form'],
            'transaction': ['operation', 'mouvement', 'transfer'],
            'client': ['customer', 'account holder', 'beneficiary'],
            'bénéficiaire': ['beneficiary', 'recipient', 'payee'],
            'taux': ['rate', 'interest rate', 'percentage'],
            'plafond': ['ceiling', 'limit', 'cap', 'maximum'],
            'retrait': ['withdrawal', 'extraction'],
            'virement': ['transfer', 'wire', 'payment'],
        }
        
        # Common banking abbreviations
        self.abbreviations = {
            'DAT': 'Dépôt à Terme',
            'CNT': 'Contrat',
            'GDI': 'Gestion De Interface',
            'BDC': 'Bon De Caisse',
            'SGE': 'Code siège',
            'DEV': 'Code devise',
        }
    
    def expand_query(self, query: str) -> List[str]:
        """Expand query with synonyms and banking terms"""
        expanded_queries = [query]
        
        query_lower = query.lower()
        
        # Add synonym expansions
        for term, synonyms in self.synonyms.items():
            if term in query_lower:
                for synonym in synonyms:
                    expanded = query_lower.replace(term, synonym)
                    if expanded != query_lower:
                        expanded_queries.append(expanded)
        
        # Expand abbreviations
        for abbr, full_form in self.abbreviations.items():
            if abbr in query:
                expanded = query.replace(abbr, full_form)
                expanded_queries.append(expanded)
            elif full_form.lower() in query_lower:
                expanded = query_lower.replace(full_form.lower(), abbr)
                expanded_queries.append(expanded)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_queries = []
        for q in expanded_queries:
            if q not in seen:
                seen.add(q)
                unique_queries.append(q)
        
        return unique_queries[:5]  # Limit to top 5 expansions

# ==============================================================================
# HYBRID RETRIEVAL SYSTEM
# ==============================================================================

class HybridRetriever:
    """Hybrid retrieval combining semantic search (ChromaDB) and keyword search (BM25)"""
    
    def __init__(self, chroma_collection, bm25_index: PersistentBM25, alpha: float = Config.HYBRID_ALPHA):
        self.chroma_collection = chroma_collection
        self.bm25_index = bm25_index
        self.alpha = alpha  # Weight between semantic (0) and keyword (1)
        self.query_expander = BankingQueryExpander()
    
    def retrieve(self, query: str, k: int = Config.INITIAL_RETRIEVAL_K) -> List[Dict]:
        """
        Hybrid retrieval with query expansion
        Returns top k documents based on weighted combination of semantic + keyword scores
        """
        # Expand query for better recall
        expanded_queries = self.query_expander.expand_query(query)
        
        all_results = {}
        
        for exp_query in expanded_queries:
            # 1. Semantic search via ChromaDB
            semantic_results = self.chroma_collection.query(
                query_texts=[exp_query],
                n_results=k,
                include=["documents", "metadatas", "distances"]
            )
            
            # Normalize semantic scores (distances -> similarities)
            semantic_scores = {}
            if semantic_results['ids'] and semantic_results['ids'][0]:
                max_distance = max(semantic_results['distances'][0]) if semantic_results['distances'][0] else 1
                for idx, doc_id in enumerate(semantic_results['ids'][0]):
                    distance = semantic_results['distances'][0][idx]
                    similarity = 1 - (distance / max_distance) if max_distance > 0 else 1
                    semantic_scores[doc_id] = similarity
            
            # 2. Keyword search via BM25
            bm25_scores = self.bm25_index.get_scores(exp_query)
            
            # Normalize BM25 scores
            max_bm25 = max(bm25_scores.values()) if bm25_scores else 1
            normalized_bm25 = {k: v / max_bm25 for k, v in bm25_scores.items()} if max_bm25 > 0 else bm25_scores
            
            # 3. Combine scores with weighted sum
            all_doc_ids = set(semantic_scores.keys()) | set(normalized_bm25.keys())
            
            for doc_id in all_doc_ids:
                sem_score = semantic_scores.get(doc_id, 0)
                bm25_score = normalized_bm25.get(doc_id, 0)
                
                # Hybrid score
                hybrid_score = (1 - self.alpha) * sem_score + self.alpha * bm25_score
                
                # Accumulate scores across expanded queries
                if doc_id in all_results:
                    all_results[doc_id]['score'] = max(all_results[doc_id]['score'], hybrid_score)
                else:
                    # Get document metadata
                    try:
                        doc_data = self.chroma_collection.get(
                            ids=[doc_id],
                            include=["documents", "metadatas"]
                        )
                        if doc_data['ids']:
                            all_results[doc_id] = {
                                'id': doc_id,
                                'text': doc_data['documents'][0],
                                'metadata': doc_data['metadatas'][0],
                                'score': hybrid_score,
                                'semantic_score': sem_score,
                                'bm25_score': bm25_score
                            }
                    except:
                        pass
        
        # Sort by score and return top k
        ranked_results = sorted(all_results.values(), key=lambda x: x['score'], reverse=True)
        return ranked_results[:k]

# ==============================================================================
# BATCH DOCUMENT INGESTION
# ==============================================================================

class BatchDocumentIngestion:
    """Optimized batch processing for large document collections"""
    
    def __init__(self, chroma_collection, processor: BankingDocumentProcessor):
        self.collection = chroma_collection
        self.processor = processor
        self.bm25_documents = []
    
    def ingest_documents(self, uploaded_files: List, progress_callback=None) -> Dict:
        """
        Batch ingest documents with progress tracking
        Returns statistics about the ingestion process
        """
        stats = {
            'total_files': len(uploaded_files),
            'processed_files': 0,
            'total_chunks': 0,
            'failed_files': [],
            'processing_time': 0
        }
        
        start_time = datetime.now()
        
        # Get existing documents to avoid duplicates
        existing_sources = self._get_existing_sources()
        
        batch_chunks = []
        
        for file_idx, uploaded_file in enumerate(uploaded_files):
            try:
                filename = uploaded_file.name
                
                # Skip if already indexed
                if filename in existing_sources:
                    if progress_callback:
                        progress_callback(file_idx + 1, len(uploaded_files), f"Skipped (already indexed): {filename}")
                    continue
                
                if progress_callback:
                    progress_callback(file_idx + 1, len(uploaded_files), f"Processing: {filename}")
                
                # Extract and chunk document
                doc_data = self.processor.extract_text_from_file(uploaded_file)
                
                if doc_data is None:
                    stats['failed_files'].append(filename)
                    continue
                
                chunks = self.processor.smart_chunk_document(doc_data, filename)
                
                # Add to batch
                batch_chunks.extend(chunks)
                self.bm25_documents.extend(chunks)
                
                # Process batch if it reaches batch size
                if len(batch_chunks) >= Config.BATCH_SIZE:
                    self._process_batch(batch_chunks)
                    stats['total_chunks'] += len(batch_chunks)
                    batch_chunks = []
                
                stats['processed_files'] += 1
                
            except Exception as e:
                stats['failed_files'].append(f"{uploaded_file.name}: {str(e)}")
        
        # Process remaining chunks
        if batch_chunks:
            self._process_batch(batch_chunks)
            stats['total_chunks'] += len(batch_chunks)
        
        stats['processing_time'] = (datetime.now() - start_time).total_seconds()
        
        return stats
    
    def _process_batch(self, chunks: List[Dict]):
        """Process a batch of chunks into ChromaDB"""
        if not chunks:
            return
        
        ids = [chunk['id'] for chunk in chunks]
        documents = [chunk['text'] for chunk in chunks]
        metadatas = [chunk['metadata'] for chunk in chunks]
        
        self.collection.add(
            ids=ids,
            documents=documents,
            metadatas=metadatas
        )
    
    def _get_existing_sources(self) -> set:
        """Get list of already indexed source files"""
        try:
            all_data = self.collection.get(include=["metadatas"])
            if all_data and all_data['metadatas']:
                return set(meta.get('source_file', '') for meta in all_data['metadatas'])
        except:
            pass
        return set()
    
    def build_bm25_index(self) -> PersistentBM25:
        """Build and save BM25 index from ingested documents"""
        bm25 = PersistentBM25()
        bm25.fit(self.bm25_documents)
        bm25.save(Config.BM25_INDEX_PATH)
        return bm25

# ==============================================================================
# RAG GENERATION WITH IMPROVED CONTEXT
# ==============================================================================

class BankingRAGGenerator:
    """Enhanced RAG generation for banking documentation"""
    
    def __init__(self, azure_client):
        self.client = azure_client
    
    def generate_response(self, query: str, context_docs: List[Dict]) -> Dict:
        """Generate response with improved context assembly"""
        
        # Assemble context with source citations
        context_parts = []
        sources = []
        
        for idx, doc in enumerate(context_docs, 1):
            metadata = doc.get('metadata', {})
            source_file = metadata.get('source_file', 'Unknown')
            page = metadata.get('page', 'N/A')
            
            context_parts.append(
                f"[Document {idx} - {source_file}, Page {page}]\n{doc['text']}\n"
            )
            
            if source_file not in sources:
                sources.append(source_file)
        
        context = "\n".join(context_parts)
        
        # Enhanced prompt for banking documentation
        prompt = f"""Vous êtes un assistant RAG spécialisé dans les systèmes bancaires, l'architecture de données, et les produits financiers.

Votre rôle est de répondre et d'expliquer clairement les informations issues des documents, en les rendant compréhensibles pour tout type d'utilisateur, du plus général au plus technique.

[Contexte]
{context}

[Requête]
{query}

[INSTRUCTIONS GÉNÉRALES]

1. Utilisez exclusivement les informations présentes dans le contexte ci-dessus.

2. Fournissez une réponse claire, structurée et explicite, adaptée à la compréhension de tout utilisateur.
   - Si le sujet est technique, expliquez les notions en termes simples.
   - Si la question est générale, donnez une réponse complète mais concise.
   - Si la question est complexe, détaillez le raisonnement et le fonctionnement.

3. Chaque fois que vous mentionnez une donnée, un mécanisme ou un fait:
   - Citez immédiatement la source sous le format: [Source: nom_du_fichier.pdf, Page <numéro>]

4. Si l'information n'existe pas dans les documents, indiquez-le clairement.

5. Structure attendue:
   a. Réponse expliquée: Détaillez le contenu et son interprétation.
   b. Synthèse (si utile): Résumez la logique ou le fonctionnement global.
   c. Sources: Liste complète des documents utilisés.

6. Objectif:
   Rendre la réponse à la fois informative, explicative et vérifiable,
   qu'il s'agisse d'un utilisateur curieux ou d'un expert technique.

[Exemple de format]
Les procédures de vérification d'identité... [Source: procedure_kyc.pdf, Page 12]. 
Selon le chapitre 3... [Source: reglement_financier.pdf, Page 45].

Sources utilisées:
- procedure_kyc.pdf (Pages 12, 15)
- reglement_financier.pdf (Page 45)
"""
        
        try:
            completion = self.client.chat.completions.create(
                model=Config.AZURE_MODEL_DEPLOYMENT,
                messages=[
                    {"role": "system", "content": "You are an expert assistant for banking system documentation based on RAG (Retrieval-Augmented Generation)."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            answer = completion.choices[0].message.content
            
            return {
                'answer': answer,
                'sources': sources,
                'context_docs': context_docs,
                'num_tokens': completion.usage.total_tokens if hasattr(completion, 'usage') else None
            }
            
        except Exception as e:
            return {
                'answer': f"Error generating response: {str(e)}",
                'sources': [],
                'context_docs': [],
                'num_tokens': None
            }

# ==============================================================================
# STREAMLIT APPLICATION
# ==============================================================================

def initialize_session_state():
    """Initialize Streamlit session state"""
    if 'chroma_client' not in st.session_state:
        st.session_state.chroma_client = chromadb.PersistentClient(path=Config.CHROMA_PERSIST_PATH)
    
    if 'collection' not in st.session_state:
        # Load embedding model
        embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL_PATH)
        
        # Create embedding function
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=Config.EMBEDDING_MODEL_PATH
        )
        
        st.session_state.collection = st.session_state.chroma_client.get_or_create_collection(
            name=Config.COLLECTION_NAME,
            embedding_function=embedding_function,
            metadata={"hnsw:space": "cosine"}
        )
    
    if 'bm25_index' not in st.session_state:
        # Load or create BM25 index
        if os.path.exists(Config.BM25_INDEX_PATH):
            st.session_state.bm25_index = PersistentBM25.load(Config.BM25_INDEX_PATH)
        else:
            st.session_state.bm25_index = PersistentBM25()
    
    if 'chat_sessions' not in st.session_state:
        st.session_state.chat_sessions = {}
    
    if 'current_chat' not in st.session_state:
        st.session_state.current_chat = str(datetime.now())
        st.session_state.chat_sessions[st.session_state.current_chat] = {
            "title": "New Chat",
            "messages": []
        }
    
    if 'processor' not in st.session_state:
        st.session_state.processor = BankingDocumentProcessor()
    
    if 'azure_client' not in st.session_state:
        # Initialize Azure OpenAI client
        http_client = httpx.Client(verify=False)
        st.session_state.azure_client = AzureOpenAI(
            api_version=Config.AZURE_API_VERSION,
            azure_endpoint=Config.AZURE_ENDPOINT,
            api_key=Config.AZURE_API_KEY,
            http_client=http_client
        )
    
    if 'rag_generator' not in st.session_state:
        st.session_state.rag_generator = BankingRAGGenerator(st.session_state.azure_client)

def main():
    st.set_page_config(page_title="BNP Banking RAG Chatbot", layout="wide")
    
    initialize_session_state()
    
    st.title("🏦 BNP Banking Documentation Chatbot")
    st.caption("Optimized RAG system with hybrid search and batch processing")
    
    # Sidebar for document management
    with st.sidebar:
        st.header("📁 Document Management")
        
        # Get collection stats
        try:
            collection_count = st.session_state.collection.count()
            st.metric("Indexed Documents", collection_count)
        except:
            collection_count = 0
            st.metric("Indexed Documents", "0")
        
        # Batch upload
        st.subheader("Upload Documents")
        uploaded_files = st.file_uploader(
            "Upload multiple documents",
            type=["pdf", "txt", "docx", "csv", "xlsx"],
            accept_multiple_files=True
        )
        
        if uploaded_files and st.button("📤 Process All Files"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(current, total, message):
                progress = current / total
                progress_bar.progress(progress)
                status_text.text(message)
            
            with st.spinner("Processing documents..."):
                batch_ingestion = BatchDocumentIngestion(
                    st.session_state.collection,
                    st.session_state.processor
                )
                
                stats = batch_ingestion.ingest_documents(
                    uploaded_files,
                    progress_callback=update_progress
                )
                
                # Rebuild BM25 index
                status_text.text("Building BM25 index...")
                st.session_state.bm25_index = batch_ingestion.build_bm25_index()
                
                # Display results
                st.success(f"""
                ✅ Processing complete!
                - Files processed: {stats['processed_files']}/{stats['total_files']}
                - Total chunks: {stats['total_chunks']}
                - Time: {stats['processing_time']:.2f}s
                """)
                
                if stats['failed_files']:
                    st.error(f"Failed files: {', '.join(stats['failed_files'])}")
        
        st.divider()
        
        # Chat management
        st.subheader("💬 Chat Sessions")
        if st.button("➕ New Chat"):
            new_chat_id = str(datetime.now())
            st.session_state.current_chat = new_chat_id
            st.session_state.chat_sessions[new_chat_id] = {
                "title": "New Chat",
                "messages": []
            }
            st.rerun()
        
        # List existing chats
        for chat_id, chat_data in list(st.session_state.chat_sessions.items()):
            if st.button(f"📝 {chat_data['title'][:30]}", key=f"chat_{chat_id}"):
                st.session_state.current_chat = chat_id
                st.rerun()
    
    # Main chat interface
    current_chat = st.session_state.chat_sessions.get(st.session_state.current_chat, {
        "title": "New Chat",
        "messages": []
    })
    
    # Display chat messages
    for message in current_chat['messages']:
        with st.chat_message(message['role']):
            st.markdown(message['content'])
            
            # Show sources if available
            if message['role'] == 'assistant' and 'sources' in message:
                with st.expander("📚 Sources"):
                    for source in message['sources']:
                        st.text(f"• {source}")
    
    # Chat input
    if prompt := st.chat_input("Ask about banking documentation..."):
        # Add user message
        current_chat['messages'].append({
            "role": "user",
            "content": prompt,
            "timestamp": str(datetime.now())
        })
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("🔍 Searching documentation..."):
                # Create hybrid retriever
                retriever = HybridRetriever(
                    st.session_state.collection,
                    st.session_state.bm25_index
                )
                
                # Retrieve relevant documents
                retrieved_docs = retriever.retrieve(prompt, k=Config.FINAL_RESULTS_K)
                
                # Generate response
                response = st.session_state.rag_generator.generate_response(
                    prompt,
                    retrieved_docs
                )
                
                # Display answer
                st.markdown(response['answer'])
                
                # Show sources
                if response['sources']:
                    with st.expander("📚 Sources"):
                        for source in response['sources']:
                            st.text(f"• {source}")
                
                # Add to chat history
                current_chat['messages'].append({
                    "role": "assistant",
                    "content": response['answer'],
                    "sources": response['sources'],
                    "timestamp": str(datetime.now())
                })
                
                # Update chat title if first message
                if current_chat['title'] == "New Chat" and len(current_chat['messages']) > 0:
                    current_chat['title'] = prompt[:50] + "..."

if __name__ == "__main__":
    main()







Nouvelle slide – Public cible
À qui s’adresse la solution RAG BNP ED ?
RAG BNP ED a été développée pour accompagner différents profils impliqués dans l’exploitation et la transformation du système d’information :
👥 Équipes métiers
Accès rapide et fiable à la documentation ATLAS
Recherche simplifiée sans connaissance technique
Réponses claires, sourcées et contextualisées
🏗️ Équipes IT / DSI
Compréhension de l’architecture de la documentation ATLAS
Accès structuré aux référentiels et aux dépendances documentaires
Support à l’analyse et à la maintenance du SI
🔄 Équipes Transformation du Système d’Information
Accompagnement des projets de migration et d’évolution du SI
Vision transverse de la documentation existante
Réduction du temps de recherche et d’analyse documentaire
💡 Variante plus synthétique (si tu veux une slide très légère)
RAG BNP ED s’adresse aux équipes métiers, à la DSI et aux acteurs de la transformation du système d’information, en facilitant l’accès, la compréhension et l’exploitation de la documentation ATLAS.














RAG BNP ED est un chatbot intelligent basé sur la méthode RAG (Retrieval Augmented Generation) et sur une base de connaissances centralisée contenant la documentation ATLAS.
Il permet une recherche conversationnelle rapide et fiable, avec des réponses systématiquement sourcées à partir des documents officiels.
La solution évolue automatiquement avec les nouvelles documentations, garantissant des réponses toujours à jour.






RAG BNP ED est un chatbot intelligent basé sur une base de connaissances centralisée (documentation ATLAS).
Il permet une recherche conversationnelle rapide et fiable, avec des réponses systématiquement sourcées.
La solution évolue automatiquement avec les nouvelles documentations, garantissant des réponses toujours à jour.








La documentation ATLAS est volumineuse et critique pour les équipes métiers
L’accès à l’information repose aujourd’hui sur une recherche manuelle chronophage
Objectif : faciliter l’accès rapide, fiable et sourcé à l’information interne via un outil intelligent




Bonjour,

Pourriez-vous s’il vous plaît procéder à la réinitialisation de mon mot de passe ? Je vous remercie par avance.

Cordialement,
Nihad Senhadji


Bonjour à tous,

Suite à notre précédent échange, je vous informe que nous avons pu identifier et renseigner les dates d’impayés en nous basant sur l’extraction transmise par Chaima.

Toutefois, 11 cas restent pour lesquels les dates d’impayés n’ont pas pu être renseignées, malgré l’utilisation de cette extraction. Ces cas demeurent donc identifiés comme anomalies.

Je reste à votre disposition pour toute précision complémentaire ou pour échanger sur ces cas si besoin.

Cordialement,
Nihad Senhadji
GA-15








Bonjour à tous,

Veuillez trouver ci-joint la nouvelle version du fichier État Central des Engagements arrêté au 31/12/2025.

Lors du traitement, nous avons constaté que certaines dates d’impayés n’ont pas été retrouvées dans les données d’extraction disponibles.
Pour cette raison, une nouvelle feuille a été ajoutée au fichier, intitulée :

« État des engagements – Anomalies dates d’impayés »

Cette feuille regroupe l’ensemble des engagements pour lesquels les dates d’impayés n’ont pas pu être identifiées à partir de l’extraction.

Le fichier contient désormais les feuilles suivantes :

1. État Central des Engagements
2. État des engagements – Anomalies dates d’impayés
3. État détaillé par chapitre
4. Rapprochement des créances – Bilan
5. Rapprochement des créances hors bilan
6. Rapprochement CR / AR / CB
7. Reporting social BG
8. État détaillé par ratios prudentiels
9. Risques – Rapprochement RP
10. Risques – Grands Risques
11. Risques – Grands Risques par entité
12. Risques – Annexe 9000
13. Rapprochement reporting provisions
14. Rapprochement affectation des garanties
15. État des garanties

Je reste à votre disposition pour toute information complémentaire ou clarification si besoin.

Cordialement,
Nihad Senhadji
GA-15







Bien sûr ! Voici une version réécrite, fluide, professionnelle et claire de ton email :


---

Objet : Méthodologie – Détection de communautés de fraude (AI Sentinilia)

Bonjour Ryma,

Dans le cadre du projet AI Sentinilia dédié à la détection de communautés de fraude, la méthodologie utilisée repose sur les critères suivants :

Les liens entre clients sont identifiés à partir de coïncidences lorsqu’ils effectuent des retraits :

à la même date ;

au même GAB (ATM) ;

pour un montant supérieur ou égal à 500 euros ;

avec une fréquence d’au moins 5 retraits entre deux clients ou plus.



Lorsque ces conditions sont remplies, les clients sont considérés comme liés, car leurs comportements de retrait présentent des similitudes répétées et inhabituelles.

Afin de détecter les communautés de fraude, nous utilisons un algorithme appelé Louvain, qui permet de regrouper automatiquement les clients ayant des comportements similaires. Chaque communauté ainsi identifiée représente un groupe potentiel de fraude, basé sur des comportements de retraits coordonnés.

Cette solution n’a pas été modifiée depuis sa mise en place, et le suivi étant trimestriel, nous enverrons les prochains résultats en février 2026.

Je reste à votre disposition pour toute information complémentaire ou clarification si nécessaire.

Cordialement,
Nihad Senhadji


---

Si tu veux, je peux aussi te faire une version encore plus courte et simple, parfaite pour des non-techniques, qui tient en 5-6 phrases.
Veux‑tu que je fasse ça ?











Comme le suivi est trimestriel, nous enverrons les prochains résultats en février 2025.


Cet algorithme n’a pas été modifié depuis sa mise en place


Afin de détecter les communautés de fraude, nous utilisons un algorithme appelé Louvain, qui permet de regrouper automatiquement les clients ayant des comportements similaires.


Bonjour,

Dans le cadre du projet AI Sentinilia dédié à la détection de communautés de fraude, la méthodologie utilisée repose sur les étapes suivantes :

- Les liens entre clients sont d’abord identifiés à partir des critères suivants :

   - la localisation du GAB (ATM) où les retraits ont été effectués ;
   - la fréquence des retraits ;
   - des coïncidences entre plusieurs clients lorsque les retraits ont eu lieu :
      - à la même date,
      - au même GAB,
      - avec un montant supérieur ou égal à 500 euros,
      - et ce au moins 5 fois entre deux clients ou plus.

- Lorsque ces conditions sont réunies, les clients sont considérés comme liés, car leurs comportements de retrait présentent des similitudes répétées et inhabituelles.

- La méthode de Louvain est ensuite utilisée pour regrouper automatiquement les clients liés :

   - elle permet de constituer des communautés de clients fortement connectés ;
   - chaque communauté représente un groupe potentiel de fraude, basé sur des comportements de retraits coordonnés.

Je reste à votre disposition pour toute information complémentaire ou clarification si nécessaire.

Cordialement,

Nihad Senhadji



















Bonjour,

Je vous informe que j’ai annulé la version du fichier déjà envoyée précédemment.
Nous sommes actuellement en train de finaliser la nouvelle version, et je vous l’enverrai dès que ce sera terminé.

Merci pour votre compréhension.

Cordialement,
Nihad Senhadji

Bonjour,

Veuillez trouver ci-joint le fichier demandé, comme convenu.

Nous vous enverrons la version finale, contenant l’ensemble des feuilles, dans la soirée.

Cordialement,
Nihad Senhadji


Les colonnes du fichier fournissent toutes les informations nécessaires — identité, dates, lieux et références clients — afin de faciliter et structurer l’analyse des équipes sur ces cas sensibles.


La deuxième feuille, avec un score de similarité inférieur à 95 %, correspond aux cas où le modèle d’IA a permis de réduire une grande partie de la recherche manuelle.
Dans cette feuille, tous les noms qui ne figurent pas dans notre base clients Atlas sont automatiquement écartés, ce qui élimine les faux positifs et allège considérablement le travail des équipes opérationnelles.
🔹 La troisième feuille, avec un score de similarité supérieur à 95 %, regroupe les noms nécessitant une analyse approfondie par l’équipe opérationnelle, car ils présentent une forte probabilité de correspondance.










🎬 Script vidéo – Présentation de Decad AI Solution

> Bonjour,

Dans cette vidéo, je vais vous présenter Decad AI Solution, accessible via un lien de web application Domino.

On commence par l’insertion du fichier DECAD, envoyé par la Banque d’Algérie, directement dans l’application.

Ensuite, un simple clic permet de lancer le traitement. La solution s’appuie sur un modèle d’intelligence artificielle exécuté en arrière-plan, avec un temps d’exécution d’environ 10 minutes.

Une fois le traitement terminé, on clique sur “Résultats” afin de télécharger le fichier Excel de sortie, comme vous pouvez le voir à l’écran.

Le fichier Excel généré contient trois feuilles distinctes :

🔹 La première feuille, appelée Pièce d’audit, regroupe les résultats avec des scores de similarité inférieurs et supérieurs à 95 %.

🔹 La deuxième feuille contient les résultats avec un score inférieur à 95 %, correspondant aux cas non identifiés par le modèle d’IA.

🔹 La troisième feuille regroupe les résultats avec un score supérieur à 95 %, qui représentent les cas prioritaires à analyser par l’équipe opérationnelle.

Il est important de noter que les trois feuilles contiennent exactement les mêmes colonnes.

Voici la liste des colonnes suivantes :

COLONNE 1 et 2 DECAD

Nom interdits chequiers

Raison sociale base atlas

Nom abrégé tiers

Date naissance interdits chequiers ET DE NOTRE BASE ATLAS

Date de création (entreprise)

Date naissance Match

Date de création Match

Lieu naissance interdits chequier ET DE LA BASE atlas

Lieu naissance

Lieu naissance Match

Date d'effet et de levée

Compte client

Code catégorie client

Id tiers

Score de similarité


Grâce à cette structuration, Decad AI Solution permet de réduire le temps de traitement, de prioriser les analyses, et de sécuriser le processus de contrôle grâce à l’intelligence artificielle.

Merci pour votre attention.




---

If you want, I can also make a version more “oral / dynamique”, so it sounds natural when you speak it in the video, like a real demo narration.

Do you want me to do that?



















Bonjour,

Merci pour votre message et pour la transmission de l’Expression de Besoin (EDB) relative à l’automatisation du processus de traitement des agios réservés et de leurs reprises liées aux clients douteux – périmètre EPS.

Suite à l’analyse de cette EDB et conformément aux éléments partagés, je vous prie de trouver en pièce jointe les résultats de l’APA relatifs au mois de décembre 2025.

Ces résultats correspondent uniquement au traitement du mois de décembre 2025 et constituent la sortie actuelle de l’APA à ce stade.
Le fichier Excel de résultats contient à la fois des clients sains et des clients douteux.

Ils sont le fruit de plusieurs ateliers de travail menés avec Mouna, au cours desquels nous avons réalisé de nombreuses vérifications manuelles, ainsi que des rapprochements détaillés entre les résultats APA et les résultats manuels du mois d’octobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les règles de gestion appliquées dans le cadre de cette APA, en cohérence avec les besoins fonctionnels et techniques décrits dans l’EDB et avec la recommandation de la Banque d’Algérie visant à remplacer le traitement manuel actuellement réalisé par l’équipe DRAC.

Il s’agit actuellement d’une phase de test. Il est proposé de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilité du dispositif et de permettre la validation définitive des résultats par vos soins.

Par ailleurs, l’APA sera livrée la semaine prochaine afin d’être déployée et exécutée sur les postes des utilisateurs concernés.
À cet effet, nous aurons besoin :

- de l’installation de l’APA sur les postes utilisateurs,
- ainsi que de la mise à disposition d’un lien d’accès vers l’emplacement de dépôt des fichiers de sortie.

Je reste bien entendu à votre disposition, ainsi qu’Amina, pour tout complément d’information ou si vous souhaitez planifier un point d’échange.

Cordialement,
Nihad Senhadji

---

If you want, I can now:

- make a very formal / regulatory version (signature–engagement context), or
- shorten it for a manager-level reply.












Le fichier Excel de résultats contient à la fois des clients sains et des clients douteux


Bonjour,

Merci pour votre message et pour les éléments transmis lors de notre échange de jeudi.

Suite à l’analyse de l’expression de besoin relative à l’automatisation du traitement des agios réservés leasing, je vous prie de trouver en pièce jointe les résultats de l’APA relatifs au mois de décembre 2025.

Ces résultats correspondent uniquement au traitement du mois de décembre 2025 et constituent la sortie actuelle de l’APA à ce stade.

Ils sont le fruit de plusieurs ateliers de travail menés avec Mouna, au cours desquels nous avons réalisé de nombreuses vérifications manuelles, ainsi que des rapprochements détaillés entre les résultats APA et les résultats manuels du mois d’octobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les règles de gestion appliquées dans le cadre de cette APA.

Il s’agit actuellement d’une phase de test. Il est proposé de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilité du dispositif et de permettre la validation définitive des résultats par vos soins.

Par ailleurs, l’APA sera livrée la semaine prochaine afin d’être déployée et exécutée sur les postes des utilisateurs concernés.
À cet effet, nous aurons besoin :

- de l’installation de l’APA sur les postes utilisateurs,
- ainsi que de la mise à disposition d’un lien d’accès vers l’emplacement de dépôt des fichiers de sortie.

Je reste bien entendu à votre disposition pour tout complément d’information, retour ou ajustement nécessaire.

Cordialement,
Nihad Senhadji

---

If you want, I can also adapt the wording to:

- explicitly request






Bonjour,

Merci pour votre message et pour les éléments transmis lors de notre échange de jeudi.

Suite à l’analyse de l’expression de besoin relative à l’automatisation du traitement des agios réservés leasing, je vous prie de trouver en pièce jointe les résultats de l’APA.

Ces résultats correspondent uniquement au traitement du mois en cours et constituent la sortie actuelle de l’APA à ce stade.

Ils sont le fruit de plusieurs ateliers de travail menés avec Mouna, au cours desquels nous avons réalisé de nombreuses vérifications manuelles, ainsi que des rapprochements détaillés entre les résultats APA et les résultats manuels du mois d’octobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les règles de gestion appliquées dans le cadre de cette APA.

Il s’agit actuellement d’une phase de test. Il est proposé de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilité du dispositif et de permettre la validation définitive des résultats par vos soins.

Par ailleurs, l’APA sera livrée la semaine prochaine afin d’être déployée et exécutée sur les postes des utilisateurs concernés.
À cet effet, nous aurons besoin :

- de l’installation de l’APA sur les postes utilisateurs,
- ainsi que de la mise à disposition d’un lien d’accès vers l’emplacement de dépôt des fichiers de sortie.

Je reste bien entendu à votre disposition pour tout complément d’information, retour ou ajustement nécessaire.

Cordialement,
Nihad Senhadji

---

If you want, I can also:

- slightly strengthen the request for validation, or
- adapt the wording to a more regulatory / audit-compliant style for DRAC.






Bonjour [Nom du manager],

Je souhaite vous partager un point de situation concernant l’avancement des projets APA.

1. Avancement général (APA ESP & APA Arrêté)
Suite aux différents workshops organisés avec la DRAC, l’ensemble des règles de gestion a été confirmé et validé pour les deux projets.
Un rapprochement a également été réalisé sur la base de leurs fichiers de novembre 2025.

2. Projet APA ESP
Un seul point reste identifié : un problème de code fusion concernant un compte unique.
La DRAC m’a confirmé que ce point est déjà porté à votre connaissance.

3. Projet APA Arrêté
Un seul point est actuellement en suspens :

- Un écart lié au solde du compte à vue.
À ce stade, la DRAC n’est pas en mesure d’identifier la méthode ou la source utilisée pour le calcul de ce solde et doit effectuer des vérifications de son côté avant de nous apporter un retour définitif.

4. Projet APA Leasing
Le projet APA Leasing n’a pas encore été démarré à ce stade.

Vous trouverez ci-dessous le lien regroupant les éléments pour les projets APA ESP et APA Arrêté :
[Lien à insérer]

Je reste à votre disposition pour toute précision complémentaire ou pour un point de suivi.

Cordialement,
Nihad Senhadji






your thanks:

"Je tiens à vous remercier pour votre accompagnement et vos conseils tout au long de l’année, et je souhaite continuer à contribuer activement à la réussite des objectifs de la Banque, tout en participant aux projets IA et métiers."

It’s professional, appreciative, and forward-looking.

I can also make a slightly shorter, sharper version if you want it for an email reply.

You can write it like this:

"Je souhaite continuer à contribuer activement à la réussite des objectifs de la Banque, tout en participant aux projets IA et métiers."

It’s concise, professional, and inclusive of both AI and other banking projects.

If you want, I can also suggest 2–3 alternative formulations that sound slightly more executive.


Understood. We need a leadership-oriented tone without implying you want to replace him—showing responsibility, initiative, and team support while still highlighting AI. Here’s a refined version:


---

Bonjour [Nom du manager],

Merci pour votre retour et pour l’évaluation de ma contribution en 2025. J’ai bien pris en compte les axes d’amélioration identifiés, notamment sur le cadrage des besoins métiers, la structuration des travaux et la clarté de la communication.

Je souhaite exprimer mon objectif professionnel, qui est d’évoluer vers un rôle sur des projets en Intelligence Artificielle, avec des responsabilités clairement définies et des livrables à impact opérationnel.

Je reste par ailleurs pleinement engagée à soutenir l’ensemble des projets bancaires et à accompagner l’équipe dans l’organisation et la coordination des travaux, afin de garantir la qualité et l’efficacité des livrables, tout en contribuant activement à la réussite des projets IA et métiers.

Cordialement,
Nihad Senhadji


---

This version:

Shows initiative and readiness to support the team

Positions you for AI projects

Avoids sounding like you want to replace him


If you want, I can make an even snappier, executive-style version that fits in one paragraph. Do you want me to do that?
