class BM25Reranker:
    def __init__(self, k=5):
        self.k = k
        self.bm25 = None
        self.documents = None

    def fit(self, documents):
        # documents: List[dict] with "text" keys
        tokenized_corpus = [doc["text"].lower().split() for doc in documents]
        self.documents = documents
        self.bm25 = BM25Okapi(tokenized_corpus)
        return self

    def rerank(self, query: str, documents: List[dict]):
        if not self.bm25:
            self.fit(documents)

        tokenized_query = query.lower().split()
        scores = self.bm25.get_scores(tokenized_query)

        if len(scores) != len(documents):
            raise ValueError(f"Mismatch between scores ({len(scores)}) and documents ({len(documents)})")

        # Correct here:
        scored_docs = [(score, doc) for score, doc in zip(scores, documents)]
        sorted_docs = sorted(scored_docs, key=lambda x: x[0], reverse=True)

        return [doc for _, doc in sorted_docs[:self.k]]






import datetime
import json
import os

class CustomRetrieverWithHistory:
    def __init__(self, collection, chat_history=None, reranker=None, top_k=4):
        self.collection = collection  # ChromaDB collection
        self.chat_history = chat_history or []
        self.reranker = reranker or PerRanker(k=top_k)

    def get_relevant_documents(self, query: str):
        # Step 1: Initial retrieval
        results = self.collection.query(
            query_texts=[query],
            include=["documents", "metadatas"]
        )

        documents = [{"text": doc, "metadata": meta} for doc, meta in zip(results["documents"][0], results["metadatas"][0])]

        # Step 2: If history exists, expand the query
        if self.chat_history:
            recent_history = self.chat_history[-3:] if len(self.chat_history) >= 3 else self.chat_history
            history_context = " ".join([h["query"] for h in recent_history])

            enhanced_query = query + " " + history_context

            extra_results = self.collection.query(
                query_texts=[enhanced_query],
                include=["documents", "metadatas"]
            )

            extra_docs = [{"text": doc, "metadata": meta} for doc, meta in zip(extra_results["documents"][0], extra_results["metadatas"][0])]
            documents += extra_docs

        # Step 3: Rerank and return top documents
        top_docs = self.reranker.rerank(query, documents)
        return top_docs

    def update_history(self, query, answer):
        self.chat_history.append({
            "query": query,
            "answer": answer,
            "timestamp": str(datetime.datetime.now())
        })

    def save_history(self, file_path="chat_history.json"):
        try:
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(self.chat_history, f, indent=2, ensure_ascii=False)
            print(f"Saved chat history to {file_path}.")
        except Exception as e:
            print(f"Error saving chat history: {e}")

    def load_history(self, file_path="chat_history.json"):
        if os.path.exists(file_path):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    state = json.load(f)
                self.chat_history = state.get("chat_history", [])
                print(f"Loaded chat history from {file_path}.")
            except Exception as e:
                print(f"Error loading chat history: {e}")
        else:
            print(f"No chat history file found at {file_path}. Starting fresh.")








import json

# --- FUNCTIONS YOU ADDED ---

def extract_source_files(documents):
    """
    Extract source files from document metadata.
    """
    sources = []
    for doc in documents:
        if "source_file" in doc.get("metadata", {}):
            sources.append(doc["metadata"]["source_file"])
        elif "source" in doc.get("metadata", {}):
            sources.append(doc["metadata"]["source"])
    return list(set(sources))


def save_chat_history(chat_history, filename="chat_history.json"):
    """
    Save the chat history to a JSON file.
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(chat_history, f, indent=2, ensure_ascii=False)
        print(f"Chat history saved to {filename}")
    except Exception as e:
        print(f"Error saving chat history: {e}")

# --- RAG WORKFLOW ---

# Initial user query
query = "Explique-moi l'architecture du système bancaire DARA."

# 1. Generate alternative queries
alternative_queries = generate_alternative_queries(query)

# Always include the original query too
all_queries = [query] + alternative_queries

all_results = []

# 2. For each query, retrieve context and generate answer
for q in all_queries:
    # Retrieve documents
    context = retriever.get_relevant_documents(q)
    
    # Generate answer
    answer = generate_response(q, context)
    
    # Store everything
    all_results.append({
        "query": q,
        "answer": answer,
        "context": context,
        "num_sources": len(context)
    })

# 3. Pick the result with the most source documents
best_result = max(all_results, key=lambda x: x['num_sources'])

final_query = best_result["query"]
final_answer = best_result["answer"]
final_context = best_result["context"]

# 4. Generate explanation of the best answer
explanation = generate_answer_explanation(final_query, final_answer, final_context)

# 5. Evaluate the RAG response
evaluation = evaluate_rag_response(final_query, final_answer, final_context)

# 6. Extract source files
sources = extract_source_files(final_context)

# 7. Update and save retriever history
retriever.update_history(final_query, final_answer)
retriever.save_history()

# 8. Save full chat history
chat_history = {
    "original_query": query,
    "alternative_queries": alternative_queries,
    "final_query_used": final_query,
    "answer": final_answer,
    "explanation": explanation,
    "evaluation": evaluation,
    "sources": sources
}
save_chat_history(chat_history)

# --- OUTPUT ---
print("\n=== FINAL OUTPUT ===")
print("Answer:", final_answer)
print("\nExplanation:", explanation)
print("\nEvaluation:", evaluation)
print("\nSources:", sources)






def extract_source_files(documents):
    """Extract source files from document metadata."""
    sources = []

    for doc in documents:
        # Ensure 'metadata' is accessed properly as a dictionary
        if "metadata" in doc:
            metadata = doc["metadata"]
            # Check for 'source_file' in metadata
            if "source_file" in metadata:
                sources.append(metadata["source_file"])
            elif "source" in metadata:
                sources.append(metadata["source"])

    # Remove duplicates by converting to a set and then back to a list
    return list(set(sources))




def evaluate_rag_response(query, answer, retrieved_docs, feedback=None):
    """
    Evaluate the quality of a RAG (Retrieval-Augmented Generation) response based on various metrics.
    """
    evaluation = {}

    # 1. Document relevance score
    if isinstance(query, list):
        query = " ".join(str(q) for q in query)
    query_tokens = set(query.lower().split())

    relevant_docs = 0
    for doc in retrieved_docs:
        text = doc.get('content') or doc.get('page_content') or ''
        doc_tokens = set(text.lower().split())
        overlap = len(query_tokens.intersection(doc_tokens)) / len(query_tokens) if query_tokens else 0
        if overlap > 0.2:  # Simple relevance threshold
            relevant_docs += 1

    doc_relevance = relevant_docs / len(retrieved_docs) if retrieved_docs else 0
    evaluation["document_relevance"] = round(doc_relevance * 10, 2)  # Score out of 10

    # 2. Source diversity
    sources = set()
    for doc in retrieved_docs:
        if "metadata" in doc and ("source_file" in doc["metadata"] or "source" in doc["metadata"]):
            sources.add(doc["metadata"].get("source_file") or doc["metadata"].get("source"))

    source_diversity = min(len(sources) / 3, 1.0)  # Normalize to maximum of 1
    evaluation["source_diversity"] = round(source_diversity * 10, 2)  # Score out of 10

    # 3. Answer relevance
    if isinstance(answer, list):
        answer = " ".join(str(a) for a in answer)
    answer_tokens = set(answer.lower().split())
    query_answer_overlap = len(query_tokens.intersection(answer_tokens)) / len(query_tokens) if query_tokens else 0
    evaluation["answer_query_alignment"] = round(query_answer_overlap * 10, 2)  # Score out of 10

    # 4. User feedback score
    if feedback is not None:
        evaluation["user_feedback"] = feedback  # Expected to be between 1-5

    # 5. Hallucination risk
    contains_source_citation = any(keyword in answer.lower() for keyword in ["source", "doc", "provient", "extrait"])
    evaluation["hallucination_risk"] = "Low" if contains_source_citation else "Medium"

    # Overall score (weighted average)
    weights = {
        "document_relevance": 0.4,
        "source_diversity": 0.1,
        "answer_query_alignment": 0.3,
    }
    overall_score = sum(evaluation[key] * weights[key] for key in weights.keys() if key in evaluation)
    evaluation["overall_score"] = round(overall_score, 2)  # Score out of 10

    return evaluation



# Prepare the documents summary safely
    sources_summary = "\n".join([
        (doc.get('content') or doc.get('page_content') or "")[:300] 
        for doc in documents[:5]
    ])


# --- FUNCTIONS YOU ADDED ---

def extract_source_files(documents):
    """
    Extract source files from document metadata.
    """
    sources = []
    for doc in documents:
        if "source_file" in doc.get("metadata", {}):
            sources.append(doc["metadata"]["source_file"])
        elif "source" in doc.get("metadata", {}):
            sources.append(doc["metadata"]["source"])
    return list(set(sources))


def save_chat_history(chat_history, filename="chat_history.json"):
    """
    Save the chat history to a JSON file.
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(chat_history, f, indent=2, ensure_ascii=False)
        print(f"Chat history saved to {filename}")
    except Exception as e:
        print(f"Error saving chat history: {e}")

# --- RAG WORKFLOW ---

query = "Explique-moi l'architecture du système bancaire DARA."

# 1. Generate alternative queries (optional)
alternatives = generate_alternative_queries(query)

# 2. Retrieve documents
context = retriever.get_relevant_documents(query)

# 3. Generate final answer
answer = generate_response(query, context)

# 4. Generate explanation of the answer
explanation = generate_answer_explanation(query, answer, context)

# 5. Evaluate the RAG response
evaluation = evaluate_rag_response(query, answer, context)

# 6. Extract source files
sources = extract_source_files(context)

# 7. Update and save retriever history
retriever.update_history(query, answer)
retriever.save_history()

# 8. (Optional) Save full chat history
chat_history = {
    "query": query,
    "alternative_queries": alternatives,
    "answer": answer,
    "explanation": explanation,
    "evaluation": evaluation,
    "sources": sources
}
save_chat_history(chat_history)

# --- OUTPUT ---
print("Answer:", answer)
print("Explanation:", explanation)
print("Evaluation:", evaluation)
print("Sources:", sources)
