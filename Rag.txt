def process_and_submit_feedback(query: str, context: list, chat_history: list, rating: int, comment=None):
    # Generate alternative phrasings of the query
    alt_queries = generate_alternative_queries(query)
    all_results = []
    
    # Process each alternative query
    for q in alt_queries:
        # Prepare the prompt for each alternative query
        response = prepare_response(q, context, chat_history)
        all_results.append({
            "answer": response,
            "source_documents": context  # assuming context provides the source documents
        })
    
    # Pick the result with the most source documents
    best_result = max(all_results, key=lambda x: len(x['source_documents']))
    answer, sources = best_result['answer'], best_result['source_documents']
    
    # Generate additional context
    explanation = generate_answer_explanation(query, answer, sources)
    commentary = generate_technical_commentary(query, answer)
    evaluation = evaluate_rag_response(query, answer, sources)
    
    # Add source files information if not already in the answer
    source_files = extract_source_files(sources)
    if source_files and "Sources:" not in answer:
        answer += f"\n\nSources: {', '.join(source_files)}"
    
    # Update chat history
    chat_history.append({'question': query, 'answer': answer})  # Save the current Q&A pair
    save_chat_history(chat_history)  # Assuming there's a function to save the history
    
    # Submit the feedback
    feedback_tracker.save_feedback(query, answer, rating, comment)

    return {
        'answer': answer,
        'sources': sources,
        'source_files': source_files,
        'explanation': explanation,
        'commentary': commentary,
        'evaluation': evaluation
    }

# Usage example:
user_query = "Dans quelle table puis-je trouver les informations sur les transactions clients ?"
context = [
    {"content": "La table des transactions contient les informations relatives aux opérations bancaires.", "metadata": {"source": "transactions.pdf"}},
    {"content": "La table des clients stocke les informations personnelles des clients.", "metadata": {"source": "clients.pdf"}}
]
chat_history = [
    {"question": "Où se trouvent les informations sur les transactions?", "answer": "Les informations sur les transactions sont dans la table 'transactions'."},
    {"question": "Où puis-je trouver les informations des clients?", "answer": "Les informations des clients sont dans la table 'clients'."}
]

# Get the response and submit feedback
result = process_and_submit_feedback(user_query, context, chat_history, rating=5, comment="Excellente réponse!")

print('Réponse:', result['answer'])
print('\nExplication:', result['explanation'])
print('\nCommentaire technique:', result['commentary'])
print('\nÉvaluation:', result['evaluation'])
print('\nSources des documents:', result['source_files'])











# Example context with metadata and content
context = [
    {"content": "La table des transactions contient les informations relatives aux opérations bancaires.", "metadata": {"source": "transactions.pdf"}},
    {"content": "La table des clients stocke les informations personnelles des clients.", "metadata": {"source": "clients.pdf"}}
]

# Example chat history
chat_history = [
    {"question": "Où se trouvent les informations sur les transactions?", "answer": "Les informations sur les transactions sont dans la table 'transactions'."},
    {"question": "Où puis-je trouver les informations des clients?", "answer": "Les informations des clients sont dans la table 'clients'."}
]

# Example query
query = "Dans quelle table puis-je trouver les informations sur les comptes clients?"

# Get the response
response = prepare_response(query, context, chat_history)




def extract_source_files(documents):
    """Extract source files from document metadata"""
    sources = []
    for doc in documents:
        if "source_file" in doc.metadata:
            sources.append(doc.metadata["source_file"])
        elif "source" in doc.metadata:
            sources.append(doc.metadata["source"])
    return list(set(sources))

def process_query(query: str):
    """Process a user query by generating alternatives and picking the best answer."""
    alt_queries = generate_alternative_queries(query)
    all_results = []
    
    for q in alt_queries:
        res = rag_chain({"question": q, "chat_history": retriever.chat_history})  # Change here
        all_results.append(res)
    
    # Pick best result (optional: here just pick the one with the longest answer)
    best_result = max(all_results, key=lambda x: len(x['source_documents']))
    answer, sources = best_result['answer'], best_result['source_documents']
    
    # Generate additional context
    explanation = generate_answer_explanation(query, answer, sources)
    commentary = generate_technical_commentary(query, answer)
    evaluation = evaluate_rag_response(query, answer, sources)
    
    # Add source files if needed
    source_files = extract_source_files(sources)
    if source_files and "Sources:" not in answer:
        answer += f"\n\nSources: {', '.join(source_files)}"

    # Update chat history
    retriever.update_history(query, answer)
    retriever.save_history()
    
    return {
        'answer': answer,
        'sources': sources,
        'source_files': source_files,
        'explanation': explanation,
        'commentary': commentary,
        'evaluation': evaluation
    }

def submit_feedback(query, answer, rating, comment=None):
    """Submit user feedback for a response."""
    return feedback_tracker.save_feedback(query, answer, rating, comment)

# Usage example
user_query = "Dans quelle table puis-je trouver les informations sur les transactions clients ?"
result = process_query(user_query)

print('Réponse:', result['answer'])
print('\nExplication:', result['explanation'])
print('\nCommentaire technique:', result['commentary'])
print('\nÉvaluation:', result['evaluation'])
print('\nSources des documents:', result['source_files'])

# Example feedback
# submit_feedback(user_query, result['answer'], 5, "Excellente réponse!")














import json
import datetime

def evaluate_rag_response(query, answer, retrieved_docs, feedback=None):
    """
    Evaluate the quality of a RAG (Retrieval-Augmented Generation) response based on various metrics.
    """
    evaluation = {}

    # 1. Document relevance score - Compare query with retrieved documents
    query_tokens = set(query.lower().split())
    relevant_docs = 0

    for doc in retrieved_docs:
        doc_tokens = set(doc.page_content.lower().split())
        overlap = len(query_tokens.intersection(doc_tokens)) / len(query_tokens) if query_tokens else 0
        if overlap > 0.2:  # Simple relevance threshold
            relevant_docs += 1

    doc_relevance = relevant_docs / len(retrieved_docs) if retrieved_docs else 0
    evaluation["document_relevance"] = round(doc_relevance * 10, 2)  # Score out of 10

    # 2. Source diversity - Are we pulling from different sources?
    sources = set()
    for doc in retrieved_docs:
        if "source_file" in doc.metadata:
            sources.add(doc.metadata["source_file"])

    source_diversity = min(len(sources) / 3, 1.0)  # Normalize to maximum of 1
    evaluation["source_diversity"] = round(source_diversity * 10, 2)  # Score out of 10

    # 3. Answer relevance - Analyze how well the answer matches the query
    answer_tokens = set(answer.lower().split())
    query_answer_overlap = len(query_tokens.intersection(answer_tokens)) / len(query_tokens) if query_tokens else 0
    evaluation["answer_query_alignment"] = round(query_answer_overlap * 10, 2)  # Score out of 10

    # 4. User feedback score, if available
    if feedback is not None:
        evaluation["user_feedback"] = feedback  # Expected to be between 1-5

    # 5. Hallucination risk - Check if answer cites sources properly
    contains_source_citation = any(keyword in answer.lower() for keyword in ["source", "pdf", "provien", "extract"])
    evaluation["hallucination_risk"] = "Low" if contains_source_citation else "Medium"

    # Overall score (weighted average)
    weights = {
        "document_relevance": 0.4,
        "source_diversity": 0.3,
        "answer_query_alignment": 0.3
    }
    overall_score = sum(evaluation[key] * weights[key] for key in weights.keys())
    evaluation["overall_score"] = round(overall_score, 2)  # Score out of 10

    return evaluation

class UserFeedbackTracker:
    """Track user feedback for RAG responses."""
    
    def __init__(self, feedback_file="user_feedback.json"):
        self.feedback_file = feedback_file
        self.feedback_history = self._load_feedback()

    def _load_feedback(self):
        """Load feedback history from a JSON file."""
        try:
            with open(self.feedback_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except Exception as e:
            print(f"Error loading feedback: {e}")
            return []

    def save_feedback(self, query, answer, rating, comment=None):
        """Save new user feedback entry."""
        feedback_entry = {
            "timestamp": str(datetime.datetime.now()),
            "query": query,
            "answer": answer[:100] + "...",  # Store a shortened version of the answer
            "rating": rating,
            "comment": comment
        }

        self.feedback_history.append(feedback_entry)

        with open(self.feedback_file, 'w') as f:
            json.dump(self.feedback_history, f, indent=2)

        print(f"Feedback saved. Current rating: {rating}/5")
        return rating

    def get_average_rating(self):
        """Calculate the average rating from the stored feedback."""
        if not self.feedback_history:
            return None

        ratings = [entry["rating"] for entry in self.feedback_history if "rating" in entry]
        return sum(ratings) / len(ratings) if ratings else None

# Initialize feedback tracker
feedback_tracker = UserFeedbackTracker()






import httpx
from openai import AzureOpenAI

# Initialize AzureOpenAI client globally
client = AzureOpenAI(
    api_version="AZURE_AOAI_API_VERSION",
    azure_endpoint="APIGEE_ENDPOINT",
    api_key="FAKE_KEY",
    http_client=httpx.Client(verify=False)
)

def generate_alternative_queries(query: str) -> list:
    """Generate 3 alternative versions of the user query."""
    
    prompt = f"""Génère 3 façons alternatives de poser la question suivante à propos d'une base de données bancaire.
Rends les alternatives spécifiques et concentrées sur la structure de la base de données et l'emplacement des données.

Question originale: {query}

Retourne uniquement les questions, une par ligne."""
    
    completion = client.chat.completions.create(
        model="gpt-4-32k",
        messages=[
            {"role": "system", "content": "You are a query generation assistant."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7
    )
    
    result = completion.choices[0].message.content.strip()
    alts = [q.strip() for q in result.split('\n') if q.strip()]
    
    return alts + [query]  # Add original query at the end

def generate_answer_explanation(query: str, answer: str, sources: list) -> str:
    """Generate an explanation on how the answer was formulated based on the sources."""
    
    sources_summary = "\n".join([doc.page_content[:300] for doc in sources[:2]])  # only first 2 sources
    
    prompt = f"""En tant qu'expert en base de données bancaire, explique comment tu as formulé la réponse suivante à la question posée.
Fais référence aux données spécifiques et à la structure de la base de données bancaire.

Question: {query}
Réponse: {answer}

Sources principales:
{sources_summary}

Explique ton raisonnement en français en te concentrant sur:
1. Comment les sources ont influencé ta réponse
2. Les éléments clés de la structure de la base de données mentionnés
3. Les relations entre les tables identifiées
4. Toute implication ou considération technique importante

Explication (en 3-5 phrases):"""
    
    completion = client.chat.completions.create(
        model="gpt-4-32k",
        messages=[
            {"role": "system", "content": "You are a banking database expert."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.5
    )
    
    explanation = completion.choices[0].message.content.strip()
    return explanation

def generate_technical_commentary(query: str, answer: str) -> str:
    """Generate a short technical commentary on the database structure mentioned."""
    
    prompt = f"""En tant qu'expert en bases de données bancaires, fournis un commentaire technique bref
sur les aspects de la structure de la base de données mentionnés dans cette question et réponse.

Question: {query}
Réponse: {answer}

Fournis un commentaire technique (2-3 phrases) qui pourrait aider un développeur ou un analyste de données
à mieux comprendre les implications techniques de cette information:"""
    
    completion = client.chat.completions.create(
        model="gpt-4-32k",
        messages=[
            {"role": "system", "content": "You are a technical database analyst."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.5
    )
    
    commentary = completion.choices[0].message.content.strip()
    return commentary

















import httpx
from azure.identity import DefaultAzureCredential

# Initialize Azure OpenAI client
import openai

openai.api_type = "azure"
openai.api_version = "AZURE_ADAI_API_VERSION"  # <- Replace with your version
openai.api_base = "APIGEE_ENDPOINT"             # <- Replace with your endpoint
openai.api_key = "FAKE_KEY"                      # <- Replace with your key

def prepare_response(query, context, chat_history):
    # 1. Extract sources
    sources = list(set([chunk["metadata"]["source"] for chunk in context]))
    sources_list = "\n".join(sources)
    
    # 2. Prepare context text
    context_text = "\n\n".join([chunk["content"] for chunk in context])

    # 3. Prepare chat history text
    chat_history_text = ""
    if chat_history:
        chat_history_text = "\n".join([f"Q: {q}\nA: {a}" for q, a in chat_history])
    
    # 4. Build the prompt
    prompt = f"""Vous êtes un assistant expert en documentation de base de données bancaire.
Votre objectif est de fournir des informations précises et spécifiques sur l'emplacement des données dans le système de base de données de la banque.

Informations contextuelles issues de la documentation de la base de données ci-dessous:
{context_text}

Compte tenu des informations contextuelles et de la question, fournissez une réponse détaillée qui indique clairement:
1. La ou les table(s) exacte(s) où se trouvent les données demandées
2. Les noms spécifiques des colonnes
3. Les relations avec d'autres tables
4. Les types de données et les contraintes si pertinent

Important: À la fin de votre réponse, indiquez clairement les sources d'où proviennent ces informations en listant les noms des fichiers PDF sources suivants:
{sources_list}

Soyez précis et technique. Si les informations ne peuvent pas être trouvées dans le contexte, reconnaissez-le clairement. N'inventez pas d'informations sur les noms de tables ou les structures.

Historique de la conversation:
{chat_history_text}

Question:
{query}
"""

    # 5. Call Azure OpenAI
    response = openai.ChatCompletion.create(
        engine="YOUR_DEPLOYMENT_NAME",  # <- Replace with your model deployment name
        messages=[
            {"role": "system", "content": "You are an assistant based on RAG (Retrieval-Augmented Generation)."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.5
    )
    
    # 6. Return the generated text
    return response['choices'][0]['message']['content']








from sentence_transformers import SentenceTransformer
import chromadb

# Step 1: Correct embedding function
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

def embed_function(texts):
    return embedding_model.encode(texts, normalize_embeddings=True).tolist()

# Step 2: Connect Chroma
chroma_client = chromadb.Client()

# Step 3: Get or create collection
collection = chroma_client.get_collection(
    "your_collection_name",
    embedding_function=embed_function
)

# Step 4: Now you can use collection.query() without any error!
results = collection.query(
    query_texts=["example query"],
    n_results=5,
    include=["documents"]
)

print(results)



from rank_bm25 import BM25Okapi
import json
import datetime
from typing import List

# BM25 Reranker
class BM25Reranker:
    """A document reranker using BM25 algorithm"""
    
    def __init__(self, k: int = 4):
        self.k = k
        self.bm25 = None
        self.document_map = {}
    
    def fit(self, documents):
        tokenized_corpus = [doc['text'].lower().split() for doc in documents]
        self.document_map = {i: doc for i, doc in enumerate(documents)}
        self.bm25 = BM25Okapi(tokenized_corpus)
        return self
    
    def rerank(self, query: str, documents: List[dict]):
        if not self.bm25:
            self.fit(documents)
        
        tokenized_query = query.lower().split()
        scores = self.bm25.get_scores(tokenized_query)
        
        # Sort documents based on scores
        scored_docs = [(score, documents[i]) for i, score in enumerate(scores)]
        sorted_docs = sorted(scored_docs, key=lambda x: x[0], reverse=True)
        
        return [doc for _, doc in sorted_docs[:self.k]]

# Custom Retriever with History
class CustomRetrieverWithHistory:
    def __init__(self, collection, chat_history=None, k=10, rerank_k=4):
        self.collection = collection  # This is the Chroma collection
        self.chat_history = chat_history or []
        self.k = k
        self.reranker = BM25Reranker(k=rerank_k)
    
    def get_relevant_documents(self, query: str):
        # Step 1: Vector search
        results = self.collection.query(
            query_texts=[query],
            n_results=self.k,
            include=["documents", "metadatas"]
        )
        
        # Extract documents
        docs = [{"text": doc, "metadata": meta} for doc, meta in zip(results["documents"][0], results["metadatas"][0])]
        
        # Step 2: If history exists, expand query
        if self.chat_history:
            recent_history = self.chat_history[-3:] if len(self.chat_history) > 3 else self.chat_history
            history_context = "\n".join([f"Q: {q}\nA: {a}" for q, a in recent_history])
            enhanced_query = f"{query} {history_context}"
            
            # Search again with enhanced query
            extra_results = self.collection.query(
                query_texts=[enhanced_query],
                n_results=5,
                include=["documents", "metadatas"]
            )
            
            extra_docs = [{"text": doc, "metadata": meta} for doc, meta in zip(extra_results["documents"][0], extra_results["metadatas"][0])]
            
            # Merge and remove duplicates
            all_docs = {d["text"]: d for d in docs + extra_docs}
            docs = list(all_docs.values())
        
        # Step 3: Rerank final set
        return self.reranker.rerank(query, docs)

    def update_history(self, query, answer):
        self.chat_history.append((query, answer))
    
    def save_history(self, file_path="bank_rag_state.json"):
        state = {
            "chat_history": self.chat_history,
            "timestamp": str(datetime.datetime.now())
        }
        with open(file_path, 'w') as f:
            json.dump(state, f)
    
    def load_history(self, file_path="bank_rag_state.json"):
        try:
            with open(file_path, 'r') as f:
                state = json.load(f)
                self.chat_history = state.get("chat_history", [])
                print(f"Loaded {len(self.chat_history)} previous conversations.")
        except FileNotFoundError:
            print("No previous history found.")
        except Exception as e:
            print(f"Error loading history: {e}")

# Example Usage

retriever = CustomRetrieverWithHistory(collection=collection, k=10, rerank_k=4)
retriever.load_history()

# Now you can call retriever.get_relevant_documents(query) anytime









# Assuming you already have chroma_client and collection created:
# chroma_client = chromadb.Client()
# collection = chroma_client.get_collection("your_collection_name")

retriever = CustomRetrieverWithHistory(collection=collection, k=10, rerank_k=4)

# Load previous chat history
retriever.load_history()

# Example query
query = "how to open a bank account?"
relevant_docs = retriever.get_relevant_documents(query)

for doc in relevant_docs:
    print(doc['content'])



from rank_bm25 import BM25Okapi
import json
import datetime

class BM25Reranker:
    def __init__(self, k: int = 4):
        self.k = k
        self.tokenized_corpus = None
        self.bm25 = None
        self.document_map = {}
    
    def fit(self, documents):
        tokenized_corpus = [doc['content'].lower().split() for doc in documents]
        self.document_map = {i: doc for i, doc in enumerate(documents)}
        self.tokenized_corpus = tokenized_corpus
        self.bm25 = BM25Okapi(tokenized_corpus)
        return self
    
    def rerank(self, query: str, documents):
        if not self.bm25:
            self.fit(documents)
        
        tokenized_query = query.lower().split()
        
        tokenized_docs = [doc['content'].lower().split() for doc in documents]
        temp_bm25 = BM25Okapi(tokenized_docs)
        scores = temp_bm25.get_scores(tokenized_query)
        
        scored_docs = [(score, documents[i]) for i, score in enumerate(scores)]
        sorted_docs = sorted(scored_docs, key=lambda x: x[0], reverse=True)
        
        return [doc for _, doc in sorted_docs[:self.k]]

class CustomRetrieverWithHistory:
    def __init__(self, collection, chat_history=None, k=10, rerank_k=4):
        self.collection = collection
        self.chat_history = chat_history or []
        self.reranker = BM25Reranker(k=rerank_k)
        self.k = k
    
    def similarity_search(self, query, k=10):
        """Use the Chroma collection to search."""
        results = self.collection.query(
            query_texts=[query],
            n_results=k,
            include=['documents', 'metadatas']
        )
        
        docs = []
        for doc, metadata in zip(results['documents'][0], results['metadatas'][0]):
            docs.append({
                "content": doc,
                "metadata": metadata
            })
        return docs

    def get_relevant_documents(self, query):
        docs = self.similarity_search(query, k=self.k)
        
        if self.chat_history and len(self.chat_history) > 0:
            recent_history = self.chat_history[-3:] if len(self.chat_history) > 3 else self.chat_history
            history_context = "\n".join([f"Q: {q}\nA: {a}" for q, a in recent_history])
            
            enhanced_query = f"{query} {history_context}"
            additional_docs = self.similarity_search(enhanced_query, k=5)
            
            unique_docs = {}
            for doc in docs + additional_docs:
                if doc['content'] not in unique_docs:
                    unique_docs[doc['content']] = doc
            
            docs = list(unique_docs.values())
        
        return self.reranker.rerank(query, docs)

    def update_history(self, query, answer):
        self.chat_history.append((query, answer))
    
    def save_history(self, file_path="bank_db_state.json"):
        state = {
            "chat_history": self.chat_history,
            "timestamp": str(datetime.datetime.now())
        }
        with open(file_path, 'w') as f:
            json.dump(state, f)
            
    def load_history(self, file_path="bank_db_state.json"):
        try:
            with open(file_path, 'r') as f:
                state = json.load(f)
                self.chat_history = state.get("chat_history", [])
                print(f"Loaded {len(self.chat_history)} previous conversations")
        except FileNotFoundError:
            print("No previous history found.")
        except Exception as e:
            print(f"Error loading history: {e}")






import weaviate

client = weaviate.Client(
    embedded_options=weaviate.embedded.EmbeddedOptions()
)



from sentence_transformers import SentenceTransformer
import weaviate

# Load BGE-M3 model
bge_model = SentenceTransformer("BAAI/bge-m3")

# Create Weaviate client
client = weaviate.Client("embedded")

WEAVIATE_CLASS = "BankDocChunk"

# Clean previous class if needed
if client.schema.exists(WEAVIATE_CLASS):
    client.schema.delete_class(WEAVIATE_CLASS)

# Create schema
client.schema.create_class({
    "class": WEAVIATE_CLASS,
    "vectorizer": "none",  # because we provide vectors ourselves
    "properties": [
        {"name": "text", "dataType": ["text"]},
        {"name": "source_file", "dataType": ["string"]},
        {"name": "source_type", "dataType": ["string"]},
        {"name": "page", "dataType": ["int"]},
    ],
})

# Now embed your documents manually
for doc in chunks:  # Assuming 'chunks' is a list of Document objects
    text = doc.page_content
    metadata = doc.metadata

    # Create the embedding vector
    vector = bge_model.encode(text, normalize_embeddings=True)

    # Prepare data object
    data_object = {
        "text": text,
        "source_file": metadata.get("source_file", ""),
        "source_type": metadata.get("source_type", ""),
        "page": metadata.get("page", -1),
    }

    # Insert into Weaviate
    client.data_object.create(
        data_object=data_object,
        class_name=WEAVIATE_CLASS,
        vector=vector.tolist()
    )

print(f"Inserted {len(chunks)} vector chunks successfully!")










from sentence_transformers import SentenceTransformer
from langchain_community.embeddings import HuggingFaceEmbeddings

# Load BGE-M3 model
bge_model = SentenceTransformer("BAAI/bge-m3")

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model=bge_model,
    encode_kwargs={"normalize_embeddings": True}
)




with open("all_documents.txt", "w", encoding="utf-8") as f: f.write("\n\n".join(doc.page_content for doc in docs))



with open("all_documents.txt", "w", encoding="utf-8") as f: f.write("\n\n".join(doc.page_content for doc in docs))









from langchain_community.document_loaders import PyPDFDirectoryLoader, CSVLoader, UnstructuredExcelLoader
from langchain.schema import Document
import glob
import os
import re
import string

DOCS_DIR = "/mnt/LLMS/Procedures"  # Your corrected path ✅

# Text cleaning function
def clean_text(text, lowercase=False):
    """Cleans raw text: removes unwanted characters, extra spaces, fixes multiple dots"""
    
    # Remove non-printable characters
    text = ''.join(filter(lambda x: x in string.printable, text))
    
    # Replace multiple consecutive dots (...) with a single dot (.)
    text = re.sub(r'\.{2,}', '.', text)
    
    # Replace multiple line breaks and spaces
    text = re.sub(r'\n+', '\n', text)  # collapse multiple newlines
    text = re.sub(r'[ \t]+', ' ', text)  # collapse multiple spaces

    # Remove leading/trailing spaces on each line
    text = "\n".join(line.strip() for line in text.splitlines())
    
    # Remove empty lines
    text = "\n".join([line for line in text.splitlines() if line.strip() != ""])
    
    if lowercase:
        text = text.lower()
    
    return text

# Apply cleaning to list of documents
def clean_documents(documents, lowercase=False):
    cleaned_docs = []
    for doc in documents:
        cleaned_text = clean_text(doc.page_content, lowercase=lowercase)
        cleaned_doc = Document(page_content=cleaned_text, metadata=doc.metadata)
        cleaned_docs.append(cleaned_doc)
    return cleaned_docs

# Loader function
def load_documents():
    documents = []
    
    # PDFs
    if os.path.exists(os.path.join(DOCS_DIR, "pdfs")):
        for file in glob.glob(os.path.join(DOCS_DIR, "pdfs", "*.pdf")):
            try:
                loader = PyPDFDirectoryLoader(os.path.dirname(file))
                docs = loader.load()
                file_name = os.path.basename(file)
                
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "pdf"
                
                documents.extend(docs)
                print(f"Loaded {file_name}")
            except Exception as e:
                print(f"Error loading {file}: {e}")

    # CSVs
    csv_dir = os.path.join(DOCS_DIR, "csv")
    if os.path.exists(csv_dir):
        for file in glob.glob(os.path.join(csv_dir, '*.csv')):
            try:
                docs = CSVLoader(file_path=file).load()
                file_name = os.path.basename(file)
                
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "csv"
                    
                documents.extend(docs)
                print(f"Loaded {file_name}")
            except Exception as e:
                print(f"Error loading {file}: {e}")

    # Excels
    xls_dir = os.path.join(DOCS_DIR, "excel")
    if os.path.exists(xls_dir):
        for file in glob.glob(os.path.join(xls_dir, '*.xls*')):
            try:
                docs = UnstructuredExcelLoader(file_path=file).load()
                file_name = os.path.basename(file)
                
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "excel"
                    
                documents.extend(docs)
                print(f"Loaded {file_name}")
            except Exception as e:
                print(f"Error loading {file}: {e}")

    print(f"Loaded {len(documents)} documents in total")
    
    # ✨ Clean the documents immediately after loading
    cleaned_documents = clean_documents(documents)
    
    print(f"Cleaned {len(cleaned_documents)} documents.")
    return cleaned_documents

# Call the function
docs = load_documents()
