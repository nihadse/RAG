import os
from pathlib import Path
import pandas as pd
from openpyxl import load_workbook
from openpyxl.cell.cell import MergedCell

def get_writable_cell(ws, row, col, merged_map):
    if (row, col) in merged_map:
        min_row, min_col = merged_map[(row, col)]
        return ws.cell(row=min_row, column=min_col)
    return ws.cell(row=row, column=col)

def normalize_arabic(text):
    if not text:
        return ""
    return (str(text)
            .replace("ÛŒ", "ÙŠ")
            .replace("Ùƒ", "Ùƒ")
            .replace("Ù€", "")
            .replace("\u200f", "")
            .replace("\u200e", "")
            .strip())

def find_merged_column_by_text(ws, header_lines, target_text):
    target_text = normalize_arabic(target_text)
    for col in range(1, ws.max_column + 1):
        header_parts = []
        for row in range(1, header_lines + 1):
            val = ws.cell(row=row, column=col).value
            if val:
                header_parts.append(normalize_arabic(val))
        full_header = "".join(header_parts)
        if target_text in full_header:
            return col
    return None

def collect_excel_files(folder):
    return list(Path(folder).rglob("*.xlsx"))

def fill_excel_from_csv(csv_path, excel_folder, output_folder):
    os.makedirs(output_folder, exist_ok=True)

    rc_headers = ["Ø±Ù‚Ù… Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ", "Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ", "registre du commerce", "Ø±Ù‚Ù… Ø§Ù„Ø³Ø¬Ù„ d'inscription"]
    loan_yn_header = "Ù…Ù† Ø§Ø³ØªÙØ§Ø¯ Ø§Ù„Ù…ØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠ Ù…Ù† ÙØ±Ø¹ Ø¨Ù†ÙƒÙŠ"
    import_yn_header = "Ù‡Ù„ Ù‚Ø§Ù… Ø§Ù„Ù…ØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠ Ø¨Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ø³ØªÙŠØ±Ø§Ø¯"
    loan_date_header = "Derniere date effet credit"
    import_date_header = "Derniere date import"

    csv_df = pd.read_csv(csv_path, dtype=str).fillna("")
    if "Registre commerce" not in csv_df.columns:
        print("X Registre commerce column missing in CSV.")
        return
    csv_df.set_index("Registre commerce", inplace=True)
    csv_data = csv_df.to_dict("index")

    excel_files = collect_excel_files(excel_folder)

    for file_path in excel_files:
        input_path = str(file_path)
        output_path = os.path.join(output_folder, os.path.relpath(input_path, excel_folder))
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        wb = load_workbook(input_path)
        updated = False

        for sheetname in wb.sheetnames:
            ws = wb[sheetname]
            merged_map = {}
            for merged in ws.merged_cells.ranges:
                for row in range(merged.min_row, merged.max_row + 1):
                    for col in range(merged.min_col, merged.max_col + 1):
                        merged_map[(row, col)] = (merged.min_row, merged.min_col)

            header_row = None
            header_map = {}

            for i in range(1, 100):
                row_vals = [str(cell.value).strip() if cell.value else "" for cell in ws[i]]
                for idx, val in enumerate(row_vals):
                    if normalize_arabic(val) in [normalize_arabic(h) for h in rc_headers]:
                        header_row = i
                        header_map["Registre commerce"] = idx + 1
                        break
                if header_row:
                    break

            if not header_row:
                print(f"No RC column found in {input_path}/ Sheet {sheetname}")
                continue

            header_map["loan date"] = find_merged_column_by_text(ws, 10, loan_date_header)
            header_map["import date"] = find_merged_column_by_text(ws, 10, import_date_header)

            for row in range(1, 11):
                for merged in ws.merged_cells.ranges:
                    cell = ws.cell(row=row, column=merged.min_col)
                    if not cell.value:
                        continue
                    val = str(cell.value).strip()
                    if normalize_arabic(val) == normalize_arabic(loan_yn_header):
                        header_map["loan yn_yes"] = merged.min_col
                        header_map["loan yn_no"] = merged.min_col + 1
                    elif normalize_arabic(val) == normalize_arabic(import_yn_header):
                        header_map["import yn_yes"] = merged.min_col
                        header_map["import yn_no"] = merged.min_col + 1

            row_num = header_row + 1
            max_row = ws.max_row

            while row_num <= max_row:
                rc = ws.cell(row=row_num, column=header_map["Registre commerce"]).value
                rc_key = str(rc).strip() if rc else None
                row_data = csv_data.get(rc_key) if rc_key else None

                for csv_col, (field_key, yn_val) in {
                    "beneficiaire credit": ("loan yn", "Ù†Ø¹Ù…"),
                    "Operation Import": ("import yn", "Ù†Ø¹Ù…"),
                    "Derniere date effet credit": ("loan date", None),
                    "Derniere date import": ("import date", None)
                }.items():
                    if "yn" in field_key:
                        yes_col = header_map.get(f"{field_key}_yes")
                        no_col = header_map.get(f"{field_key}_no")
                        if yes_col and no_col:
                            yes_cell = get_writable_cell(ws, row_num, yes_col, merged_map)
                            no_cell = get_writable_cell(ws, row_num, no_col, merged_map)
                            if row_data:
                                val = row_data.get(csv_col)
                                if isinstance(val, str) and val.strip().upper() == "X":
                                    if yn_val == "Ù†Ø¹Ù…":
                                        yes_cell.value = "/"
                                    elif yn_val == "Y":
                                        no_cell.value = "/"
                                    updated = True
                                elif not yes_cell.value and not no_cell.value:
                                    no_cell.value = "/"
                                    updated = True
                    elif "date" in field_key:
                        date_col = header_map.get(field_key)
                        if row_data and date_col:
                            date_val = row_data.get(csv_col)
                            if isinstance(date_val, pd.Series):
                                date_val = date_val.iloc[0]
                            if date_val:
                                date_cell = get_writable_cell(ws, row_num, date_col, merged_map)
                                if not date_cell.value:
                                    date_cell.value = date_val
                                    updated = True

                row_num += 1

        wb.save(output_path)
        print(f"{'Saved' if updated else 'No updates made'}: {output_path}")


âœ… Done! Iâ€™ve rewritten your full script to:

Handle Registre de commerce correctly even if it's missing temporarily.

Stop using break on empty values.

Avoid slowness from merged cell lookups.

Optimize row processing using .to_dict("index").


You can now run this version and it will:

âœ… Process rows like â€œMelissaâ€ even after an empty â€œSifoâ€.

âœ… Run significantly faster on large files.


Let me know if you want me to:

Add logging to track how many rows were updated.

Add multi-sheet progress bars.

Output a CSV log of changes made.







import os from pathlib import Path import pandas as pd from openpyxl import load_workbook from openpyxl.cell.cell import MergedCell

def get_writable_cell(ws, row, col, merged_map): if (row, col) in merged_map: min_row, min_col = merged_map[(row, col)] return ws.cell(row=min_row, column=min_col) return ws.cell(row=row, column=col)

def normalize_arabic(text): if not text: return "" return (str(text) .replace("ÛŒ", "ÙŠ") .replace("Ùƒ", "Ùƒ") .replace("Ù€", "") .replace("\u200f", "") .replace("\u200e", "") .strip())

def find_merged_column_by_text(ws, header_lines, target_text): target_text = normalize_arabic(target_text) for col in range(1, ws.max_column + 1): header_parts = [] for row in range(1, header_lines + 1): val = ws.cell(row=row, column=col).value if val: header_parts.append(normalize_arabic(val)) full_header = "".join(header_parts) if target_text in full_header: return col return None

def collect_excel_files(folder): return list(Path(folder).rglob("*.xlsx"))

def fill_excel_from_csv(csv_path, excel_folder, output_folder): os.makedirs(output_folder, exist_ok=True)

rc_headers = ["Ø±Ù‚Ù… Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ", "Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ", "registre du commerce", "Ø±Ù‚Ù… Ø§Ù„Ø³Ø¬Ù„ d'inscription"]
loan_yn_header = "Ù…Ù† Ø§Ø³ØªÙØ§Ø¯ Ø§Ù„Ù…ØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠ Ù…Ù† ÙØ±Ø¹ Ø¨Ù†ÙƒÙŠ"
import_yn_header = "Ù‡Ù„ Ù‚Ø§Ù… Ø§Ù„Ù…ØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠ Ø¨Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ø³ØªÙŠØ±Ø§Ø¯"
loan_date_header = "Derniere date effet credit"
import_date_header = "Derniere date import"

csv_df = pd.read_csv(csv_path, dtype=str).fillna("")
if "Registre commerce" not in csv_df.columns:
    print("X Registre commerce column missing in CSV.")
    return
csv_df.set_index("Registre commerce", inplace=True)
csv_data = csv_df.to_dict("index")

excel_files = collect_excel_files(excel_folder)

for file_path in excel_files:
    input_path = str(file_path)
    output_path = os.path.join(output_folder, os.path.relpath(input_path, excel_folder))
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    wb = load_workbook(input_path)
    updated = False

    for sheetname in wb.sheetnames:
        ws = wb[sheetname]
        merged_map = {}
        for merged in ws.merged_cells.ranges:
            for row in range(merged.min_row, merged.max_row + 1):
                for col in range(merged.min_col, merged.max_col + 1):
                    merged_map[(row, col)] = (merged.min_row, merged.min_col)

        header_row = None
        header_map = {}

        for i in range(1, 100):
            row_vals = [str(cell.value).strip() if cell.value else "" for cell in ws[i]]
            for idx, val in enumerate(row_vals):
                if normalize_arabic(val) in [normalize_arabic(h) for h in rc_headers]:
                    header_row = i
                    header_map["Registre commerce"] = idx + 1
                    break
            if header_row:
                break

        if not header_row:
            print(f"No RC column found in {input_path}/ Sheet {sheetname}")
            continue

        header_map["loan date"] = find_merged_column_by_text(ws, 10, loan_date_header)
        header_map["import date"] = find_merged_column_by_text(ws, 10, import_date_header)

        for row in range(1, 11):
            for merged in ws.merged_cells.ranges:
                cell = ws.cell(row=row, column=merged.min_col)
                if not cell.value:
                    continue
                val = str(cell.value).strip()
                if normalize_arabic(val) == normalize_arabic(loan_yn_header):
                    header_map["loan yn_yes"] = merged.min_col
                    header_map["loan yn_no"] = merged.min_col + 1
                elif normalize_arabic(val) == normalize_arabic(import_yn_header):
                    header_map["import yn_yes"] = merged.min_col
                    header_map["import yn_no"] = merged.min_col + 1

        row_num = header_row + 1
        max_row = ws.max_row

        while row_num <= max_row:
            rc = ws.cell(row=row_num, column=header_map["Registre commerce"]).value
            rc_key = str(rc).strip() if rc else None
            row_data = csv_data.get(rc_key) if rc_key else None

            for csv_col, (field_key, yn_val) in {
                "beneficiaire credit": ("loan yn", "Ù†Ø¹Ù…"),
                "Operation Import": ("import yn", "Ù†Ø¹Ù…"),
                "Derniere date effet credit": ("loan date", None),
                "Derniere date import": ("import date", None)
            }.items():
                if "yn" in field_key:
                    yes_col = header_map.get(f"{field_key}_yes")
                    no_col = header_map.get(f"{field_key}_no")
                    if yes_col and no_col:
                        yes_cell = get_writable_cell(ws, row_num, yes_col, merged_map)
                        no_cell = get_writable_cell(ws, row_num, no_col, merged_map)
                        if row_data:
                            val = row_data.get(csv_col)
                            if isinstance(val, str) and val.strip().upper() == "X":
                                if yn_val == "Ù†Ø¹Ù…":
                                    yes_cell.value = "/"
                                elif yn_val == "Y":
                                    no_cell.value = "/"
                                updated = True
                            elif not yes_cell.value and not no_cell.value:
                                no_cell.value = "/"
                                updated = True
                elif "date" in field_key:
                    date_col = header_map.get(field_key)
                    if row_data and date_col:
                        date_val = row_data.get(csv_col)
                        if isinstance(date_val, pd.Series):
                            date_val = date_val.iloc[0]
                        if date_val:
                            date_cell = get_writable_cell(ws, row_num, date_col, merged_map)
                            if not date_cell.value:
                                date_cell.value = date_val
                                updated = True

            row_num += 1

    wb.save(output_path)
    print(f"{'Saved' if updated else 'No updates made'}: {output_path}")







row_num = header_row + 1
max_row = ws.max_row
while row_num <= max_row:
    rc = ws.cell(row=row_num, column=header_map["Registre commerce"]).value
    rc_key = str(rc).strip() if rc else None
    row_data = csv_data.get(rc_key) if rc_key else None

    # loop over fields and use row_data.get(...)
    # use fast get_writable_cell with merged_map
    # write only if needed

    row_num += 1

row_num = header_row + 1
while row_num <= ws.max_row:
    rc = ws.cell(row=row_num, column=header_map["Registre commerce"]).value
    if rc is None or str(rc).strip() == "":
        row_num += 1
        continue  # Nothing here, skip

    rc_key = str(rc).strip()
    if rc_key not in csv_df.index:
        row_num += 1
        continue  # No matching record, skip

    # Process the row...

    row_num += 1  # Always move to the next row



if rc is None or str(rc).strip() == "":
    row_num += 1
    continue



# 1. RC column is detected from header row (usually row 1)
rc_col = header_map.get("Registre commerce")
if not rc_col:
    print(f"âŒ RC column not found in sheet {sheetname}")
    continue

# 2. Start scanning rows from row 2 to the last row
for row_num in range(2, ws.max_row + 1):
    rc_cell = ws.cell(row=row_num, column=rc_col)
    rc = rc_cell.value
    if rc is None or str(rc).strip() == "":
        continue  # Skip empty rows

    rc_key = str(rc).strip()
    matched = rc_key in csv_df.index

    # Now your logic for processing this row:
    for csv_col, (field_key, yn_val) in csv_column_map.items():
        if "yn" in field_key:
            yes_col = header_map.get(f"{field_key} yes")
            no_col = header_map.get(f"{field_key} no")
            if yes_col and no_col:
                yes_cell = get_writable_cell(ws, row_num, yes_col)
                no_cell = get_writable_cell(ws, row_num, no_col)
                if matched:
                    val = csv_df.at[rc_key, csv_col]
                    if isinstance(val, str) and val.strip().upper() == "X":
                        if yn_val == "Ù†Ø¹Ù…":
                            yes_cell.value = "/"
                        elif yn_val == "Ù„Ø§":
                            no_cell.value = "/"
                        updated = True
                else:
                    if not yes_cell.value and not no_cell.value:
                        no_cell.value = "/"
                        updated = True
        elif "date" in field_key:
            date_col = header_map.get(field_key)
            if matched and date_col:
                date_val = csv_df.at[rc_key, csv_col]
                if isinstance(date_val, pd.Series):
                    date_val = date_val.iloc[0]
                if date_val:
                    date_cell = get_writable_cell(ws, row_num, date_col)
                    if not date_cell.value:
                        date_cell.value = date_val
                        updated = True




# Step 1: Find first row with actual data under the header
data_start_row = None
for r in range(header_row + 1, ws.max_row + 1):
    val = ws.cell(row=r, column=header_map["Registre commerce"]).value
    if val and str(val).strip():
        data_start_row = r
        break

if not data_start_row:
    print(f"âš ï¸ No data found under Registre commerce in {input_path} / Sheet({sheet_name})")
    continue

# Step 2: Start from first data row
row_num = data_start_row
while True:
    rc_cell = ws.cell(row=row_num, column=header_map["Registre commerce"])
    rc = rc_cell.value
    if rc is None:
        break



Objet : Demande de gravure de fichiers zippÃ©s

Bonjour lâ€™Ã©quipe support,

Je vous prie de bien vouloir graver les deux fichiers zippÃ©s disponibles aux liens suivants :

[Lien fichier 1]

[Lien fichier 2]


N'hÃ©sitez pas Ã  me contacter si vous avez besoin dâ€™informations supplÃ©mentaires.

Merci dâ€™avance pour votre aide.

Cordialement,
[Votre nom]


import uuid
from pydub import AudioSegment

def convert_mp3_to_wav(mp3_path: str) -> str:
    try:
        # Set a temporary output file
        wav_path = f"/tmp/{uuid.uuid4()}.wav"

        # Load MP3 without needing ffmpeg
        audio = AudioSegment.from_file(mp3_path, format="mp3")  # uses av now
        audio.export(wav_path, format="wav")

        print("ğŸ§ Successfully converted MP3 to WAV.")
        return wav_path
    except Exception as e:
        print("âŒ Error converting MP3 to WAV:", e)
        return None


!apt-get update && apt-get install -y ffmpeg


import uuid
import ipywidgets as widgets
from IPython.display import display
import time

# Step 1: Create the upload widget
upload_widget = widgets.FileUpload(accept='.mp3', multiple=False)
display(upload_widget)

# Step 2: Function to wait and extract uploaded file
def get_uploaded_file():
    print("ğŸ“¥ Waiting for upload...")
    while not upload_widget.value:
        time.sleep(1)

    # Handle both dict-like or list-of-dict structures
    uploaded_items = upload_widget.value

    if isinstance(uploaded_items, dict):
        uploaded_info = next(iter(uploaded_items.values()))
    elif isinstance(uploaded_items, (list, tuple)):
        uploaded_info = uploaded_items[0]
    else:
        raise ValueError("Unexpected upload format")

    # Extract content and filename
    file_bytes = uploaded_info["content"]
    filename = uploaded_info.get("name", f"{uuid.uuid4()}.mp3")

    # Save to /tmp
    temp_mp3_path = f"/tmp/{uuid.uuid4()}.mp3"
    with open(temp_mp3_path, "wb") as f:
        f.write(file_bytes)

    print(f"âœ… Saved file '{filename}' to: {temp_mp3_path}")
    return temp_mp3_path



import uuid
import ipywidgets as widgets
from IPython.display import display

# Display upload widget
upload_widget = widgets.FileUpload(accept='.mp3', multiple=False)
display(upload_widget)

def get_uploaded_file():
    import time
    while not upload_widget.value:
        time.sleep(1)  # Wait for upload

    uploaded_data = list(upload_widget.value.values())[0]  # Get the first uploaded file's content
    file_content = uploaded_data["content"]
    filename = uploaded_data["metadata"]["name"]

    temp_mp3_path = f"/tmp/{uuid.uuid4()}.mp3"
    with open(temp_mp3_path, "wb") as f:
        f.write(file_content)

    print(f"âœ… File '{filename}' saved to: {temp_mp3_path}")
    return temp_mp3_path



pip install whisper pydub httpx openai TTS soundfile torchaudio ipywidgets
import os
import uuid
import whisper
from pydub import AudioSegment
import httpx
from TTS.api import TTS
from IPython.display import Audio, display
import ipywidgets as widgets
from IPython.display import display as ipydisplay

# --------- Step 1: Upload MP3 file ---------
upload_widget = widgets.FileUpload(accept='.mp3', multiple=False)
ipydisplay(upload_widget)
print("â¬†ï¸ Please upload your MP3 file")

def get_uploaded_file():
    while not upload_widget.value:
        pass  # Wait for upload
    uploaded_filename = list(upload_widget.value.keys())[0]
    content = upload_widget.value[uploaded_filename]['content']
    temp_mp3_path = f"/tmp/{uuid.uuid4()}.mp3"
    with open(temp_mp3_path, 'wb') as f:
        f.write(content)
    print(f"âœ… Uploaded: {uploaded_filename}")
    return temp_mp3_path

mp3_path = get_uploaded_file()

# --------- Step 2: Convert MP3 to WAV ---------
wav_path = f"/tmp/{uuid.uuid4()}.wav"
audio = AudioSegment.from_file(mp3_path, format="mp3")
audio.export(wav_path, format="wav")
print("ğŸ§ Converted MP3 to WAV")

# --------- Step 3: Transcribe audio with Whisper ---------
model = whisper.load_model("base")  # You can use "small", "medium", "large" for better accuracy
result = model.transcribe(wav_path)
transcribed_text = result["text"]
detected_language = result["language"]

print(f"ğŸ—£ Detected Language: {detected_language}")
print(f"ğŸ“ Transcription: {transcribed_text}")

# --------- Step 4: Define the generate_response function ---------
def generate_response(query: str, context: list) -> str:
    """
    Generate a structured, detailed response using Azure OpenAI chat completion.

    Args:
        query: User question string.
        context: List of dicts, each with 'page_content' and 'metadata' keys.

    Returns:
        AI assistant's response as string.
    """
    sources = list(set(chunk["metadata"].get("source", "unknown") for chunk in context))
    context_text = "\n\n".join(chunk["page_content"] for chunk in context)

    prompt = f"""
You are an AI assistant specialized in Retrieval-Augmented Generation (RAG) for banking system architecture.

[Context - Sources: {', '.join(sources)}]

{context_text}

[Query]

{query}

Please provide a **structured and detailed answer**. For example:
- If the context contains data architecture info and the query is about data location,
  include the **table name** (e.g., TDT31), the **data code** (e.g., A00090),
  and explain where it is located within the bank's data systems.
- Always provide the name and codification of the data within the tables.
- Provide definitions and explanations to help the user fully understand.
"""

    # Azure OpenAI API details - replace with your real info
    AZURE_API_VERSION = "2023-05-15"
    AZURE_ENDPOINT = "https://your-azure-openai-endpoint.openai.azure.com/"
    AZURE_API_KEY = "YOUR_AZURE_API_KEY"

    headers = {
        "api-key": AZURE_API_KEY,
        "Content-Type": "application/json"
    }

    json_payload = {
        "model": "gpt4o",
        "messages": [
            {"role": "system", "content": "You are assistant based on RAG (Retrieval-Augmented Generation) for banking system architecture."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.5,
        "max_tokens": 1000
    }

    with httpx.Client(verify=False) as client:
        response = client.post(
            url=f"{AZURE_ENDPOINT}openai/deployments/gpt4o/chat/completions?api-version={AZURE_API_VERSION}",
            headers=headers,
            json=json_payload
        )
        response.raise_for_status()
        completion = response.json()

    answer = completion["choices"][0]["message"]["content"]
    return answer

# --------- Step 5: Prepare example context ---------
# Replace this with your real retrieved docs from vector DB
context_example = [
    {
        "page_content": "Table TDT31 contains corporate account data. The data code A00090 represents client identification number in the Atlas system.",
        "metadata": {"source": "Bank Data Architecture Doc"}
    },
    {
        "page_content": "Corporate clients can open accounts by submitting documents X, Y, and Z as per regulation 2023.",
        "metadata": {"source": "Corporate Account Opening Procedure"}
    }
]

# --------- Step 6: Generate AI response ---------
ai_response = generate_response(transcribed_text, context_example)
print("ğŸ¤– AI Response:\n", ai_response)

# --------- Step 7: Convert AI response to speech ---------
tts = TTS(model_name="tts_models/multilingual/multi-dataset/your_tts", progress_bar=False)
audio_response_path = f"/tmp/{uuid.uuid4()}.wav"
tts.tts_to_file(text=ai_response, file_path=audio_response_path)
print("ğŸ”Š Audio response generated!")

# --------- Step 8: Play the audio response ---------
display(Audio(audio_response_path))
