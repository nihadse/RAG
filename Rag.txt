if rc is None or str(rc).strip() == "":
    row_num += 1
    continue



# 1. RC column is detected from header row (usually row 1)
rc_col = header_map.get("Registre commerce")
if not rc_col:
    print(f"âŒ RC column not found in sheet {sheetname}")
    continue

# 2. Start scanning rows from row 2 to the last row
for row_num in range(2, ws.max_row + 1):
    rc_cell = ws.cell(row=row_num, column=rc_col)
    rc = rc_cell.value
    if rc is None or str(rc).strip() == "":
        continue  # Skip empty rows

    rc_key = str(rc).strip()
    matched = rc_key in csv_df.index

    # Now your logic for processing this row:
    for csv_col, (field_key, yn_val) in csv_column_map.items():
        if "yn" in field_key:
            yes_col = header_map.get(f"{field_key} yes")
            no_col = header_map.get(f"{field_key} no")
            if yes_col and no_col:
                yes_cell = get_writable_cell(ws, row_num, yes_col)
                no_cell = get_writable_cell(ws, row_num, no_col)
                if matched:
                    val = csv_df.at[rc_key, csv_col]
                    if isinstance(val, str) and val.strip().upper() == "X":
                        if yn_val == "Ù†Ø¹Ù…":
                            yes_cell.value = "/"
                        elif yn_val == "Ù„Ø§":
                            no_cell.value = "/"
                        updated = True
                else:
                    if not yes_cell.value and not no_cell.value:
                        no_cell.value = "/"
                        updated = True
        elif "date" in field_key:
            date_col = header_map.get(field_key)
            if matched and date_col:
                date_val = csv_df.at[rc_key, csv_col]
                if isinstance(date_val, pd.Series):
                    date_val = date_val.iloc[0]
                if date_val:
                    date_cell = get_writable_cell(ws, row_num, date_col)
                    if not date_cell.value:
                        date_cell.value = date_val
                        updated = True




# Step 1: Find first row with actual data under the header
data_start_row = None
for r in range(header_row + 1, ws.max_row + 1):
    val = ws.cell(row=r, column=header_map["Registre commerce"]).value
    if val and str(val).strip():
        data_start_row = r
        break

if not data_start_row:
    print(f"âš ï¸ No data found under Registre commerce in {input_path} / Sheet({sheet_name})")
    continue

# Step 2: Start from first data row
row_num = data_start_row
while True:
    rc_cell = ws.cell(row=row_num, column=header_map["Registre commerce"])
    rc = rc_cell.value
    if rc is None:
        break



Objet : Demande de gravure de fichiers zippÃ©s

Bonjour lâ€™Ã©quipe support,

Je vous prie de bien vouloir graver les deux fichiers zippÃ©s disponibles aux liens suivants :

[Lien fichier 1]

[Lien fichier 2]


N'hÃ©sitez pas Ã  me contacter si vous avez besoin dâ€™informations supplÃ©mentaires.

Merci dâ€™avance pour votre aide.

Cordialement,
[Votre nom]


import uuid
from pydub import AudioSegment

def convert_mp3_to_wav(mp3_path: str) -> str:
    try:
        # Set a temporary output file
        wav_path = f"/tmp/{uuid.uuid4()}.wav"

        # Load MP3 without needing ffmpeg
        audio = AudioSegment.from_file(mp3_path, format="mp3")  # uses av now
        audio.export(wav_path, format="wav")

        print("ðŸŽ§ Successfully converted MP3 to WAV.")
        return wav_path
    except Exception as e:
        print("âŒ Error converting MP3 to WAV:", e)
        return None


!apt-get update && apt-get install -y ffmpeg


import uuid
import ipywidgets as widgets
from IPython.display import display
import time

# Step 1: Create the upload widget
upload_widget = widgets.FileUpload(accept='.mp3', multiple=False)
display(upload_widget)

# Step 2: Function to wait and extract uploaded file
def get_uploaded_file():
    print("ðŸ“¥ Waiting for upload...")
    while not upload_widget.value:
        time.sleep(1)

    # Handle both dict-like or list-of-dict structures
    uploaded_items = upload_widget.value

    if isinstance(uploaded_items, dict):
        uploaded_info = next(iter(uploaded_items.values()))
    elif isinstance(uploaded_items, (list, tuple)):
        uploaded_info = uploaded_items[0]
    else:
        raise ValueError("Unexpected upload format")

    # Extract content and filename
    file_bytes = uploaded_info["content"]
    filename = uploaded_info.get("name", f"{uuid.uuid4()}.mp3")

    # Save to /tmp
    temp_mp3_path = f"/tmp/{uuid.uuid4()}.mp3"
    with open(temp_mp3_path, "wb") as f:
        f.write(file_bytes)

    print(f"âœ… Saved file '{filename}' to: {temp_mp3_path}")
    return temp_mp3_path



import uuid
import ipywidgets as widgets
from IPython.display import display

# Display upload widget
upload_widget = widgets.FileUpload(accept='.mp3', multiple=False)
display(upload_widget)

def get_uploaded_file():
    import time
    while not upload_widget.value:
        time.sleep(1)  # Wait for upload

    uploaded_data = list(upload_widget.value.values())[0]  # Get the first uploaded file's content
    file_content = uploaded_data["content"]
    filename = uploaded_data["metadata"]["name"]

    temp_mp3_path = f"/tmp/{uuid.uuid4()}.mp3"
    with open(temp_mp3_path, "wb") as f:
        f.write(file_content)

    print(f"âœ… File '{filename}' saved to: {temp_mp3_path}")
    return temp_mp3_path



pip install whisper pydub httpx openai TTS soundfile torchaudio ipywidgets
import os
import uuid
import whisper
from pydub import AudioSegment
import httpx
from TTS.api import TTS
from IPython.display import Audio, display
import ipywidgets as widgets
from IPython.display import display as ipydisplay

# --------- Step 1: Upload MP3 file ---------
upload_widget = widgets.FileUpload(accept='.mp3', multiple=False)
ipydisplay(upload_widget)
print("â¬†ï¸ Please upload your MP3 file")

def get_uploaded_file():
    while not upload_widget.value:
        pass  # Wait for upload
    uploaded_filename = list(upload_widget.value.keys())[0]
    content = upload_widget.value[uploaded_filename]['content']
    temp_mp3_path = f"/tmp/{uuid.uuid4()}.mp3"
    with open(temp_mp3_path, 'wb') as f:
        f.write(content)
    print(f"âœ… Uploaded: {uploaded_filename}")
    return temp_mp3_path

mp3_path = get_uploaded_file()

# --------- Step 2: Convert MP3 to WAV ---------
wav_path = f"/tmp/{uuid.uuid4()}.wav"
audio = AudioSegment.from_file(mp3_path, format="mp3")
audio.export(wav_path, format="wav")
print("ðŸŽ§ Converted MP3 to WAV")

# --------- Step 3: Transcribe audio with Whisper ---------
model = whisper.load_model("base")  # You can use "small", "medium", "large" for better accuracy
result = model.transcribe(wav_path)
transcribed_text = result["text"]
detected_language = result["language"]

print(f"ðŸ—£ Detected Language: {detected_language}")
print(f"ðŸ“ Transcription: {transcribed_text}")

# --------- Step 4: Define the generate_response function ---------
def generate_response(query: str, context: list) -> str:
    """
    Generate a structured, detailed response using Azure OpenAI chat completion.

    Args:
        query: User question string.
        context: List of dicts, each with 'page_content' and 'metadata' keys.

    Returns:
        AI assistant's response as string.
    """
    sources = list(set(chunk["metadata"].get("source", "unknown") for chunk in context))
    context_text = "\n\n".join(chunk["page_content"] for chunk in context)

    prompt = f"""
You are an AI assistant specialized in Retrieval-Augmented Generation (RAG) for banking system architecture.

[Context - Sources: {', '.join(sources)}]

{context_text}

[Query]

{query}

Please provide a **structured and detailed answer**. For example:
- If the context contains data architecture info and the query is about data location,
  include the **table name** (e.g., TDT31), the **data code** (e.g., A00090),
  and explain where it is located within the bank's data systems.
- Always provide the name and codification of the data within the tables.
- Provide definitions and explanations to help the user fully understand.
"""

    # Azure OpenAI API details - replace with your real info
    AZURE_API_VERSION = "2023-05-15"
    AZURE_ENDPOINT = "https://your-azure-openai-endpoint.openai.azure.com/"
    AZURE_API_KEY = "YOUR_AZURE_API_KEY"

    headers = {
        "api-key": AZURE_API_KEY,
        "Content-Type": "application/json"
    }

    json_payload = {
        "model": "gpt4o",
        "messages": [
            {"role": "system", "content": "You are assistant based on RAG (Retrieval-Augmented Generation) for banking system architecture."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.5,
        "max_tokens": 1000
    }

    with httpx.Client(verify=False) as client:
        response = client.post(
            url=f"{AZURE_ENDPOINT}openai/deployments/gpt4o/chat/completions?api-version={AZURE_API_VERSION}",
            headers=headers,
            json=json_payload
        )
        response.raise_for_status()
        completion = response.json()

    answer = completion["choices"][0]["message"]["content"]
    return answer

# --------- Step 5: Prepare example context ---------
# Replace this with your real retrieved docs from vector DB
context_example = [
    {
        "page_content": "Table TDT31 contains corporate account data. The data code A00090 represents client identification number in the Atlas system.",
        "metadata": {"source": "Bank Data Architecture Doc"}
    },
    {
        "page_content": "Corporate clients can open accounts by submitting documents X, Y, and Z as per regulation 2023.",
        "metadata": {"source": "Corporate Account Opening Procedure"}
    }
]

# --------- Step 6: Generate AI response ---------
ai_response = generate_response(transcribed_text, context_example)
print("ðŸ¤– AI Response:\n", ai_response)

# --------- Step 7: Convert AI response to speech ---------
tts = TTS(model_name="tts_models/multilingual/multi-dataset/your_tts", progress_bar=False)
audio_response_path = f"/tmp/{uuid.uuid4()}.wav"
tts.tts_to_file(text=ai_response, file_path=audio_response_path)
print("ðŸ”Š Audio response generated!")

# --------- Step 8: Play the audio response ---------
display(Audio(audio_response_path))
