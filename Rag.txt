
def get_chat_history(max_turns=6):
    messages = st.session_state.chat_sessions[
        st.session_state.current_chat
    ]["messages"]

    history = []
    for msg in messages[-max_turns:]:
        role = msg["role"].capitalize()
        history.append(f"{role}: {msg['content']}")

    return history

from openai import AzureOpenAI
import httpx


def generate_response(query, docs):
    context = build_context(docs)
    chat_history = get_chat_history()

    history_text = "\n".join(chat_history)

    prompt = f"""
Vous êtes un assistant RAG spécialisé dans les systèmes bancaires,
l’architecture de données et les produits financiers.

[Historique]
{history_text}

[Contexte]
{context}

[Question]
{query}

INSTRUCTIONS:
- Utilisez exclusivement le contexte
- Citez les sources: [Source: fichier.pdf, Page X]
- Si l'information n'existe pas, dites-le clairement
"""

    client = AzureOpenAI(
        api_key=AZURE_OPENAI_API_KEY,
        api_version=AZURE_OPENAI_API_VERSION,
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        http_client=httpx.Client(verify=False)
    )

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "Assistant RAG bancaire"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3
    )

    return response.choices[0].message.content


def rag_chatbot(query):
    chat_history = get_chat_history()

    retriever = CustomRetrieverWithHistory(
        collection=collection,
        chat_history=chat_history,
        k=2500,
        rerank_k=50
    )

    docs = retriever.get_relevant_documents(query)
    answer = generate_response(query, docs)

    return {
        "answer": answer,
        "documents": docs
    }














def generate_response(query, context):
    # Collect sources
    sources = list(set([chunk["metadata"]["source"] for chunk in context]))

    # Build prompt
    context_text = build_context(context)
    prompt = f"""
Vous êtes un assistant RAG spécialisé dans les systèmes bancaires,
l'architecture de données, et les produits financiers. Votre rôle est de répondre et d'expliquer clairement les
informations issues des documents, en les rendant compréhensibles pour tout type d'utilisateur, du plus général au plus technique.

[Contexte]
{context_text}

[Requête]
{query}

[INSTRUCTIONS GENERALES]
1. Utilisez exclusivement les informations présentes dans le contexte ci-dessus.
2. Fournissez une réponse claire, structurée et détaillée, adaptée à la compréhension de tout utilisateur.
3. Citez immédiatement la source sous le format [Source: <nom du fichier.pdf, Page <numéro>].
4. Si l'information n'existe pas dans les documents, indiquez-le clairement.
5. Structure attendue:
   a. Réponse détaillée
   b. Synthèse (si utile)
   c. Sources
6. Objectif: rendre la réponse informative, explicative et vérifiable.
"""

    # Initialize Azure/OpenAI client
    # Make sure the API key, endpoint, and version are correct
    import httpx
    from openai import OpenAI  # or your wrapper

    client = OpenAI(
        api_key="YOUR_API_KEY",
        api_base="YOUR_AZURE_ENDPOINT",
        api_type="azure",
        api_version="2023-03-15-preview"  # replace with your version
    )

    # Call the model
    completion = client.chat.completions.create(
        model="gpt4o",
        messages=[
            {"role": "system", "content": "You are an assistant based on RAG for banking system architecture."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.5  # 0.5 is reasonable; 8.5 is invalid
    )

    # Handle return type (Azure wrapper may return string or dict)
    if isinstance(completion, str):
        response = completion
    elif hasattr(completion, "choices"):
        response = completion.choices[0].message["content"]
    else:
        raise ValueError(f"Unexpected completion type: {type(completion)}")

    # Clean up whitespace for readability
    lines = [line.strip() for line in response.split("\n") if line.strip()]
    readable_response = "\n".join(lines)

    return readable_response










Here’s a polished French version of your email:


---

Objet : Mise à jour de la documentation « Usine de Fiable »

Bonjour [Nom du collègue],

Veuillez trouver ci-joint la version mise à jour de la documentation « Usine de Fiable », à laquelle j’ai ajouté le point [ou “cette section” selon ce que vous avez ajouté].

Merci de bien vouloir confirmer la réception et n’hésitez pas à m’ajouter tout élément que vous souhaitez discuter ou revoir.

Cordialement,
[Votre nom]


---

If you want, I can also make a slightly more friendly and collaborative version that encourages discussion without sounding too formal. Do you want me to do that?






appliquées après la production de ce mois.


Ajout d'une nouvelle donnée / nouveau contrôle « DCC » :
Ainsi, si vous avez reçu un nouveau contrôle de donnée, effectué une modification sur un contrôle existant, ou réactivé un ancien contrôle de donnée dans le périmètre, ou effectué toute autre forme de modification, merci de ne pas appliquer ces modifications après le 20 de chaque mois.




# Initialize retriever
retriever = CustomRetrieverWithHistory(
    collection=collection,      # your Chroma collection
    query_model=query_model,    # BGE-M3 with instruction
    doc_model=doc_model,        # BGE-M3 without instruction
    chat_history=[],            # optional
    k=100,                      # fetch top 100 dense docs
    rerank_k=50                 # rerank top 50 with BM25
)

query = "Which table stores AML risk classification for retail clients?"

docs = retriever.get_relevant_documents(query)

# Output results
for d in docs:
    print(d["metadata"], "\n", d["text"])




import string
import re
import numpy as np
from rank_bm25 import BM25Okapi

# ---------- BM25 Reranker ----------
class BM25Reranker:
    def __init__(self, k=50):
        self.k = k
        self.bm25 = None
        self.document_map = {}
        self.tokenizer = str.maketrans("", "", string.punctuation)

    def _tokenize(self, text):
        if not isinstance(text, str):
            return []
        cleaned = text.translate(self.tokenizer).lower()
        return [t for t in cleaned.split() if t.strip()]

    def fit(self, documents):
        if not documents:
            raise ValueError("Empty document list given to BM25Reranker")
        tokenized_corpus = []
        valid_docs = []
        for i, doc in enumerate(documents):
            text = doc.get("text") or doc.get("page content") or doc.get("content")
            tokens = self._tokenize(text)
            if tokens:
                tokenized_corpus.append(tokens)
                valid_docs.append((i, doc))
        if not tokenized_corpus:
            raise ValueError("No valid documents to fit BM25 model")
        self.document_map = {i: doc for i, doc in valid_docs}
        self.bm25 = BM25Okapi(tokenized_corpus)

    def rerank(self, query, documents):
        if not documents:
            return []
        self.fit(documents)
        tokens = self._tokenize(query)
        if not tokens:
            return documents[:self.k]
        scores = self.bm25.get_scores(tokens)
        ranked = sorted(
            zip(scores, self.document_map.values()),
            key=lambda x: x[0],
            reverse=True
        )
        return [doc for _, doc in ranked[:self.k]]



# ---------- Custom Retriever with Query & Document Embeddings ----------
class CustomRetrieverWithHistory:
    def __init__(self, collection, query_model, doc_model, chat_history=None, k=50, rerank_k=50):
        """
        collection: Chroma collection
        query_model: BGE-M3 model with query_instruction
        doc_model: BGE-M3 model without instruction
        chat_history: optional chat context
        k: number of dense results to fetch
        rerank_k: number of results for BM25 reranking
        """
        self.collection = collection
        self.chat_history = chat_history or []
        self.k = k
        self.rerank_k = rerank_k
        self.query_model = query_model
        self.doc_model = doc_model
        self.reranker = BM25Reranker(k=rerank_k)

    def get_relevant_documents(self, query):
        # --- combine chat history if exists ---
        if self.chat_history:
            full_query = " ".join(self.chat_history) + " " + query
        else:
            full_query = query

        # --- encode the query using the correct query embedding model ---
        query_embedding = self.query_model.encode([full_query])[0].tolist()

        # --- dense retrieval using query embedding ---
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=self.k,
            include=["documents", "metadatas"]
        )

        # --- collect dense docs ---
        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(results["documents"][0], results["metadatas"][0])
        ]

        # --- BM25 rerank over dense docs as hybrid step ---
        reranked_docs = self.reranker.rerank(full_query, dense_docs)

        return reranked_docs


query = "Which table stores AML risk classification for retail clients?"

query_embedding = query_model.encode([query])[0].tolist()

results = collection.query(
    query_embeddings=[query_embedding],
    n_results=10,
    include=["documents", "metadatas"]
)




MODEL_PATH = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"

# ---- Document embedding model (NO instruction) ----
doc_model = FlagModel(
    MODEL_PATH,
    use_fp16=True
)

# ---- Query embedding model (WITH instruction) ----
query_model = FlagModel(
    MODEL_PATH,
    query_instruction_for_retrieval=(
        "Generate representation for this sentence to retrieve relevant articles:"
    ),
    use_fp16=True
)












class BGE_M3_EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        self.dimension = 1024  # BGE-M3 dimension

    def __call__(self, inputs: List[str]) -> List[List[float]]:
        if isinstance(inputs, str):
            inputs = [inputs]

        embeddings = self.model.encode(inputs)

        if len(embeddings[0]) != self.dimension:
            raise ValueError(
                f"Embedding dimension mismatch: expected {self.dimension}, "
                f"got {len(embeddings[0])}"
            )

        return embeddings.tolist()collection = chroma_client.get_or_create_collection(
    name="my_documents",
    embedding_function=BGE_M3_EmbeddingFunction(doc_model),
    metadata={
        "hnsw:space": "cosine",
        "hnsw:construction_ef": 200,
        "hnsw:M": 16
    }
)




query = "CLIENT_ACCOUNT risk_level AML"

dense = chroma_collection.query(
    query_texts=[query],
    n_results=1000,   # very large on purpose
    include=["documents"]
)

dense_docs = dense["documents"][0]

found = any(
    any(chunk_id in d for chunk_id in ["CLIENT_ACCOUNT", "risk_level"])
    for d in dense_docs
)

print("Found in dense@1000:", found)




class BM25Reranker:
    def __init__(self, documents, k=50):
        """
        documents: list of {"text": ..., "metadata": ...}
        """
        self.k = k
        self.documents = documents
        self.tokenized_corpus = [self._tokenize(d["text"]) for d in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)

    def _tokenize(self, text: str):
        # preserves table names, columns, acronyms
        return re.findall(r"[A-Za-z0-9_\.]+", text.lower())

    def retrieve(self, query: str):
        tokens = self._tokenize(query)
        scores = self.bm25.get_scores(tokens)

        ranked = sorted(
            zip(scores, self.documents),
            key=lambda x: x[0],
            reverse=True
        )

        return [doc for _, doc in ranked[:self.k]]


class HybridRetriever:
    def __init__(
        self,
        chroma_collection,
        bm25_reranker,
        k_dense=200,
        k_final=20
    ):
        self.collection = chroma_collection
        self.bm25 = bm25_reranker
        self.k_dense = k_dense
        self.k_final = k_final

    def retrieve(self, query: str):
        # ---- 1. Query expansion (light, safe) ----
        expanded_query = (
            query
            + " table database schema column risk AML architecture"
        )

        # ---- 2. Dense retrieval (high recall) ----
        dense = self.collection.query(
            query_texts=[expanded_query],
            n_results=self.k_dense,
            include=["documents", "metadatas"]
        )

        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(dense["documents"][0], dense["metadatas"][0])
        ]

        # ---- 3. Sparse retrieval (BM25 rescue) ----
        sparse_docs = self.bm25.retrieve(query)

        # ---- 4. Union + dedup ----
        merged = {}
        for d in dense_docs + sparse_docs:
            merged[hash(d["text"])] = d

        merged_docs = list(merged.values())

        return merged_docs[:self.k_final]

def output_matching_chunks(collection, keyword: str):
    data = collection.get(include=["documents", "metadatas"])

    documents = data["documents"]
    metadatas = data["metadatas"]

    # handle list-of-lists
    if documents and isinstance(documents[0], list):
        documents = documents[0]
        metadatas = metadatas[0]

    hits = 0
    for i, (doc, meta) in enumerate(zip(documents, metadatas)):
        if keyword.lower() in doc.lower():
            print(f"\n--- MATCHING CHUNK {i} ---")
            print("Metadata:", meta)
            print(doc)
            hits += 1

    print("\nTotal matching chunks:", hits)

def build_context(docs):
    blocks = []
    for i, d in enumerate(docs):
        meta = d.get("metadata", {})
        header = f"""
[Chunk {i+1}]
Database: {meta.get('db', 'N/A')}
Schema: {meta.get('schema', 'N/A')}
Table: {meta.get('table', 'N/A')}
"""
        blocks.append(header + d["text"])
    return "\n\n".join(blocks)

# documents = same docs used for Chroma ingestion
bm25 = BM25Reranker(documents, k=100)

retriever = HybridRetriever(
    chroma_collection=chroma_collection,
    bm25_reranker=bm25,
    k_dense=250,
    k_final=20
)

query = "Which table stores AML risk classification for retail clients?"

docs = retriever.retrieve(query)

for d in docs:
    print(d["metadata"])query = "Which table stores AML risk classification for retail clients?"

docs = retriever.retrieve(query)

for d in docs:
    print(d["metadata"])

context = build_context(docs)

prompt = f"""
You are a banking information system architect.

Answer ONLY using the context.
If not found, say "Information not found".

Context:
{context}

Question:
{query}
"""








keyword = "CLIENT_ACCOUNT"

data = chroma_collection.get(
    include=["documents", "metadatas"]
)

hits = 0

documents = data["documents"]
metadatas = data["metadatas"]

# Handle both flat list and list-of-lists
if documents and isinstance(documents[0], list):
    documents = documents[0]
    metadatas = metadatas[0]

for i, (doc, meta) in enumerate(zip(documents, metadatas)):
    if keyword.lower() in doc.lower():
        print(f"\n--- MATCHING CHUNK {i} ---")
        print("Metadata:", meta)
        print(doc)
        hits += 1

print("\nTotal matching chunks:", hits)











data = chroma_collection.get(
    where={"source": "core_banking_architecture.pdf"},
    include=["documents", "metadatas"]
)

for i, doc in enumerate(data["documents"]):
    print(f"\n--- CHUNK {i} ---\n")
    print(doc)





bm25_reranker = BM25Reranker(
    documents=documents,
    k=50   # how many BM25 docs you want
)


hybrid_retriever = HybridRetriever(
    collection=chroma_collection,
    bm25_reranker=bm25_reranker,
    k_dense=120,   # large recall
    k_final=20     # final docs sent to LLM
)

query = "Which database stores AML risk classification for retail clients?"

retrieved_docs = hybrid_retriever.retrieve(query)



from rank_bm25 import BM25Okapi
import re

class BM25Reranker:
    def __init__(self, documents, k=20):
        self.k = k
        self.documents = documents
        self.tokenized_corpus = [self._tokenize(d["text"]) for d in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)

    def _tokenize(self, text):
        return re.findall(r"[A-Za-z0-9_\.]+", text.lower())

    def rerank(self, query):
        query_tokens = self._tokenize(query)
        scores = self.bm25.get_scores(query_tokens)

        ranked = sorted(
            zip(scores, self.documents),
            key=lambda x: x[0],
            reverse=True
        )
        return [doc for _, doc in ranked[:self.k]]class HybridRetriever:
    def __init__(self, collection, bm25_reranker, k_dense=100, k_final=20):
        self.collection = collection
        self.bm25 = bm25_reranker
        self.k_dense = k_dense
        self.k_final = k_final

    def retrieve(self, query):
        dense = self.collection.query(
            query_texts=[query],
            n_results=self.k_dense,
            include=["documents", "metadatas"]
        )

        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(dense["documents"][0], dense["metadatas"][0])
        ]

        # BM25 rescue
        sparse_docs = self.bm25.rerank(query)

        # Union + dedup
        seen = set()
        merged = []
        for d in dense_docs + sparse_docs:
            h = hash(d["text"])
            if h not in seen:
                seen.add(h)
                merged.append(d)

        return merged[:self.k_final]from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_community.document_compressors.rankllm_rerank import RankLLMRerank
from typing import List

def hybrid_retrieve_langchain(
    collection,           # Your vector DB collection (Chroma/FAISS)
    llm_client,           # Azure/OpenAI client (LLM)
    query: str,
    k_dense: int = 50,    # Number of top docs from embeddings
    k_final: int = 20     # Number of final docs after reranking
) -> List[dict]:
    """
    Hybrid Retrieval using:
    Step 1: Dense retrieval from vector DB
    Step 2: RankLLM reranking (via LangChain)
    Returns top documents (text + metadata)
    """

    # --------------------------
    # Step 1: Dense retrieval
    # --------------------------
    dense_results = collection.query(
        query_texts=[query],
        n_results=k_dense,
        include=["documents", "metadatas"]
    )

    candidates = []
    for text, meta in zip(dense_results["documents"][0], dense_results["metadatas"][0]):
        candidates.append({"text": text, "metadata": meta})

    if not candidates:
        return []

    # --------------------------
    # Step 2: RankLLM reranker
    # --------------------------
    reranker = RankLLMRerank.from_llm(
        llm=llm_client,  # your Azure/OpenAI client
        top_n=k_final,
        verbose=True
    )

    # Optional: wrap with ContextualCompressionRetriever if you want
    # to compress context before passing to reranker (useful for long chunks)
    retriever = ContextualCompressionRetriever(
        base_retriever=candidates,  # here just the list of candidates
        compressor=reranker
    )

    # Run reranking
    reranked_docs = reranker.rerank(query=query, documents=candidates)

    return reranked_docs[:k_final]






query = "How to configure user permissions in the system"

top_docs = hybrid_retrieve_langchain(
    collection=chroma_collection,
    llm_client=azure_openai_client,
    query=query,
    k_dense=80,
    k_final=20
)

for doc in top_docs:
    print(doc["text"][:300], "...\n")







import streamlit as st
import chromadb
from chromadb.utils import embedding_functions
import fitz  # PyMuPDF
from docx import Document
import pandas as pd
from datetime import datetime
import json
import os
from typing import List, Dict, Tuple, Optional
import re
from collections import defaultdict
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
import hashlib

# Azure OpenAI imports
from openai import AzureOpenAI
import httpx

# ==============================================================================
# CONFIGURATION
# ==============================================================================

class Config:
    """Centralized configuration management"""
    
    # Azure OpenAI Configuration
    OIDC_CLIENT_ID = os.getenv("OIDC_CLIENT_ID", "your_client_id")
    OIDC_CLIENT_SECRET = os.getenv("OIDC_CLIENT_SECRET", "your_client_secret")
    OIDC_ENDPOINT = os.getenv("OIDC_ENDPOINT", "https://alfactory.api.staging.schonet/auth/oauth2/v2/token")
    OIDC_SCOPE = os.getenv("OIDC_SCOPE", "genai-model")
    
    AZURE_ENDPOINT = os.getenv("AZURE_ENDPOINT", "https://alfactory.api.staging.schonet/genai-model/v1")
    AZURE_API_VERSION = os.getenv("AZURE_API_VERSION", "2024-05-01-preview")
    AZURE_API_KEY = os.getenv("AZURE_API_KEY", "FAKE_KEY")
    AZURE_MODEL_DEPLOYMENT = os.getenv("AZURE_MODEL_DEPLOYMENT", "gpt-4o")
    
    # Embedding Configuration
    EMBEDDING_MODEL_PATH = "/domino/datasets/local/test-prd-base"
    EMBEDDING_DIMENSION = 1024
    
    # ChromaDB Configuration
    CHROMA_PERSIST_PATH = "/domino/datasets/local/chroma_persistent_db"
    COLLECTION_NAME = "banking_documents"
    
    # Chunking Configuration for Banking Documents
    CHUNK_SIZES = {
        'header': 300,      # For sections with headers
        'table': 500,       # For tables and structured data
        'paragraph': 400,   # For regular paragraphs
        'list': 350,        # For lists and enumerations
    }
    CHUNK_OVERLAP = 100
    
    # BM25 Configuration
    BM25_INDEX_PATH = "/domino/datasets/local/bm25_index.pkl"
    BM25_K1 = 1.5
    BM25_B = 0.75
    
    # Retrieval Configuration
    INITIAL_RETRIEVAL_K = 20  # Retrieve more for reranking
    FINAL_RESULTS_K = 5       # Final results after reranking
    HYBRID_ALPHA = 0.5        # Balance between semantic (0) and keyword (1)
    
    # Batch Processing Configuration
    BATCH_SIZE = 100
    MAX_WORKERS = 4

# ==============================================================================
# DOCUMENT PROCESSING & CHUNKING
# ==============================================================================

class BankingDocumentProcessor:
    """Advanced document processor optimized for banking/technical documents"""
    
    def __init__(self):
        self.chunk_patterns = {
            'section_header': re.compile(r'^(Sect\.|Section|Chapitre|Article|§)\s*[\dIVX]+[\.\s]', re.IGNORECASE),
            'subsection': re.compile(r'^[A-Z]{1,3}\.\d+(\.\d+)*\s+', re.MULTILINE),
            'table_marker': re.compile(r'(Description Rubrique|Num\s+donn\.|Val\s+init)', re.IGNORECASE),
            'code_block': re.compile(r'(Code|Référence|Type de contrat):\s*\d+', re.IGNORECASE),
            'list_item': re.compile(r'^\s*[-•*]\s+|\d+\.\s+', re.MULTILINE),
        }
    
    def extract_text_from_file(self, uploaded_file) -> Optional[Dict]:
        """Extract text with metadata from uploaded files"""
        filename = uploaded_file.name.lower()
        
        try:
            if filename.endswith('.pdf'):
                return self._extract_from_pdf(uploaded_file)
            elif filename.endswith('.txt'):
                text = uploaded_file.read().decode('utf-8')
                return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
            elif filename.endswith('.docx'):
                return self._extract_from_docx(uploaded_file)
            elif filename.endswith(('.csv', '.xlsx')):
                return self._extract_from_spreadsheet(uploaded_file, filename)
            else:
                return None
        except Exception as e:
            st.error(f"Error extracting from {uploaded_file.name}: {str(e)}")
            return None
    
    def _extract_from_pdf(self, uploaded_file) -> Dict:
        """Extract text from PDF with page-level granularity"""
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        pages = []
        
        for page_num, page in enumerate(doc, start=1):
            text = page.get_text()
            pages.append({
                'page_num': page_num,
                'text': text
            })
        
        full_text = "\n".join([p['text'] for p in pages])
        return {'text': full_text, 'pages': pages}
    
    def _extract_from_docx(self, uploaded_file) -> Dict:
        """Extract text from DOCX"""
        doc = Document(uploaded_file)
        text = "\n".join([p.text for p in doc.paragraphs])
        return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
    
    def _extract_from_spreadsheet(self, uploaded_file, filename: str) -> Dict:
        """Extract text from CSV/Excel"""
        if filename.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)
        
        text = df.to_string(index=False)
        return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
    
    def clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove excessive whitespace
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        
        # Normalize line breaks
        text = text.replace('\r\n', '\n')
        
        # Remove page headers/footers (common patterns)
        text = re.sub(r'Page \d+( of \d+)?', '', text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def smart_chunk_document(self, doc_data: Dict, source_file: str) -> List[Dict]:
        """
        Intelligent chunking that preserves document structure
        Optimized for banking/technical documents
        """
        chunks = []
        pages = doc_data.get('pages', [])
        
        for page_info in pages:
            page_num = page_info['page_num']
            text = self.clean_text(page_info['text'])
            
            # Detect document structure
            if self._is_table_content(text):
                page_chunks = self._chunk_table_content(text, source_file, page_num)
            elif self._has_clear_sections(text):
                page_chunks = self._chunk_by_sections(text, source_file, page_num)
            else:
                page_chunks = self._chunk_semantic(text, source_file, page_num)
            
            chunks.extend(page_chunks)
        
        return chunks
    
    def _is_table_content(self, text: str) -> bool:
        """Detect if content is primarily tabular"""
        return bool(self.chunk_patterns['table_marker'].search(text))
    
    def _has_clear_sections(self, text: str) -> bool:
        """Detect if content has clear section structure"""
        headers = self.chunk_patterns['section_header'].findall(text)
        return len(headers) >= 2
    
    def _chunk_table_content(self, text: str, source: str, page: int) -> List[Dict]:
        """Chunk table content preserving structure"""
        chunks = []
        lines = text.split('\n')
        current_chunk = []
        current_size = 0
        
        for line in lines:
            line_size = len(line)
            
            if current_size + line_size > Config.CHUNK_SIZES['table']:
                if current_chunk:
                    chunks.append(self._create_chunk(
                        '\n'.join(current_chunk), source, page, 'table'
                    ))
                    # Overlap: keep last few lines
                    overlap_lines = current_chunk[-3:] if len(current_chunk) > 3 else current_chunk
                    current_chunk = overlap_lines
                    current_size = sum(len(l) for l in current_chunk)
            
            current_chunk.append(line)
            current_size += line_size
        
        if current_chunk:
            chunks.append(self._create_chunk(
                '\n'.join(current_chunk), source, page, 'table'
            ))
        
        return chunks
    
    def _chunk_by_sections(self, text: str, source: str, page: int) -> List[Dict]:
        """Chunk by document sections"""
        chunks = []
        sections = re.split(r'(\n(?:Sect\.|Section|Chapitre|Article)\s+[\dIVX]+)', text, flags=re.IGNORECASE)
        
        current_section = ""
        
        for i, section in enumerate(sections):
            if i % 2 == 0:  # Content
                current_section += section
            else:  # Header
                if current_section.strip():
                    chunks.append(self._create_chunk(
                        current_section.strip(), source, page, 'section'
                    ))
                current_section = section
        
        if current_section.strip():
            chunks.append(self._create_chunk(
                current_section.strip(), source, page, 'section'
            ))
        
        return chunks
    
    def _chunk_semantic(self, text: str, source: str, page: int) -> List[Dict]:
        """Semantic chunking with overlap for regular content"""
        chunks = []
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            if current_size + sentence_size > Config.CHUNK_SIZES['paragraph']:
                if current_chunk:
                    chunks.append(self._create_chunk(
                        ' '.join(current_chunk), source, page, 'paragraph'
                    ))
                    # Overlap: keep last 2 sentences
                    overlap = current_chunk[-2:] if len(current_chunk) > 2 else current_chunk
                    current_chunk = overlap
                    current_size = sum(len(s) for s in current_chunk)
            
            current_chunk.append(sentence)
            current_size += sentence_size
        
        if current_chunk:
            chunks.append(self._create_chunk(
                ' '.join(current_chunk), source, page, 'paragraph'
            ))
        
        return chunks
    
    def _create_chunk(self, text: str, source: str, page: int, chunk_type: str) -> Dict:
        """Create a standardized chunk object"""
        chunk_id = hashlib.md5(f"{source}_{page}_{text[:50]}".encode()).hexdigest()
        
        return {
            'id': chunk_id,
            'text': text,
            'metadata': {
                'source_file': source,
                'page': page,
                'chunk_type': chunk_type,
                'char_count': len(text),
                'timestamp': str(datetime.now())
            }
        }

# ==============================================================================
# BM25 IMPLEMENTATION WITH PERSISTENT INDEX
# ==============================================================================

class PersistentBM25:
    """BM25 implementation with disk persistence for large-scale retrieval"""
    
    def __init__(self, k1: float = Config.BM25_K1, b: float = Config.BM25_B):
        self.k1 = k1
        self.b = b
        self.corpus_size = 0
        self.avgdl = 0
        self.doc_freqs = defaultdict(int)
        self.idf = {}
        self.doc_len = []
        self.doc_ids = []
        self.tokenized_corpus = []
        
    def tokenize(self, text: str) -> List[str]:
        """Tokenize with banking-specific preservation"""
        # Preserve codes and references
        text = re.sub(r'([A-Z]{2,}\d+)', r' \1 ', text)
        
        # Remove punctuation but keep hyphens in codes
        text = re.sub(r'[^\w\s-]', ' ', text.lower())
        
        # Tokenize
        tokens = text.split()
        
        # Filter stopwords (French banking context)
        stopwords = {'le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'ou', 'dans', 'pour', 'sur', 'avec'}
        tokens = [t for t in tokens if t and t not in stopwords]
        
        return tokens
    
    def fit(self, documents: List[Dict]):
        """Build BM25 index from documents"""
        self.corpus_size = len(documents)
        self.doc_ids = [doc['id'] for doc in documents]
        self.tokenized_corpus = []
        
        # Tokenize all documents
        for doc in documents:
            tokens = self.tokenize(doc['text'])
            self.tokenized_corpus.append(tokens)
            self.doc_len.append(len(tokens))
            
            # Count document frequencies
            unique_tokens = set(tokens)
            for token in unique_tokens:
                self.doc_freqs[token] += 1
        
        # Calculate average document length
        self.avgdl = sum(self.doc_len) / self.corpus_size if self.corpus_size > 0 else 0
        
        # Calculate IDF values
        for token, freq in self.doc_freqs.items():
            self.idf[token] = np.log((self.corpus_size - freq + 0.5) / (freq + 0.5) + 1)
    
    def get_scores(self, query: str) -> Dict[str, float]:
        """Calculate BM25 scores for a query"""
        query_tokens = self.tokenize(query)
        scores = {}
        
        for idx, (doc_id, doc_tokens, doc_length) in enumerate(
            zip(self.doc_ids, self.tokenized_corpus, self.doc_len)
        ):
            score = 0
            for token in query_tokens:
                if token not in self.idf:
                    continue
                
                # Calculate term frequency in document
                tf = doc_tokens.count(token)
                
                # BM25 formula
                numerator = tf * (self.k1 + 1)
                denominator = tf + self.k1 * (1 - self.b + self.b * (doc_length / self.avgdl))
                score += self.idf[token] * (numerator / denominator)
            
            scores[doc_id] = score
        
        return scores
    
    def save(self, filepath: str):
        """Save BM25 index to disk"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'k1': self.k1,
                'b': self.b,
                'corpus_size': self.corpus_size,
                'avgdl': self.avgdl,
                'doc_freqs': dict(self.doc_freqs),
                'idf': self.idf,
                'doc_len': self.doc_len,
                'doc_ids': self.doc_ids,
                'tokenized_corpus': self.tokenized_corpus
            }, f)
    
    @classmethod
    def load(cls, filepath: str) -> 'PersistentBM25':
        """Load BM25 index from disk"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        
        bm25 = cls(k1=data['k1'], b=data['b'])
        bm25.corpus_size = data['corpus_size']
        bm25.avgdl = data['avgdl']
        bm25.doc_freqs = defaultdict(int, data['doc_freqs'])
        bm25.idf = data['idf']
        bm25.doc_len = data['doc_len']
        bm25.doc_ids = data['doc_ids']
        bm25.tokenized_corpus = data['tokenized_corpus']
        
        return bm25

# ==============================================================================
# QUERY EXPANSION FOR BANKING TERMINOLOGY
# ==============================================================================

class BankingQueryExpander:
    """Query expansion specialized for banking/financial terminology"""
    
    def __init__(self):
        # Banking-specific synonym mappings
        self.synonyms = {
            'compte': ['account', 'compte support', 'compte client'],
            'contrat': ['contract', 'agreement', 'convention'],
            'dépôt': ['deposit', 'versement', 'apport'],
            'garantie': ['guarantee', 'caution', 'warranty'],
            'échéance': ['maturity', 'deadline', 'due date', 'expiration'],
            'nantissement': ['pledge', 'collateral', 'guarantee'],
            'saisie': ['entry', 'input', 'capture'],
            'grille': ['grid', 'screen', 'form'],
            'transaction': ['operation', 'mouvement', 'transfer'],
            'client': ['customer', 'account holder', 'beneficiary'],
            'bénéficiaire': ['beneficiary', 'recipient', 'payee'],
            'taux': ['rate', 'interest rate', 'percentage'],
            'plafond': ['ceiling', 'limit', 'cap', 'maximum'],
            'retrait': ['withdrawal', 'extraction'],
            'virement': ['transfer', 'wire', 'payment'],
        }
        
        # Common banking abbreviations
        self.abbreviations = {
            'DAT': 'Dépôt à Terme',
            'CNT': 'Contrat',
            'GDI': 'Gestion De Interface',
            'BDC': 'Bon De Caisse',
            'SGE': 'Code siège',
            'DEV': 'Code devise',
        }
    
    def expand_query(self, query: str) -> List[str]:
        """Expand query with synonyms and banking terms"""
        expanded_queries = [query]
        
        query_lower = query.lower()
        
        # Add synonym expansions
        for term, synonyms in self.synonyms.items():
            if term in query_lower:
                for synonym in synonyms:
                    expanded = query_lower.replace(term, synonym)
                    if expanded != query_lower:
                        expanded_queries.append(expanded)
        
        # Expand abbreviations
        for abbr, full_form in self.abbreviations.items():
            if abbr in query:
                expanded = query.replace(abbr, full_form)
                expanded_queries.append(expanded)
            elif full_form.lower() in query_lower:
                expanded = query_lower.replace(full_form.lower(), abbr)
                expanded_queries.append(expanded)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_queries = []
        for q in expanded_queries:
            if q not in seen:
                seen.add(q)
                unique_queries.append(q)
        
        return unique_queries[:5]  # Limit to top 5 expansions

# ==============================================================================
# HYBRID RETRIEVAL SYSTEM
# ==============================================================================

class HybridRetriever:
    """Hybrid retrieval combining semantic search (ChromaDB) and keyword search (BM25)"""
    
    def __init__(self, chroma_collection, bm25_index: PersistentBM25, alpha: float = Config.HYBRID_ALPHA):
        self.chroma_collection = chroma_collection
        self.bm25_index = bm25_index
        self.alpha = alpha  # Weight between semantic (0) and keyword (1)
        self.query_expander = BankingQueryExpander()
    
    def retrieve(self, query: str, k: int = Config.INITIAL_RETRIEVAL_K) -> List[Dict]:
        """
        Hybrid retrieval with query expansion
        Returns top k documents based on weighted combination of semantic + keyword scores
        """
        # Expand query for better recall
        expanded_queries = self.query_expander.expand_query(query)
        
        all_results = {}
        
        for exp_query in expanded_queries:
            # 1. Semantic search via ChromaDB
            semantic_results = self.chroma_collection.query(
                query_texts=[exp_query],
                n_results=k,
                include=["documents", "metadatas", "distances"]
            )
            
            # Normalize semantic scores (distances -> similarities)
            semantic_scores = {}
            if semantic_results['ids'] and semantic_results['ids'][0]:
                max_distance = max(semantic_results['distances'][0]) if semantic_results['distances'][0] else 1
                for idx, doc_id in enumerate(semantic_results['ids'][0]):
                    distance = semantic_results['distances'][0][idx]
                    similarity = 1 - (distance / max_distance) if max_distance > 0 else 1
                    semantic_scores[doc_id] = similarity
            
            # 2. Keyword search via BM25
            bm25_scores = self.bm25_index.get_scores(exp_query)
            
            # Normalize BM25 scores
            max_bm25 = max(bm25_scores.values()) if bm25_scores else 1
            normalized_bm25 = {k: v / max_bm25 for k, v in bm25_scores.items()} if max_bm25 > 0 else bm25_scores
            
            # 3. Combine scores with weighted sum
            all_doc_ids = set(semantic_scores.keys()) | set(normalized_bm25.keys())
            
            for doc_id in all_doc_ids:
                sem_score = semantic_scores.get(doc_id, 0)
                bm25_score = normalized_bm25.get(doc_id, 0)
                
                # Hybrid score
                hybrid_score = (1 - self.alpha) * sem_score + self.alpha * bm25_score
                
                # Accumulate scores across expanded queries
                if doc_id in all_results:
                    all_results[doc_id]['score'] = max(all_results[doc_id]['score'], hybrid_score)
                else:
                    # Get document metadata
                    try:
                        doc_data = self.chroma_collection.get(
                            ids=[doc_id],
                            include=["documents", "metadatas"]
                        )
                        if doc_data['ids']:
                            all_results[doc_id] = {
                                'id': doc_id,
                                'text': doc_data['documents'][0],
                                'metadata': doc_data['metadatas'][0],
                                'score': hybrid_score,
                                'semantic_score': sem_score,
                                'bm25_score': bm25_score
                            }
                    except:
                        pass
        
        # Sort by score and return top k
        ranked_results = sorted(all_results.values(), key=lambda x: x['score'], reverse=True)
        return ranked_results[:k]

# ==============================================================================
# BATCH DOCUMENT INGESTION
# ==============================================================================

class BatchDocumentIngestion:
    """Optimized batch processing for large document collections"""
    
    def __init__(self, chroma_collection, processor: BankingDocumentProcessor):
        self.collection = chroma_collection
        self.processor = processor
        self.bm25_documents = []
    
    def ingest_documents(self, uploaded_files: List, progress_callback=None) -> Dict:
        """
        Batch ingest documents with progress tracking
        Returns statistics about the ingestion process
        """
        stats = {
            'total_files': len(uploaded_files),
            'processed_files': 0,
            'total_chunks': 0,
            'failed_files': [],
            'processing_time': 0
        }
        
        start_time = datetime.now()
        
        # Get existing documents to avoid duplicates
        existing_sources = self._get_existing_sources()
        
        batch_chunks = []
        
        for file_idx, uploaded_file in enumerate(uploaded_files):
            try:
                filename = uploaded_file.name
                
                # Skip if already indexed
                if filename in existing_sources:
                    if progress_callback:
                        progress_callback(file_idx + 1, len(uploaded_files), f"Skipped (already indexed): {filename}")
                    continue
                
                if progress_callback:
                    progress_callback(file_idx + 1, len(uploaded_files), f"Processing: {filename}")
                
                # Extract and chunk document
                doc_data = self.processor.extract_text_from_file(uploaded_file)
                
                if doc_data is None:
                    stats['failed_files'].append(filename)
                    continue
                
                chunks = self.processor.smart_chunk_document(doc_data, filename)
                
                # Add to batch
                batch_chunks.extend(chunks)
                self.bm25_documents.extend(chunks)
                
                # Process batch if it reaches batch size
                if len(batch_chunks) >= Config.BATCH_SIZE:
                    self._process_batch(batch_chunks)
                    stats['total_chunks'] += len(batch_chunks)
                    batch_chunks = []
                
                stats['processed_files'] += 1
                
            except Exception as e:
                stats['failed_files'].append(f"{uploaded_file.name}: {str(e)}")
        
        # Process remaining chunks
        if batch_chunks:
            self._process_batch(batch_chunks)
            stats['total_chunks'] += len(batch_chunks)
        
        stats['processing_time'] = (datetime.now() - start_time).total_seconds()
        
        return stats
    
    def _process_batch(self, chunks: List[Dict]):
        """Process a batch of chunks into ChromaDB"""
        if not chunks:
            return
        
        ids = [chunk['id'] for chunk in chunks]
        documents = [chunk['text'] for chunk in chunks]
        metadatas = [chunk['metadata'] for chunk in chunks]
        
        self.collection.add(
            ids=ids,
            documents=documents,
            metadatas=metadatas
        )
    
    def _get_existing_sources(self) -> set:
        """Get list of already indexed source files"""
        try:
            all_data = self.collection.get(include=["metadatas"])
            if all_data and all_data['metadatas']:
                return set(meta.get('source_file', '') for meta in all_data['metadatas'])
        except:
            pass
        return set()
    
    def build_bm25_index(self) -> PersistentBM25:
        """Build and save BM25 index from ingested documents"""
        bm25 = PersistentBM25()
        bm25.fit(self.bm25_documents)
        bm25.save(Config.BM25_INDEX_PATH)
        return bm25

# ==============================================================================
# RAG GENERATION WITH IMPROVED CONTEXT
# ==============================================================================

class BankingRAGGenerator:
    """Enhanced RAG generation for banking documentation"""
    
    def __init__(self, azure_client):
        self.client = azure_client
    
    def generate_response(self, query: str, context_docs: List[Dict]) -> Dict:
        """Generate response with improved context assembly"""
        
        # Assemble context with source citations
        context_parts = []
        sources = []
        
        for idx, doc in enumerate(context_docs, 1):
            metadata = doc.get('metadata', {})
            source_file = metadata.get('source_file', 'Unknown')
            page = metadata.get('page', 'N/A')
            
            context_parts.append(
                f"[Document {idx} - {source_file}, Page {page}]\n{doc['text']}\n"
            )
            
            if source_file not in sources:
                sources.append(source_file)
        
        context = "\n".join(context_parts)
        
        # Enhanced prompt for banking documentation
        prompt = f"""Vous êtes un assistant RAG spécialisé dans les systèmes bancaires, l'architecture de données, et les produits financiers.

Votre rôle est de répondre et d'expliquer clairement les informations issues des documents, en les rendant compréhensibles pour tout type d'utilisateur, du plus général au plus technique.

[Contexte]
{context}

[Requête]
{query}

[INSTRUCTIONS GÉNÉRALES]

1. Utilisez exclusivement les informations présentes dans le contexte ci-dessus.

2. Fournissez une réponse claire, structurée et explicite, adaptée à la compréhension de tout utilisateur.
   - Si le sujet est technique, expliquez les notions en termes simples.
   - Si la question est générale, donnez une réponse complète mais concise.
   - Si la question est complexe, détaillez le raisonnement et le fonctionnement.

3. Chaque fois que vous mentionnez une donnée, un mécanisme ou un fait:
   - Citez immédiatement la source sous le format: [Source: nom_du_fichier.pdf, Page <numéro>]

4. Si l'information n'existe pas dans les documents, indiquez-le clairement.

5. Structure attendue:
   a. Réponse expliquée: Détaillez le contenu et son interprétation.
   b. Synthèse (si utile): Résumez la logique ou le fonctionnement global.
   c. Sources: Liste complète des documents utilisés.

6. Objectif:
   Rendre la réponse à la fois informative, explicative et vérifiable,
   qu'il s'agisse d'un utilisateur curieux ou d'un expert technique.

[Exemple de format]
Les procédures de vérification d'identité... [Source: procedure_kyc.pdf, Page 12]. 
Selon le chapitre 3... [Source: reglement_financier.pdf, Page 45].

Sources utilisées:
- procedure_kyc.pdf (Pages 12, 15)
- reglement_financier.pdf (Page 45)
"""
        
        try:
            completion = self.client.chat.completions.create(
                model=Config.AZURE_MODEL_DEPLOYMENT,
                messages=[
                    {"role": "system", "content": "You are an expert assistant for banking system documentation based on RAG (Retrieval-Augmented Generation)."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            answer = completion.choices[0].message.content
            
            return {
                'answer': answer,
                'sources': sources,
                'context_docs': context_docs,
                'num_tokens': completion.usage.total_tokens if hasattr(completion, 'usage') else None
            }
            
        except Exception as e:
            return {
                'answer': f"Error generating response: {str(e)}",
                'sources': [],
                'context_docs': [],
                'num_tokens': None
            }

# ==============================================================================
# STREAMLIT APPLICATION
# ==============================================================================

def initialize_session_state():
    """Initialize Streamlit session state"""
    if 'chroma_client' not in st.session_state:
        st.session_state.chroma_client = chromadb.PersistentClient(path=Config.CHROMA_PERSIST_PATH)
    
    if 'collection' not in st.session_state:
        # Load embedding model
        embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL_PATH)
        
        # Create embedding function
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=Config.EMBEDDING_MODEL_PATH
        )
        
        st.session_state.collection = st.session_state.chroma_client.get_or_create_collection(
            name=Config.COLLECTION_NAME,
            embedding_function=embedding_function,
            metadata={"hnsw:space": "cosine"}
        )
    
    if 'bm25_index' not in st.session_state:
        # Load or create BM25 index
        if os.path.exists(Config.BM25_INDEX_PATH):
            st.session_state.bm25_index = PersistentBM25.load(Config.BM25_INDEX_PATH)
        else:
            st.session_state.bm25_index = PersistentBM25()
    
    if 'chat_sessions' not in st.session_state:
        st.session_state.chat_sessions = {}
    
    if 'current_chat' not in st.session_state:
        st.session_state.current_chat = str(datetime.now())
        st.session_state.chat_sessions[st.session_state.current_chat] = {
            "title": "New Chat",
            "messages": []
        }
    
    if 'processor' not in st.session_state:
        st.session_state.processor = BankingDocumentProcessor()
    
    if 'azure_client' not in st.session_state:
        # Initialize Azure OpenAI client
        http_client = httpx.Client(verify=False)
        st.session_state.azure_client = AzureOpenAI(
            api_version=Config.AZURE_API_VERSION,
            azure_endpoint=Config.AZURE_ENDPOINT,
            api_key=Config.AZURE_API_KEY,
            http_client=http_client
        )
    
    if 'rag_generator' not in st.session_state:
        st.session_state.rag_generator = BankingRAGGenerator(st.session_state.azure_client)

def main():
    st.set_page_config(page_title="BNP Banking RAG Chatbot", layout="wide")
    
    initialize_session_state()
    
    st.title("🏦 BNP Banking Documentation Chatbot")
    st.caption("Optimized RAG system with hybrid search and batch processing")
    
    # Sidebar for document management
    with st.sidebar:
        st.header("📁 Document Management")
        
        # Get collection stats
        try:
            collection_count = st.session_state.collection.count()
            st.metric("Indexed Documents", collection_count)
        except:
            collection_count = 0
            st.metric("Indexed Documents", "0")
        
        # Batch upload
        st.subheader("Upload Documents")
        uploaded_files = st.file_uploader(
            "Upload multiple documents",
            type=["pdf", "txt", "docx", "csv", "xlsx"],
            accept_multiple_files=True
        )
        
        if uploaded_files and st.button("📤 Process All Files"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(current, total, message):
                progress = current / total
                progress_bar.progress(progress)
                status_text.text(message)
            
            with st.spinner("Processing documents..."):
                batch_ingestion = BatchDocumentIngestion(
                    st.session_state.collection,
                    st.session_state.processor
                )
                
                stats = batch_ingestion.ingest_documents(
                    uploaded_files,
                    progress_callback=update_progress
                )
                
                # Rebuild BM25 index
                status_text.text("Building BM25 index...")
                st.session_state.bm25_index = batch_ingestion.build_bm25_index()
                
                # Display results
                st.success(f"""
                ✅ Processing complete!
                - Files processed: {stats['processed_files']}/{stats['total_files']}
                - Total chunks: {stats['total_chunks']}
                - Time: {stats['processing_time']:.2f}s
                """)
                
                if stats['failed_files']:
                    st.error(f"Failed files: {', '.join(stats['failed_files'])}")
        
        st.divider()
        
        # Chat management
        st.subheader("💬 Chat Sessions")
        if st.button("➕ New Chat"):
            new_chat_id = str(datetime.now())
            st.session_state.current_chat = new_chat_id
            st.session_state.chat_sessions[new_chat_id] = {
                "title": "New Chat",
                "messages": []
            }
            st.rerun()
        
        # List existing chats
        for chat_id, chat_data in list(st.session_state.chat_sessions.items()):
            if st.button(f"📝 {chat_data['title'][:30]}", key=f"chat_{chat_id}"):
                st.session_state.current_chat = chat_id
                st.rerun()
    
    # Main chat interface
    current_chat = st.session_state.chat_sessions.get(st.session_state.current_chat, {
        "title": "New Chat",
        "messages": []
    })
    
    # Display chat messages
    for message in current_chat['messages']:
        with st.chat_message(message['role']):
            st.markdown(message['content'])
            
            # Show sources if available
            if message['role'] == 'assistant' and 'sources' in message:
                with st.expander("📚 Sources"):
                    for source in message['sources']:
                        st.text(f"• {source}")
    
    # Chat input
    if prompt := st.chat_input("Ask about banking documentation..."):
        # Add user message
        current_chat['messages'].append({
            "role": "user",
            "content": prompt,
            "timestamp": str(datetime.now())
        })
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("🔍 Searching documentation..."):
                # Create hybrid retriever
                retriever = HybridRetriever(
                    st.session_state.collection,
                    st.session_state.bm25_index
                )
                
                # Retrieve relevant documents
                retrieved_docs = retriever.retrieve(prompt, k=Config.FINAL_RESULTS_K)
                
                # Generate response
                response = st.session_state.rag_generator.generate_response(
                    prompt,
                    retrieved_docs
                )
                
                # Display answer
                st.markdown(response['answer'])
                
                # Show sources
                if response['sources']:
                    with st.expander("📚 Sources"):
                        for source in response['sources']:
                            st.text(f"• {source}")
                
                # Add to chat history
                current_chat['messages'].append({
                    "role": "assistant",
                    "content": response['answer'],
                    "sources": response['sources'],
                    "timestamp": str(datetime.now())
                })
                
                # Update chat title if first message
                if current_chat['title'] == "New Chat" and len(current_chat['messages']) > 0:
                    current_chat['title'] = prompt[:50] + "..."

if __name__ == "__main__":
    main()







Nouvelle slide – Public cible
À qui s’adresse la solution RAG BNP ED ?
RAG BNP ED a été développée pour accompagner différents profils impliqués dans l’exploitation et la transformation du système d’information :
👥 Équipes métiers
Accès rapide et fiable à la documentation ATLAS
Recherche simplifiée sans connaissance technique
Réponses claires, sourcées et contextualisées
🏗️ Équipes IT / DSI
Compréhension de l’architecture de la documentation ATLAS
Accès structuré aux référentiels et aux dépendances documentaires
Support à l’analyse et à la maintenance du SI
🔄 Équipes Transformation du Système d’Information
Accompagnement des projets de migration et d’évolution du SI
Vision transverse de la documentation existante
Réduction du temps de recherche et d’analyse documentaire
💡 Variante plus synthétique (si tu veux une slide très légère)
RAG BNP ED s’adresse aux équipes métiers, à la DSI et aux acteurs de la transformation du système d’information, en facilitant l’accès, la compréhension et l’exploitation de la documentation ATLAS.














RAG BNP ED est un chatbot intelligent basé sur la méthode RAG (Retrieval Augmented Generation) et sur une base de connaissances centralisée contenant la documentation ATLAS.
Il permet une recherche conversationnelle rapide et fiable, avec des réponses systématiquement sourcées à partir des documents officiels.
La solution évolue automatiquement avec les nouvelles documentations, garantissant des réponses toujours à jour.






RAG BNP ED est un chatbot intelligent basé sur une base de connaissances centralisée (documentation ATLAS).
Il permet une recherche conversationnelle rapide et fiable, avec des réponses systématiquement sourcées.
La solution évolue automatiquement avec les nouvelles documentations, garantissant des réponses toujours à jour.








La documentation ATLAS est volumineuse et critique pour les équipes métiers
L’accès à l’information repose aujourd’hui sur une recherche manuelle chronophage
Objectif : faciliter l’accès rapide, fiable et sourcé à l’information interne via un outil intelligent




Bonjour,

Pourriez-vous s’il vous plaît procéder à la réinitialisation de mon mot de passe ? Je vous remercie par avance.

Cordialement,
Nihad Senhadji


Bonjour à tous,

Suite à notre précédent échange, je vous informe que nous avons pu identifier et renseigner les dates d’impayés en nous basant sur l’extraction transmise par Chaima.

Toutefois, 11 cas restent pour lesquels les dates d’impayés n’ont pas pu être renseignées, malgré l’utilisation de cette extraction. Ces cas demeurent donc identifiés comme anomalies.

Je reste à votre disposition pour toute précision complémentaire ou pour échanger sur ces cas si besoin.

Cordialement,
Nihad Senhadji
GA-15








Bonjour à tous,

Veuillez trouver ci-joint la nouvelle version du fichier État Central des Engagements arrêté au 31/12/2025.

Lors du traitement, nous avons constaté que certaines dates d’impayés n’ont pas été retrouvées dans les données d’extraction disponibles.
Pour cette raison, une nouvelle feuille a été ajoutée au fichier, intitulée :

« État des engagements – Anomalies dates d’impayés »

Cette feuille regroupe l’ensemble des engagements pour lesquels les dates d’impayés n’ont pas pu être identifiées à partir de l’extraction.

Le fichier contient désormais les feuilles suivantes :

1. État Central des Engagements
2. État des engagements – Anomalies dates d’impayés
3. État détaillé par chapitre
4. Rapprochement des créances – Bilan
5. Rapprochement des créances hors bilan
6. Rapprochement CR / AR / CB
7. Reporting social BG
8. État détaillé par ratios prudentiels
9. Risques – Rapprochement RP
10. Risques – Grands Risques
11. Risques – Grands Risques par entité
12. Risques – Annexe 9000
13. Rapprochement reporting provisions
14. Rapprochement affectation des garanties
15. État des garanties

Je reste à votre disposition pour toute information complémentaire ou clarification si besoin.

Cordialement,
Nihad Senhadji
GA-15







Bien sûr ! Voici une version réécrite, fluide, professionnelle et claire de ton email :


---

Objet : Méthodologie – Détection de communautés de fraude (AI Sentinilia)

Bonjour Ryma,

Dans le cadre du projet AI Sentinilia dédié à la détection de communautés de fraude, la méthodologie utilisée repose sur les critères suivants :

Les liens entre clients sont identifiés à partir de coïncidences lorsqu’ils effectuent des retraits :

à la même date ;

au même GAB (ATM) ;

pour un montant supérieur ou égal à 500 euros ;

avec une fréquence d’au moins 5 retraits entre deux clients ou plus.



Lorsque ces conditions sont remplies, les clients sont considérés comme liés, car leurs comportements de retrait présentent des similitudes répétées et inhabituelles.

Afin de détecter les communautés de fraude, nous utilisons un algorithme appelé Louvain, qui permet de regrouper automatiquement les clients ayant des comportements similaires. Chaque communauté ainsi identifiée représente un groupe potentiel de fraude, basé sur des comportements de retraits coordonnés.

Cette solution n’a pas été modifiée depuis sa mise en place, et le suivi étant trimestriel, nous enverrons les prochains résultats en février 2026.

Je reste à votre disposition pour toute information complémentaire ou clarification si nécessaire.

Cordialement,
Nihad Senhadji


---

Si tu veux, je peux aussi te faire une version encore plus courte et simple, parfaite pour des non-techniques, qui tient en 5-6 phrases.
Veux‑tu que je fasse ça ?











Comme le suivi est trimestriel, nous enverrons les prochains résultats en février 2025.


Cet algorithme n’a pas été modifié depuis sa mise en place


Afin de détecter les communautés de fraude, nous utilisons un algorithme appelé Louvain, qui permet de regrouper automatiquement les clients ayant des comportements similaires.


Bonjour,

Dans le cadre du projet AI Sentinilia dédié à la détection de communautés de fraude, la méthodologie utilisée repose sur les étapes suivantes :

- Les liens entre clients sont d’abord identifiés à partir des critères suivants :

   - la localisation du GAB (ATM) où les retraits ont été effectués ;
   - la fréquence des retraits ;
   - des coïncidences entre plusieurs clients lorsque les retraits ont eu lieu :
      - à la même date,
      - au même GAB,
      - avec un montant supérieur ou égal à 500 euros,
      - et ce au moins 5 fois entre deux clients ou plus.

- Lorsque ces conditions sont réunies, les clients sont considérés comme liés, car leurs comportements de retrait présentent des similitudes répétées et inhabituelles.

- La méthode de Louvain est ensuite utilisée pour regrouper automatiquement les clients liés :

   - elle permet de constituer des communautés de clients fortement connectés ;
   - chaque communauté représente un groupe potentiel de fraude, basé sur des comportements de retraits coordonnés.

Je reste à votre disposition pour toute information complémentaire ou clarification si nécessaire.

Cordialement,

Nihad Senhadji



















Bonjour,

Je vous informe que j’ai annulé la version du fichier déjà envoyée précédemment.
Nous sommes actuellement en train de finaliser la nouvelle version, et je vous l’enverrai dès que ce sera terminé.

Merci pour votre compréhension.

Cordialement,
Nihad Senhadji

Bonjour,

Veuillez trouver ci-joint le fichier demandé, comme convenu.

Nous vous enverrons la version finale, contenant l’ensemble des feuilles, dans la soirée.

Cordialement,
Nihad Senhadji


Les colonnes du fichier fournissent toutes les informations nécessaires — identité, dates, lieux et références clients — afin de faciliter et structurer l’analyse des équipes sur ces cas sensibles.


La deuxième feuille, avec un score de similarité inférieur à 95 %, correspond aux cas où le modèle d’IA a permis de réduire une grande partie de la recherche manuelle.
Dans cette feuille, tous les noms qui ne figurent pas dans notre base clients Atlas sont automatiquement écartés, ce qui élimine les faux positifs et allège considérablement le travail des équipes opérationnelles.
🔹 La troisième feuille, avec un score de similarité supérieur à 95 %, regroupe les noms nécessitant une analyse approfondie par l’équipe opérationnelle, car ils présentent une forte probabilité de correspondance.










🎬 Script vidéo – Présentation de Decad AI Solution

> Bonjour,

Dans cette vidéo, je vais vous présenter Decad AI Solution, accessible via un lien de web application Domino.

On commence par l’insertion du fichier DECAD, envoyé par la Banque d’Algérie, directement dans l’application.

Ensuite, un simple clic permet de lancer le traitement. La solution s’appuie sur un modèle d’intelligence artificielle exécuté en arrière-plan, avec un temps d’exécution d’environ 10 minutes.

Une fois le traitement terminé, on clique sur “Résultats” afin de télécharger le fichier Excel de sortie, comme vous pouvez le voir à l’écran.

Le fichier Excel généré contient trois feuilles distinctes :

🔹 La première feuille, appelée Pièce d’audit, regroupe les résultats avec des scores de similarité inférieurs et supérieurs à 95 %.

🔹 La deuxième feuille contient les résultats avec un score inférieur à 95 %, correspondant aux cas non identifiés par le modèle d’IA.

🔹 La troisième feuille regroupe les résultats avec un score supérieur à 95 %, qui représentent les cas prioritaires à analyser par l’équipe opérationnelle.

Il est important de noter que les trois feuilles contiennent exactement les mêmes colonnes.

Voici la liste des colonnes suivantes :

COLONNE 1 et 2 DECAD

Nom interdits chequiers

Raison sociale base atlas

Nom abrégé tiers

Date naissance interdits chequiers ET DE NOTRE BASE ATLAS

Date de création (entreprise)

Date naissance Match

Date de création Match

Lieu naissance interdits chequier ET DE LA BASE atlas

Lieu naissance

Lieu naissance Match

Date d'effet et de levée

Compte client

Code catégorie client

Id tiers

Score de similarité


Grâce à cette structuration, Decad AI Solution permet de réduire le temps de traitement, de prioriser les analyses, et de sécuriser le processus de contrôle grâce à l’intelligence artificielle.

Merci pour votre attention.




---

If you want, I can also make a version more “oral / dynamique”, so it sounds natural when you speak it in the video, like a real demo narration.

Do you want me to do that?



















Bonjour,

Merci pour votre message et pour la transmission de l’Expression de Besoin (EDB) relative à l’automatisation du processus de traitement des agios réservés et de leurs reprises liées aux clients douteux – périmètre EPS.

Suite à l’analyse de cette EDB et conformément aux éléments partagés, je vous prie de trouver en pièce jointe les résultats de l’APA relatifs au mois de décembre 2025.

Ces résultats correspondent uniquement au traitement du mois de décembre 2025 et constituent la sortie actuelle de l’APA à ce stade.
Le fichier Excel de résultats contient à la fois des clients sains et des clients douteux.

Ils sont le fruit de plusieurs ateliers de travail menés avec Mouna, au cours desquels nous avons réalisé de nombreuses vérifications manuelles, ainsi que des rapprochements détaillés entre les résultats APA et les résultats manuels du mois d’octobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les règles de gestion appliquées dans le cadre de cette APA, en cohérence avec les besoins fonctionnels et techniques décrits dans l’EDB et avec la recommandation de la Banque d’Algérie visant à remplacer le traitement manuel actuellement réalisé par l’équipe DRAC.

Il s’agit actuellement d’une phase de test. Il est proposé de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilité du dispositif et de permettre la validation définitive des résultats par vos soins.

Par ailleurs, l’APA sera livrée la semaine prochaine afin d’être déployée et exécutée sur les postes des utilisateurs concernés.
À cet effet, nous aurons besoin :

- de l’installation de l’APA sur les postes utilisateurs,
- ainsi que de la mise à disposition d’un lien d’accès vers l’emplacement de dépôt des fichiers de sortie.

Je reste bien entendu à votre disposition, ainsi qu’Amina, pour tout complément d’information ou si vous souhaitez planifier un point d’échange.

Cordialement,
Nihad Senhadji

---

If you want, I can now:

- make a very formal / regulatory version (signature–engagement context), or
- shorten it for a manager-level reply.












Le fichier Excel de résultats contient à la fois des clients sains et des clients douteux


Bonjour,

Merci pour votre message et pour les éléments transmis lors de notre échange de jeudi.

Suite à l’analyse de l’expression de besoin relative à l’automatisation du traitement des agios réservés leasing, je vous prie de trouver en pièce jointe les résultats de l’APA relatifs au mois de décembre 2025.

Ces résultats correspondent uniquement au traitement du mois de décembre 2025 et constituent la sortie actuelle de l’APA à ce stade.

Ils sont le fruit de plusieurs ateliers de travail menés avec Mouna, au cours desquels nous avons réalisé de nombreuses vérifications manuelles, ainsi que des rapprochements détaillés entre les résultats APA et les résultats manuels du mois d’octobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les règles de gestion appliquées dans le cadre de cette APA.

Il s’agit actuellement d’une phase de test. Il est proposé de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilité du dispositif et de permettre la validation définitive des résultats par vos soins.

Par ailleurs, l’APA sera livrée la semaine prochaine afin d’être déployée et exécutée sur les postes des utilisateurs concernés.
À cet effet, nous aurons besoin :

- de l’installation de l’APA sur les postes utilisateurs,
- ainsi que de la mise à disposition d’un lien d’accès vers l’emplacement de dépôt des fichiers de sortie.

Je reste bien entendu à votre disposition pour tout complément d’information, retour ou ajustement nécessaire.

Cordialement,
Nihad Senhadji

---

If you want, I can also adapt the wording to:

- explicitly request






Bonjour,

Merci pour votre message et pour les éléments transmis lors de notre échange de jeudi.

Suite à l’analyse de l’expression de besoin relative à l’automatisation du traitement des agios réservés leasing, je vous prie de trouver en pièce jointe les résultats de l’APA.

Ces résultats correspondent uniquement au traitement du mois en cours et constituent la sortie actuelle de l’APA à ce stade.

Ils sont le fruit de plusieurs ateliers de travail menés avec Mouna, au cours desquels nous avons réalisé de nombreuses vérifications manuelles, ainsi que des rapprochements détaillés entre les résultats APA et les résultats manuels du mois d’octobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les règles de gestion appliquées dans le cadre de cette APA.

Il s’agit actuellement d’une phase de test. Il est proposé de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilité du dispositif et de permettre la validation définitive des résultats par vos soins.

Par ailleurs, l’APA sera livrée la semaine prochaine afin d’être déployée et exécutée sur les postes des utilisateurs concernés.
À cet effet, nous aurons besoin :

- de l’installation de l’APA sur les postes utilisateurs,
- ainsi que de la mise à disposition d’un lien d’accès vers l’emplacement de dépôt des fichiers de sortie.

Je reste bien entendu à votre disposition pour tout complément d’information, retour ou ajustement nécessaire.

Cordialement,
Nihad Senhadji

---

If you want, I can also:

- slightly strengthen the request for validation, or
- adapt the wording to a more regulatory / audit-compliant style for DRAC.






Bonjour [Nom du manager],

Je souhaite vous partager un point de situation concernant l’avancement des projets APA.

1. Avancement général (APA ESP & APA Arrêté)
Suite aux différents workshops organisés avec la DRAC, l’ensemble des règles de gestion a été confirmé et validé pour les deux projets.
Un rapprochement a également été réalisé sur la base de leurs fichiers de novembre 2025.

2. Projet APA ESP
Un seul point reste identifié : un problème de code fusion concernant un compte unique.
La DRAC m’a confirmé que ce point est déjà porté à votre connaissance.

3. Projet APA Arrêté
Un seul point est actuellement en suspens :

- Un écart lié au solde du compte à vue.
À ce stade, la DRAC n’est pas en mesure d’identifier la méthode ou la source utilisée pour le calcul de ce solde et doit effectuer des vérifications de son côté avant de nous apporter un retour définitif.

4. Projet APA Leasing
Le projet APA Leasing n’a pas encore été démarré à ce stade.

Vous trouverez ci-dessous le lien regroupant les éléments pour les projets APA ESP et APA Arrêté :
[Lien à insérer]

Je reste à votre disposition pour toute précision complémentaire ou pour un point de suivi.

Cordialement,
Nihad Senhadji






your thanks:

"Je tiens à vous remercier pour votre accompagnement et vos conseils tout au long de l’année, et je souhaite continuer à contribuer activement à la réussite des objectifs de la Banque, tout en participant aux projets IA et métiers."

It’s professional, appreciative, and forward-looking.

I can also make a slightly shorter, sharper version if you want it for an email reply.

You can write it like this:

"Je souhaite continuer à contribuer activement à la réussite des objectifs de la Banque, tout en participant aux projets IA et métiers."

It’s concise, professional, and inclusive of both AI and other banking projects.

If you want, I can also suggest 2–3 alternative formulations that sound slightly more executive.


Understood. We need a leadership-oriented tone without implying you want to replace him—showing responsibility, initiative, and team support while still highlighting AI. Here’s a refined version:


---

Bonjour [Nom du manager],

Merci pour votre retour et pour l’évaluation de ma contribution en 2025. J’ai bien pris en compte les axes d’amélioration identifiés, notamment sur le cadrage des besoins métiers, la structuration des travaux et la clarté de la communication.

Je souhaite exprimer mon objectif professionnel, qui est d’évoluer vers un rôle sur des projets en Intelligence Artificielle, avec des responsabilités clairement définies et des livrables à impact opérationnel.

Je reste par ailleurs pleinement engagée à soutenir l’ensemble des projets bancaires et à accompagner l’équipe dans l’organisation et la coordination des travaux, afin de garantir la qualité et l’efficacité des livrables, tout en contribuant activement à la réussite des projets IA et métiers.

Cordialement,
Nihad Senhadji


---

This version:

Shows initiative and readiness to support the team

Positions you for AI projects

Avoids sounding like you want to replace him


If you want, I can make an even snappier, executive-style version that fits in one paragraph. Do you want me to do that?
