prompt = f"""
[Contexte]
{context}

[Requête]
{query}

[Instructions]
1. Répondez en français de manière détaillée et précise en utilisant exclusivement les informations fournies
2. Pour chaque élément de réponse :
   - Mentionnez le document source entre crochets IMMÉDIATEMENT après l'information
   - Format : [Source: {{nom_fichier}}.pdf, Page {{numéro_page}}]
3. Si plusieurs documents contiennent l'information :
   - Citez toutes les sources concernées
4. Structure de réponse :
   a. Réponse détaillée avec citations inline
   b. Section "Sources" à la fin listant tous les documents utilisés
5. Si l'information n'existe pas dans les documents, indiquez-le clairement

[Exemple de format]
Les procédures de vérification d'identité... [Source: procedure_kyc.pdf, Page 12]. 
Selon le chapitre 3... [Source: reglement_financier.pdf, Page 45].

Sources utilisées : 
- procedure_kyc.pdf (Pages 12, 15)
- reglement_financier.pdf (Page 45)
"""

from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
import torch

def split_documents_semantic(documents):
    """Semantic chunking with local model"""
    # Path to your downloaded model
    model_path = "/path/to/your/bge-m3"  # Replace with your actual path
    
    # Initialize embeddings from local files
    embeddings = HuggingFaceEmbeddings(
        model_name=model_path,
        model_kwargs={
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "trust_remote_code": True
        },
        encode_kwargs={
            "normalize_embeddings": True,
            "batch_size": 32  # Adjust based on VRAM
        }
    )

    # Configure semantic splitter
    text_splitter = SemanticChunker(
        embeddings=embeddings,
        add_start_index=True,
        max_chunk_size=1500,  # Target max token count
        min_chunk_size=300,    # Minimum meaningful chunk size
        breakpoint_percentile_threshold=92  # 92% similarity threshold
    )
    
    return text_splitter.split_documents(documents)


from langchain_experimental.text_splitter import SemanticChunker
def split_documents_semantic(documents):
    """Hybrid semantic+size-based splitting"""
    model = SentenceTransformer("BAAI/bge-m3", device="cuda" if torch.cuda.is_available() else "cpu")
    
    # 1. Semantic chunking


    semantic_splitter = SemanticChunker(
        embeddings=model,
        breakpoint_threshold=0.82,  # More splits
        add_start_index=True
    )
    semi_chunks = semantic_splitter.split_documents(documents)
    
    # 2. Size enforcement
    size_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1200,
        chunk_overlap=200,
        separators=["\n\n## ", "\n\n", ". ", "; "]
    )
    
    final_chunks = size_splitter.split_documents(semi_chunks)
    
    # 3. Validate chunks
    avg_size = sum(len(c.page_content) for c in final_chunks)/len(final_chunks)
    print(f"Average chunk size: {avg_size:.0f} chars")
    
    return final_chunks
