# Uninstall potentially conflicting packages
pip uninstall qwen zeta -y

# Install PyTorch first (if not already installed)
pip install torch torchvision --break-system-packages

# Install the correct Qwen package
pip install transformers --break-system-packages
pip install qwen-vl-utils --break-system-packages




pip install torch torchvision torchaudio  # Core PyTorch
pip install pillow                         # Image handling
pip install pymupdf                        # PDF ‚Üí images
pip install qwen                           # Official Qwen VL package




pip install tqdm                            # Progress bars
pip install numpy                           # Array operations (often required by Qwen)
pip install transformers                     # Only if you need extra transformer utils


pip install tqdm                            # Progress bars
pip install numpy                           # Array operations (often required by Qwen)
pip install transformers                     # Only if you need extra transformer utils






import fitz  # PyMuPDF
import json
from PIL import Image
from qwen import QwenVL, QwenProcessor  # official Qwen VL loader

# ===============================
# CONFIG
# ===============================
MODEL_PATH = "/path/to/Qwen2.5-VL-32B-Instruct"  # your local folder
PDF_PATH = "scanned.pdf"
OUTPUT_FILE = "output.json"

# ===============================
# LOAD MODEL
# ===============================
print("Loading Qwen2.5-VL-32B-Instruct...")
processor = QwenProcessor.from_pretrained(MODEL_PATH)
model = QwenVL.from_pretrained(MODEL_PATH)
model.eval()  # CPU by default
print("Model loaded successfully.")

# ===============================
# CONVERT PDF TO IMAGES
# ===============================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page_number in range(len(doc)):
        page = doc[page_number]
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# ===============================
# EXTRACT JSON FROM IMAGE
# ===============================
def extract_json_from_image(image):
    prompt = """
You are a document extraction engine.

Extract ALL visible text and structure from this scanned document.

Return strictly valid JSON only.
Do not add explanations or summaries.
"""

    # Preprocess the image + prompt
    inputs = processor(prompt, image, return_tensors="pt")

    # Generate the model output
    with torch.no_grad():
        output = model.generate(**inputs, max_new_tokens=2048, temperature=0.0)

    # Decode text
    text_output = processor.decode(output[0])
    return text_output

# ===============================
# MAIN PIPELINE
# ===============================
def main():
    images = pdf_to_images(PDF_PATH)
    all_pages = []

    for i, img in enumerate(images):
        print(f"Processing page {i+1}...")
        result_text = extract_json_from_image(img)

        # Attempt to parse JSON, fallback to raw output
        try:
            parsed = json.loads(result_text)
        except json.JSONDecodeError:
            parsed = {"raw_output": result_text}

        all_pages.append({
            "page_number": i + 1,
            "content": parsed
        })

    final_output = {
        "document_name": PDF_PATH,
        "total_pages": len(all_pages),
        "pages": all_pages
    }

    # Save to JSON file
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)

    print(f"Extraction complete. Saved to {OUTPUT_FILE}")

# ===============================
# ENTRY POINT
# ===============================
if __name__ == "__main__":
    main()




import fitz  # PyMuPDF
import json
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq

# ===============================
# CONFIG
# ===============================
MODEL_PATH = "/path/to/Qwen2.5-VL-32B-Instruct"  # Your local folder
PDF_PATH = "scanned.pdf"
OUTPUT_FILE = "output.json"

# ===============================
# Load Model
# ===============================
print("Loading model...")
processor = AutoProcessor.from_pretrained(MODEL_PATH)
model = AutoModelForVision2Seq.from_pretrained(MODEL_PATH)
model.eval()
print("Model loaded.")

# ===============================
# Convert PDF to Images
# ===============================
def pdf_to_images(pdf_path):
    doc = fitz.open(pdf_path)
    images = []
    for page_number in range(len(doc)):
        page = doc[page_number]
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images

# ===============================
# Extract JSON from Image
# ===============================
def extract_json_from_image(image):
    prompt = """
You are a document extraction engine.

Extract ALL visible text and structure from this scanned document.

Return strictly valid JSON.
Do not add explanations.
"""

    inputs = processor(
        text=prompt,
        images=image,
        return_tensors="pt"
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            temperature=0.0
        )

    result = processor.decode(output[0], skip_special_tokens=True)
    return result

# ===============================
# MAIN
# ===============================
def main():
    images = pdf_to_images(PDF_PATH)
    all_pages = []

    for i, img in enumerate(images):
        print(f"Processing page {i+1}...")
        result = extract_json_from_image(img)

        try:
            parsed = json.loads(result)
        except:
            parsed = {"raw_output": result}

        all_pages.append({
            "page_number": i + 1,
            "content": parsed
        })

    final_output = {
        "document_name": PDF_PATH,
        "total_pages": len(all_pages),
        "pages": all_pages
    }

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)

    print(f"Extraction complete. Saved to {OUTPUT_FILE}")

if __name__ == "__main__":
    main()





pip install torch torchvision transformers pymupdf pillow




if "chat_sessions" not in st.session_state:
    st.session_state.chat_sessions = load_chat_sessions()

import json
import os

CHAT_STORAGE_PATH = "chat_sessions.json"


def load_chat_sessions():
    if os.path.exists(CHAT_STORAGE_PATH):
        with open(CHAT_STORAGE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_chat_sessions(chat_sessions):
    with open(CHAT_STORAGE_PATH, "w", encoding="utf-8") as f:
        json.dump(chat_sessions, f, indent=2, ensure_ascii=False)









if st.sidebar.button("‚ûï Start New Chat"):
    chat_id = str(datetime.datetime.now())
    st.session_state.current_chat = chat_id
    st.session_state.chat_sessions[chat_id] = {
        "title": "New Chat",
        "messages": []
    }
    save_chat_sessions(st.session_state.chat_sessions)  # ‚úÖ ADD THIS





chat_data["messages"].append({
    "role": "user",
    "content": user_input,
    "timestamp": timestamp
})

save_chat_sessions(st.session_state.chat_sessions)  # ‚úÖ ADD THIS






import os
import glob

from langchain.document_loaders import (
    PyPDFLoader,
    CSVLoader,
    UnstructuredExcelLoader,
    UnstructuredWordDocumentLoader
)

# Your real documents directory
DOCS_DIR = "ant/"

def load_documents():
    documents = []

    procedures_dir = os.path.join(DOCS_DIR, "Procedures")

    # ---------- Load PDFs ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.pdf")):
            try:
                loader = PyPDFLoader(file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "pdf"

                documents.extend(docs)
                print(f"Loaded PDF: {file_name}")

            except Exception as e:
                print(f"Error loading PDF {file}: {e}")

    # ---------- Load CSVs ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.csv")):
            try:
                loader = CSVLoader(file_path=file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "csv"

                documents.extend(docs)
                print(f"Loaded CSV: {file_name}")

            except Exception as e:
                print(f"Error loading CSV {file}: {e}")

    # ---------- Load Excel ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.xlsx")):
            try:
                loader = UnstructuredExcelLoader(file_path=file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "excel"

                documents.extend(docs)
                print(f"Loaded Excel: {file_name}")

            except Exception as e:
                print(f"Error loading Excel {file}: {e}")

    # ---------- Load Word (.docx) ----------
    if os.path.exists(procedures_dir):
        for file in glob.glob(os.path.join(procedures_dir, "*.docx")):
            try:
                loader = UnstructuredWordDocumentLoader(file_path=file)
                docs = loader.load()

                file_name = os.path.basename(file)
                for doc in docs:
                    doc.metadata["source_file"] = file_name
                    doc.metadata["source_type"] = "word"

                documents.extend(docs)
                print(f"Loaded Word: {file_name}")

            except Exception as e:
                print(f"Error loading Word {file}: {e}")

    print(f"\nLoaded {len(documents)} documents in total.")
    return documents













def main():
    st.title("üí¨ Chatbot BNP ED")

    # -----------------------------
    # 1Ô∏è‚É£ Session initialization
    # -----------------------------
    if "chat_sessions" not in st.session_state:
        st.session_state.chat_sessions = {}

    if "current_chat" not in st.session_state:
        chat_id = str(datetime.datetime.now())
        st.session_state.current_chat = chat_id
        st.session_state.chat_sessions[chat_id] = {
            "title": "New Chat",
            "messages": []
        }

    # -----------------------------
    # 2Ô∏è‚É£ Start new chat
    # -----------------------------
    if st.sidebar.button("‚ûï Start New Chat"):
        chat_id = str(datetime.datetime.now())
        st.session_state.current_chat = chat_id
        st.session_state.chat_sessions[chat_id] = {
            "title": "New Chat",
            "messages": []
        }

    # -----------------------------
    # 3Ô∏è‚É£ Chat selector
    # -----------------------------
    chat_ids = list(st.session_state.chat_sessions.keys())

    selected_chat = st.sidebar.radio(
        "Chat Sessions",
        chat_ids,
        format_func=lambda k: st.session_state.chat_sessions[k]["title"],
        index=chat_ids.index(st.session_state.current_chat)
        if st.session_state.current_chat in chat_ids else 0
    )

    st.session_state.current_chat = selected_chat
    chat_data = st.session_state.chat_sessions[selected_chat]

    # -----------------------------
    # 4Ô∏è‚É£ Display chat history
    # -----------------------------
    for message in chat_data["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"], unsafe_allow_html=True)

    # -----------------------------
    # 5Ô∏è‚É£ User input
    # -----------------------------
    user_input = st.chat_input("Type your message...")

    if user_input:
        timestamp = datetime.datetime.now().isoformat()

        # Rename chat on first message
        if chat_data["title"] == "New Chat":
            chat_data["title"] = user_input[:30] + "..."

        # Save user message
        chat_data["messages"].append({
            "role": "user",
            "content": user_input,
            "timestamp": timestamp
        })

        with st.chat_message("user"):
            st.markdown(user_input)

        # Generate assistant response
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = rag_chatbot(user_input)
                answer = response["answer"]
                st.markdown(answer, unsafe_allow_html=True)

        # Save assistant message
        chat_data["messages"].append({
            "role": "assistant",
            "content": answer,
            "timestamp": timestamp
        })










def get_chat_history(max_turns=6):
    messages = st.session_state.chat_sessions[
        st.session_state.current_chat
    ]["messages"]

    history = []
    for msg in messages[-max_turns:]:
        role = msg["role"].capitalize()
        history.append(f"{role}: {msg['content']}")

    return history

from openai import AzureOpenAI
import httpx


def generate_response(query, docs):
    context = build_context(docs)
    chat_history = get_chat_history()

    history_text = "\n".join(chat_history)

    prompt = f"""
Vous √™tes un assistant RAG sp√©cialis√© dans les syst√®mes bancaires,
l‚Äôarchitecture de donn√©es et les produits financiers.

[Historique]
{history_text}

[Contexte]
{context}

[Question]
{query}

INSTRUCTIONS:
- Utilisez exclusivement le contexte
- Citez les sources: [Source: fichier.pdf, Page X]
- Si l'information n'existe pas, dites-le clairement
"""

    client = AzureOpenAI(
        api_key=AZURE_OPENAI_API_KEY,
        api_version=AZURE_OPENAI_API_VERSION,
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        http_client=httpx.Client(verify=False)
    )

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "Assistant RAG bancaire"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3
    )

    return response.choices[0].message.content


def rag_chatbot(query):
    chat_history = get_chat_history()

    retriever = CustomRetrieverWithHistory(
        collection=collection,
        chat_history=chat_history,
        k=2500,
        rerank_k=50
    )

    docs = retriever.get_relevant_documents(query)
    answer = generate_response(query, docs)

    return {
        "answer": answer,
        "documents": docs
    }














def generate_response(query, context):
    # Collect sources
    sources = list(set([chunk["metadata"]["source"] for chunk in context]))

    # Build prompt
    context_text = build_context(context)
    prompt = f"""
Vous √™tes un assistant RAG sp√©cialis√© dans les syst√®mes bancaires,
l'architecture de donn√©es, et les produits financiers. Votre r√¥le est de r√©pondre et d'expliquer clairement les
informations issues des documents, en les rendant compr√©hensibles pour tout type d'utilisateur, du plus g√©n√©ral au plus technique.

[Contexte]
{context_text}

[Requ√™te]
{query}

[INSTRUCTIONS GENERALES]
1. Utilisez exclusivement les informations pr√©sentes dans le contexte ci-dessus.
2. Fournissez une r√©ponse claire, structur√©e et d√©taill√©e, adapt√©e √† la compr√©hension de tout utilisateur.
3. Citez imm√©diatement la source sous le format [Source: <nom du fichier.pdf, Page <num√©ro>].
4. Si l'information n'existe pas dans les documents, indiquez-le clairement.
5. Structure attendue:
   a. R√©ponse d√©taill√©e
   b. Synth√®se (si utile)
   c. Sources
6. Objectif: rendre la r√©ponse informative, explicative et v√©rifiable.
"""

    # Initialize Azure/OpenAI client
    # Make sure the API key, endpoint, and version are correct
    import httpx
    from openai import OpenAI  # or your wrapper

    client = OpenAI(
        api_key="YOUR_API_KEY",
        api_base="YOUR_AZURE_ENDPOINT",
        api_type="azure",
        api_version="2023-03-15-preview"  # replace with your version
    )

    # Call the model
    completion = client.chat.completions.create(
        model="gpt4o",
        messages=[
            {"role": "system", "content": "You are an assistant based on RAG for banking system architecture."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.5  # 0.5 is reasonable; 8.5 is invalid
    )

    # Handle return type (Azure wrapper may return string or dict)
    if isinstance(completion, str):
        response = completion
    elif hasattr(completion, "choices"):
        response = completion.choices[0].message["content"]
    else:
        raise ValueError(f"Unexpected completion type: {type(completion)}")

    # Clean up whitespace for readability
    lines = [line.strip() for line in response.split("\n") if line.strip()]
    readable_response = "\n".join(lines)

    return readable_response










Here‚Äôs a polished French version of your email:


---

Objet : Mise √† jour de la documentation ¬´ Usine de Fiable ¬ª

Bonjour [Nom du coll√®gue],

Veuillez trouver ci-joint la version mise √† jour de la documentation ¬´ Usine de Fiable ¬ª, √† laquelle j‚Äôai ajout√© le point [ou ‚Äúcette section‚Äù selon ce que vous avez ajout√©].

Merci de bien vouloir confirmer la r√©ception et n‚Äôh√©sitez pas √† m‚Äôajouter tout √©l√©ment que vous souhaitez discuter ou revoir.

Cordialement,
[Votre nom]


---

If you want, I can also make a slightly more friendly and collaborative version that encourages discussion without sounding too formal. Do you want me to do that?






appliqu√©es apr√®s la production de ce mois.


Ajout d'une nouvelle donn√©e / nouveau contr√¥le ¬´ DCC ¬ª :
Ainsi, si vous avez re√ßu un nouveau contr√¥le de donn√©e, effectu√© une modification sur un contr√¥le existant, ou r√©activ√© un ancien contr√¥le de donn√©e dans le p√©rim√®tre, ou effectu√© toute autre forme de modification, merci de ne pas appliquer ces modifications apr√®s le 20 de chaque mois.




# Initialize retriever
retriever = CustomRetrieverWithHistory(
    collection=collection,      # your Chroma collection
    query_model=query_model,    # BGE-M3 with instruction
    doc_model=doc_model,        # BGE-M3 without instruction
    chat_history=[],            # optional
    k=100,                      # fetch top 100 dense docs
    rerank_k=50                 # rerank top 50 with BM25
)

query = "Which table stores AML risk classification for retail clients?"

docs = retriever.get_relevant_documents(query)

# Output results
for d in docs:
    print(d["metadata"], "\n", d["text"])




import string
import re
import numpy as np
from rank_bm25 import BM25Okapi

# ---------- BM25 Reranker ----------
class BM25Reranker:
    def __init__(self, k=50):
        self.k = k
        self.bm25 = None
        self.document_map = {}
        self.tokenizer = str.maketrans("", "", string.punctuation)

    def _tokenize(self, text):
        if not isinstance(text, str):
            return []
        cleaned = text.translate(self.tokenizer).lower()
        return [t for t in cleaned.split() if t.strip()]

    def fit(self, documents):
        if not documents:
            raise ValueError("Empty document list given to BM25Reranker")
        tokenized_corpus = []
        valid_docs = []
        for i, doc in enumerate(documents):
            text = doc.get("text") or doc.get("page content") or doc.get("content")
            tokens = self._tokenize(text)
            if tokens:
                tokenized_corpus.append(tokens)
                valid_docs.append((i, doc))
        if not tokenized_corpus:
            raise ValueError("No valid documents to fit BM25 model")
        self.document_map = {i: doc for i, doc in valid_docs}
        self.bm25 = BM25Okapi(tokenized_corpus)

    def rerank(self, query, documents):
        if not documents:
            return []
        self.fit(documents)
        tokens = self._tokenize(query)
        if not tokens:
            return documents[:self.k]
        scores = self.bm25.get_scores(tokens)
        ranked = sorted(
            zip(scores, self.document_map.values()),
            key=lambda x: x[0],
            reverse=True
        )
        return [doc for _, doc in ranked[:self.k]]



# ---------- Custom Retriever with Query & Document Embeddings ----------
class CustomRetrieverWithHistory:
    def __init__(self, collection, query_model, doc_model, chat_history=None, k=50, rerank_k=50):
        """
        collection: Chroma collection
        query_model: BGE-M3 model with query_instruction
        doc_model: BGE-M3 model without instruction
        chat_history: optional chat context
        k: number of dense results to fetch
        rerank_k: number of results for BM25 reranking
        """
        self.collection = collection
        self.chat_history = chat_history or []
        self.k = k
        self.rerank_k = rerank_k
        self.query_model = query_model
        self.doc_model = doc_model
        self.reranker = BM25Reranker(k=rerank_k)

    def get_relevant_documents(self, query):
        # --- combine chat history if exists ---
        if self.chat_history:
            full_query = " ".join(self.chat_history) + " " + query
        else:
            full_query = query

        # --- encode the query using the correct query embedding model ---
        query_embedding = self.query_model.encode([full_query])[0].tolist()

        # --- dense retrieval using query embedding ---
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=self.k,
            include=["documents", "metadatas"]
        )

        # --- collect dense docs ---
        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(results["documents"][0], results["metadatas"][0])
        ]

        # --- BM25 rerank over dense docs as hybrid step ---
        reranked_docs = self.reranker.rerank(full_query, dense_docs)

        return reranked_docs


query = "Which table stores AML risk classification for retail clients?"

query_embedding = query_model.encode([query])[0].tolist()

results = collection.query(
    query_embeddings=[query_embedding],
    n_results=10,
    include=["documents", "metadatas"]
)




MODEL_PATH = "/domino/edv/modelhub/ModelHub-model-huggingface-BAAI/bge-m3/main"

# ---- Document embedding model (NO instruction) ----
doc_model = FlagModel(
    MODEL_PATH,
    use_fp16=True
)

# ---- Query embedding model (WITH instruction) ----
query_model = FlagModel(
    MODEL_PATH,
    query_instruction_for_retrieval=(
        "Generate representation for this sentence to retrieve relevant articles:"
    ),
    use_fp16=True
)












class BGE_M3_EmbeddingFunction:
    def __init__(self, model):
        self.model = model
        self.dimension = 1024  # BGE-M3 dimension

    def __call__(self, inputs: List[str]) -> List[List[float]]:
        if isinstance(inputs, str):
            inputs = [inputs]

        embeddings = self.model.encode(inputs)

        if len(embeddings[0]) != self.dimension:
            raise ValueError(
                f"Embedding dimension mismatch: expected {self.dimension}, "
                f"got {len(embeddings[0])}"
            )

        return embeddings.tolist()collection = chroma_client.get_or_create_collection(
    name="my_documents",
    embedding_function=BGE_M3_EmbeddingFunction(doc_model),
    metadata={
        "hnsw:space": "cosine",
        "hnsw:construction_ef": 200,
        "hnsw:M": 16
    }
)




query = "CLIENT_ACCOUNT risk_level AML"

dense = chroma_collection.query(
    query_texts=[query],
    n_results=1000,   # very large on purpose
    include=["documents"]
)

dense_docs = dense["documents"][0]

found = any(
    any(chunk_id in d for chunk_id in ["CLIENT_ACCOUNT", "risk_level"])
    for d in dense_docs
)

print("Found in dense@1000:", found)




class BM25Reranker:
    def __init__(self, documents, k=50):
        """
        documents: list of {"text": ..., "metadata": ...}
        """
        self.k = k
        self.documents = documents
        self.tokenized_corpus = [self._tokenize(d["text"]) for d in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)

    def _tokenize(self, text: str):
        # preserves table names, columns, acronyms
        return re.findall(r"[A-Za-z0-9_\.]+", text.lower())

    def retrieve(self, query: str):
        tokens = self._tokenize(query)
        scores = self.bm25.get_scores(tokens)

        ranked = sorted(
            zip(scores, self.documents),
            key=lambda x: x[0],
            reverse=True
        )

        return [doc for _, doc in ranked[:self.k]]


class HybridRetriever:
    def __init__(
        self,
        chroma_collection,
        bm25_reranker,
        k_dense=200,
        k_final=20
    ):
        self.collection = chroma_collection
        self.bm25 = bm25_reranker
        self.k_dense = k_dense
        self.k_final = k_final

    def retrieve(self, query: str):
        # ---- 1. Query expansion (light, safe) ----
        expanded_query = (
            query
            + " table database schema column risk AML architecture"
        )

        # ---- 2. Dense retrieval (high recall) ----
        dense = self.collection.query(
            query_texts=[expanded_query],
            n_results=self.k_dense,
            include=["documents", "metadatas"]
        )

        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(dense["documents"][0], dense["metadatas"][0])
        ]

        # ---- 3. Sparse retrieval (BM25 rescue) ----
        sparse_docs = self.bm25.retrieve(query)

        # ---- 4. Union + dedup ----
        merged = {}
        for d in dense_docs + sparse_docs:
            merged[hash(d["text"])] = d

        merged_docs = list(merged.values())

        return merged_docs[:self.k_final]

def output_matching_chunks(collection, keyword: str):
    data = collection.get(include=["documents", "metadatas"])

    documents = data["documents"]
    metadatas = data["metadatas"]

    # handle list-of-lists
    if documents and isinstance(documents[0], list):
        documents = documents[0]
        metadatas = metadatas[0]

    hits = 0
    for i, (doc, meta) in enumerate(zip(documents, metadatas)):
        if keyword.lower() in doc.lower():
            print(f"\n--- MATCHING CHUNK {i} ---")
            print("Metadata:", meta)
            print(doc)
            hits += 1

    print("\nTotal matching chunks:", hits)

def build_context(docs):
    blocks = []
    for i, d in enumerate(docs):
        meta = d.get("metadata", {})
        header = f"""
[Chunk {i+1}]
Database: {meta.get('db', 'N/A')}
Schema: {meta.get('schema', 'N/A')}
Table: {meta.get('table', 'N/A')}
"""
        blocks.append(header + d["text"])
    return "\n\n".join(blocks)

# documents = same docs used for Chroma ingestion
bm25 = BM25Reranker(documents, k=100)

retriever = HybridRetriever(
    chroma_collection=chroma_collection,
    bm25_reranker=bm25,
    k_dense=250,
    k_final=20
)

query = "Which table stores AML risk classification for retail clients?"

docs = retriever.retrieve(query)

for d in docs:
    print(d["metadata"])query = "Which table stores AML risk classification for retail clients?"

docs = retriever.retrieve(query)

for d in docs:
    print(d["metadata"])

context = build_context(docs)

prompt = f"""
You are a banking information system architect.

Answer ONLY using the context.
If not found, say "Information not found".

Context:
{context}

Question:
{query}
"""








keyword = "CLIENT_ACCOUNT"

data = chroma_collection.get(
    include=["documents", "metadatas"]
)

hits = 0

documents = data["documents"]
metadatas = data["metadatas"]

# Handle both flat list and list-of-lists
if documents and isinstance(documents[0], list):
    documents = documents[0]
    metadatas = metadatas[0]

for i, (doc, meta) in enumerate(zip(documents, metadatas)):
    if keyword.lower() in doc.lower():
        print(f"\n--- MATCHING CHUNK {i} ---")
        print("Metadata:", meta)
        print(doc)
        hits += 1

print("\nTotal matching chunks:", hits)











data = chroma_collection.get(
    where={"source": "core_banking_architecture.pdf"},
    include=["documents", "metadatas"]
)

for i, doc in enumerate(data["documents"]):
    print(f"\n--- CHUNK {i} ---\n")
    print(doc)





bm25_reranker = BM25Reranker(
    documents=documents,
    k=50   # how many BM25 docs you want
)


hybrid_retriever = HybridRetriever(
    collection=chroma_collection,
    bm25_reranker=bm25_reranker,
    k_dense=120,   # large recall
    k_final=20     # final docs sent to LLM
)

query = "Which database stores AML risk classification for retail clients?"

retrieved_docs = hybrid_retriever.retrieve(query)



from rank_bm25 import BM25Okapi
import re

class BM25Reranker:
    def __init__(self, documents, k=20):
        self.k = k
        self.documents = documents
        self.tokenized_corpus = [self._tokenize(d["text"]) for d in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)

    def _tokenize(self, text):
        return re.findall(r"[A-Za-z0-9_\.]+", text.lower())

    def rerank(self, query):
        query_tokens = self._tokenize(query)
        scores = self.bm25.get_scores(query_tokens)

        ranked = sorted(
            zip(scores, self.documents),
            key=lambda x: x[0],
            reverse=True
        )
        return [doc for _, doc in ranked[:self.k]]class HybridRetriever:
    def __init__(self, collection, bm25_reranker, k_dense=100, k_final=20):
        self.collection = collection
        self.bm25 = bm25_reranker
        self.k_dense = k_dense
        self.k_final = k_final

    def retrieve(self, query):
        dense = self.collection.query(
            query_texts=[query],
            n_results=self.k_dense,
            include=["documents", "metadatas"]
        )

        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(dense["documents"][0], dense["metadatas"][0])
        ]

        # BM25 rescue
        sparse_docs = self.bm25.rerank(query)

        # Union + dedup
        seen = set()
        merged = []
        for d in dense_docs + sparse_docs:
            h = hash(d["text"])
            if h not in seen:
                seen.add(h)
                merged.append(d)

        return merged[:self.k_final]from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_community.document_compressors.rankllm_rerank import RankLLMRerank
from typing import List

def hybrid_retrieve_langchain(
    collection,           # Your vector DB collection (Chroma/FAISS)
    llm_client,           # Azure/OpenAI client (LLM)
    query: str,
    k_dense: int = 50,    # Number of top docs from embeddings
    k_final: int = 20     # Number of final docs after reranking
) -> List[dict]:
    """
    Hybrid Retrieval using:
    Step 1: Dense retrieval from vector DB
    Step 2: RankLLM reranking (via LangChain)
    Returns top documents (text + metadata)
    """

    # --------------------------
    # Step 1: Dense retrieval
    # --------------------------
    dense_results = collection.query(
        query_texts=[query],
        n_results=k_dense,
        include=["documents", "metadatas"]
    )

    candidates = []
    for text, meta in zip(dense_results["documents"][0], dense_results["metadatas"][0]):
        candidates.append({"text": text, "metadata": meta})

    if not candidates:
        return []

    # --------------------------
    # Step 2: RankLLM reranker
    # --------------------------
    reranker = RankLLMRerank.from_llm(
        llm=llm_client,  # your Azure/OpenAI client
        top_n=k_final,
        verbose=True
    )

    # Optional: wrap with ContextualCompressionRetriever if you want
    # to compress context before passing to reranker (useful for long chunks)
    retriever = ContextualCompressionRetriever(
        base_retriever=candidates,  # here just the list of candidates
        compressor=reranker
    )

    # Run reranking
    reranked_docs = reranker.rerank(query=query, documents=candidates)

    return reranked_docs[:k_final]






query = "How to configure user permissions in the system"

top_docs = hybrid_retrieve_langchain(
    collection=chroma_collection,
    llm_client=azure_openai_client,
    query=query,
    k_dense=80,
    k_final=20
)

for doc in top_docs:
    print(doc["text"][:300], "...\n")







import streamlit as st
import chromadb
from chromadb.utils import embedding_functions
import fitz  # PyMuPDF
from docx import Document
import pandas as pd
from datetime import datetime
import json
import os
from typing import List, Dict, Tuple, Optional
import re
from collections import defaultdict
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
import hashlib

# Azure OpenAI imports
from openai import AzureOpenAI
import httpx

# ==============================================================================
# CONFIGURATION
# ==============================================================================

class Config:
    """Centralized configuration management"""
    
    # Azure OpenAI Configuration
    OIDC_CLIENT_ID = os.getenv("OIDC_CLIENT_ID", "your_client_id")
    OIDC_CLIENT_SECRET = os.getenv("OIDC_CLIENT_SECRET", "your_client_secret")
    OIDC_ENDPOINT = os.getenv("OIDC_ENDPOINT", "https://alfactory.api.staging.schonet/auth/oauth2/v2/token")
    OIDC_SCOPE = os.getenv("OIDC_SCOPE", "genai-model")
    
    AZURE_ENDPOINT = os.getenv("AZURE_ENDPOINT", "https://alfactory.api.staging.schonet/genai-model/v1")
    AZURE_API_VERSION = os.getenv("AZURE_API_VERSION", "2024-05-01-preview")
    AZURE_API_KEY = os.getenv("AZURE_API_KEY", "FAKE_KEY")
    AZURE_MODEL_DEPLOYMENT = os.getenv("AZURE_MODEL_DEPLOYMENT", "gpt-4o")
    
    # Embedding Configuration
    EMBEDDING_MODEL_PATH = "/domino/datasets/local/test-prd-base"
    EMBEDDING_DIMENSION = 1024
    
    # ChromaDB Configuration
    CHROMA_PERSIST_PATH = "/domino/datasets/local/chroma_persistent_db"
    COLLECTION_NAME = "banking_documents"
    
    # Chunking Configuration for Banking Documents
    CHUNK_SIZES = {
        'header': 300,      # For sections with headers
        'table': 500,       # For tables and structured data
        'paragraph': 400,   # For regular paragraphs
        'list': 350,        # For lists and enumerations
    }
    CHUNK_OVERLAP = 100
    
    # BM25 Configuration
    BM25_INDEX_PATH = "/domino/datasets/local/bm25_index.pkl"
    BM25_K1 = 1.5
    BM25_B = 0.75
    
    # Retrieval Configuration
    INITIAL_RETRIEVAL_K = 20  # Retrieve more for reranking
    FINAL_RESULTS_K = 5       # Final results after reranking
    HYBRID_ALPHA = 0.5        # Balance between semantic (0) and keyword (1)
    
    # Batch Processing Configuration
    BATCH_SIZE = 100
    MAX_WORKERS = 4

# ==============================================================================
# DOCUMENT PROCESSING & CHUNKING
# ==============================================================================

class BankingDocumentProcessor:
    """Advanced document processor optimized for banking/technical documents"""
    
    def __init__(self):
        self.chunk_patterns = {
            'section_header': re.compile(r'^(Sect\.|Section|Chapitre|Article|¬ß)\s*[\dIVX]+[\.\s]', re.IGNORECASE),
            'subsection': re.compile(r'^[A-Z]{1,3}\.\d+(\.\d+)*\s+', re.MULTILINE),
            'table_marker': re.compile(r'(Description Rubrique|Num\s+donn\.|Val\s+init)', re.IGNORECASE),
            'code_block': re.compile(r'(Code|R√©f√©rence|Type de contrat):\s*\d+', re.IGNORECASE),
            'list_item': re.compile(r'^\s*[-‚Ä¢*]\s+|\d+\.\s+', re.MULTILINE),
        }
    
    def extract_text_from_file(self, uploaded_file) -> Optional[Dict]:
        """Extract text with metadata from uploaded files"""
        filename = uploaded_file.name.lower()
        
        try:
            if filename.endswith('.pdf'):
                return self._extract_from_pdf(uploaded_file)
            elif filename.endswith('.txt'):
                text = uploaded_file.read().decode('utf-8')
                return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
            elif filename.endswith('.docx'):
                return self._extract_from_docx(uploaded_file)
            elif filename.endswith(('.csv', '.xlsx')):
                return self._extract_from_spreadsheet(uploaded_file, filename)
            else:
                return None
        except Exception as e:
            st.error(f"Error extracting from {uploaded_file.name}: {str(e)}")
            return None
    
    def _extract_from_pdf(self, uploaded_file) -> Dict:
        """Extract text from PDF with page-level granularity"""
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        pages = []
        
        for page_num, page in enumerate(doc, start=1):
            text = page.get_text()
            pages.append({
                'page_num': page_num,
                'text': text
            })
        
        full_text = "\n".join([p['text'] for p in pages])
        return {'text': full_text, 'pages': pages}
    
    def _extract_from_docx(self, uploaded_file) -> Dict:
        """Extract text from DOCX"""
        doc = Document(uploaded_file)
        text = "\n".join([p.text for p in doc.paragraphs])
        return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
    
    def _extract_from_spreadsheet(self, uploaded_file, filename: str) -> Dict:
        """Extract text from CSV/Excel"""
        if filename.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)
        
        text = df.to_string(index=False)
        return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
    
    def clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove excessive whitespace
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        
        # Normalize line breaks
        text = text.replace('\r\n', '\n')
        
        # Remove page headers/footers (common patterns)
        text = re.sub(r'Page \d+( of \d+)?', '', text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def smart_chunk_document(self, doc_data: Dict, source_file: str) -> List[Dict]:
        """
        Intelligent chunking that preserves document structure
        Optimized for banking/technical documents
        """
        chunks = []
        pages = doc_data.get('pages', [])
        
        for page_info in pages:
            page_num = page_info['page_num']
            text = self.clean_text(page_info['text'])
            
            # Detect document structure
            if self._is_table_content(text):
                page_chunks = self._chunk_table_content(text, source_file, page_num)
            elif self._has_clear_sections(text):
                page_chunks = self._chunk_by_sections(text, source_file, page_num)
            else:
                page_chunks = self._chunk_semantic(text, source_file, page_num)
            
            chunks.extend(page_chunks)
        
        return chunks
    
    def _is_table_content(self, text: str) -> bool:
        """Detect if content is primarily tabular"""
        return bool(self.chunk_patterns['table_marker'].search(text))
    
    def _has_clear_sections(self, text: str) -> bool:
        """Detect if content has clear section structure"""
        headers = self.chunk_patterns['section_header'].findall(text)
        return len(headers) >= 2
    
    def _chunk_table_content(self, text: str, source: str, page: int) -> List[Dict]:
        """Chunk table content preserving structure"""
        chunks = []
        lines = text.split('\n')
        current_chunk = []
        current_size = 0
        
        for line in lines:
            line_size = len(line)
            
            if current_size + line_size > Config.CHUNK_SIZES['table']:
                if current_chunk:
                    chunks.append(self._create_chunk(
                        '\n'.join(current_chunk), source, page, 'table'
                    ))
                    # Overlap: keep last few lines
                    overlap_lines = current_chunk[-3:] if len(current_chunk) > 3 else current_chunk
                    current_chunk = overlap_lines
                    current_size = sum(len(l) for l in current_chunk)
            
            current_chunk.append(line)
            current_size += line_size
        
        if current_chunk:
            chunks.append(self._create_chunk(
                '\n'.join(current_chunk), source, page, 'table'
            ))
        
        return chunks
    
    def _chunk_by_sections(self, text: str, source: str, page: int) -> List[Dict]:
        """Chunk by document sections"""
        chunks = []
        sections = re.split(r'(\n(?:Sect\.|Section|Chapitre|Article)\s+[\dIVX]+)', text, flags=re.IGNORECASE)
        
        current_section = ""
        
        for i, section in enumerate(sections):
            if i % 2 == 0:  # Content
                current_section += section
            else:  # Header
                if current_section.strip():
                    chunks.append(self._create_chunk(
                        current_section.strip(), source, page, 'section'
                    ))
                current_section = section
        
        if current_section.strip():
            chunks.append(self._create_chunk(
                current_section.strip(), source, page, 'section'
            ))
        
        return chunks
    
    def _chunk_semantic(self, text: str, source: str, page: int) -> List[Dict]:
        """Semantic chunking with overlap for regular content"""
        chunks = []
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            if current_size + sentence_size > Config.CHUNK_SIZES['paragraph']:
                if current_chunk:
                    chunks.append(self._create_chunk(
                        ' '.join(current_chunk), source, page, 'paragraph'
                    ))
                    # Overlap: keep last 2 sentences
                    overlap = current_chunk[-2:] if len(current_chunk) > 2 else current_chunk
                    current_chunk = overlap
                    current_size = sum(len(s) for s in current_chunk)
            
            current_chunk.append(sentence)
            current_size += sentence_size
        
        if current_chunk:
            chunks.append(self._create_chunk(
                ' '.join(current_chunk), source, page, 'paragraph'
            ))
        
        return chunks
    
    def _create_chunk(self, text: str, source: str, page: int, chunk_type: str) -> Dict:
        """Create a standardized chunk object"""
        chunk_id = hashlib.md5(f"{source}_{page}_{text[:50]}".encode()).hexdigest()
        
        return {
            'id': chunk_id,
            'text': text,
            'metadata': {
                'source_file': source,
                'page': page,
                'chunk_type': chunk_type,
                'char_count': len(text),
                'timestamp': str(datetime.now())
            }
        }

# ==============================================================================
# BM25 IMPLEMENTATION WITH PERSISTENT INDEX
# ==============================================================================

class PersistentBM25:
    """BM25 implementation with disk persistence for large-scale retrieval"""
    
    def __init__(self, k1: float = Config.BM25_K1, b: float = Config.BM25_B):
        self.k1 = k1
        self.b = b
        self.corpus_size = 0
        self.avgdl = 0
        self.doc_freqs = defaultdict(int)
        self.idf = {}
        self.doc_len = []
        self.doc_ids = []
        self.tokenized_corpus = []
        
    def tokenize(self, text: str) -> List[str]:
        """Tokenize with banking-specific preservation"""
        # Preserve codes and references
        text = re.sub(r'([A-Z]{2,}\d+)', r' \1 ', text)
        
        # Remove punctuation but keep hyphens in codes
        text = re.sub(r'[^\w\s-]', ' ', text.lower())
        
        # Tokenize
        tokens = text.split()
        
        # Filter stopwords (French banking context)
        stopwords = {'le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'ou', 'dans', 'pour', 'sur', 'avec'}
        tokens = [t for t in tokens if t and t not in stopwords]
        
        return tokens
    
    def fit(self, documents: List[Dict]):
        """Build BM25 index from documents"""
        self.corpus_size = len(documents)
        self.doc_ids = [doc['id'] for doc in documents]
        self.tokenized_corpus = []
        
        # Tokenize all documents
        for doc in documents:
            tokens = self.tokenize(doc['text'])
            self.tokenized_corpus.append(tokens)
            self.doc_len.append(len(tokens))
            
            # Count document frequencies
            unique_tokens = set(tokens)
            for token in unique_tokens:
                self.doc_freqs[token] += 1
        
        # Calculate average document length
        self.avgdl = sum(self.doc_len) / self.corpus_size if self.corpus_size > 0 else 0
        
        # Calculate IDF values
        for token, freq in self.doc_freqs.items():
            self.idf[token] = np.log((self.corpus_size - freq + 0.5) / (freq + 0.5) + 1)
    
    def get_scores(self, query: str) -> Dict[str, float]:
        """Calculate BM25 scores for a query"""
        query_tokens = self.tokenize(query)
        scores = {}
        
        for idx, (doc_id, doc_tokens, doc_length) in enumerate(
            zip(self.doc_ids, self.tokenized_corpus, self.doc_len)
        ):
            score = 0
            for token in query_tokens:
                if token not in self.idf:
                    continue
                
                # Calculate term frequency in document
                tf = doc_tokens.count(token)
                
                # BM25 formula
                numerator = tf * (self.k1 + 1)
                denominator = tf + self.k1 * (1 - self.b + self.b * (doc_length / self.avgdl))
                score += self.idf[token] * (numerator / denominator)
            
            scores[doc_id] = score
        
        return scores
    
    def save(self, filepath: str):
        """Save BM25 index to disk"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'k1': self.k1,
                'b': self.b,
                'corpus_size': self.corpus_size,
                'avgdl': self.avgdl,
                'doc_freqs': dict(self.doc_freqs),
                'idf': self.idf,
                'doc_len': self.doc_len,
                'doc_ids': self.doc_ids,
                'tokenized_corpus': self.tokenized_corpus
            }, f)
    
    @classmethod
    def load(cls, filepath: str) -> 'PersistentBM25':
        """Load BM25 index from disk"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        
        bm25 = cls(k1=data['k1'], b=data['b'])
        bm25.corpus_size = data['corpus_size']
        bm25.avgdl = data['avgdl']
        bm25.doc_freqs = defaultdict(int, data['doc_freqs'])
        bm25.idf = data['idf']
        bm25.doc_len = data['doc_len']
        bm25.doc_ids = data['doc_ids']
        bm25.tokenized_corpus = data['tokenized_corpus']
        
        return bm25

# ==============================================================================
# QUERY EXPANSION FOR BANKING TERMINOLOGY
# ==============================================================================

class BankingQueryExpander:
    """Query expansion specialized for banking/financial terminology"""
    
    def __init__(self):
        # Banking-specific synonym mappings
        self.synonyms = {
            'compte': ['account', 'compte support', 'compte client'],
            'contrat': ['contract', 'agreement', 'convention'],
            'd√©p√¥t': ['deposit', 'versement', 'apport'],
            'garantie': ['guarantee', 'caution', 'warranty'],
            '√©ch√©ance': ['maturity', 'deadline', 'due date', 'expiration'],
            'nantissement': ['pledge', 'collateral', 'guarantee'],
            'saisie': ['entry', 'input', 'capture'],
            'grille': ['grid', 'screen', 'form'],
            'transaction': ['operation', 'mouvement', 'transfer'],
            'client': ['customer', 'account holder', 'beneficiary'],
            'b√©n√©ficiaire': ['beneficiary', 'recipient', 'payee'],
            'taux': ['rate', 'interest rate', 'percentage'],
            'plafond': ['ceiling', 'limit', 'cap', 'maximum'],
            'retrait': ['withdrawal', 'extraction'],
            'virement': ['transfer', 'wire', 'payment'],
        }
        
        # Common banking abbreviations
        self.abbreviations = {
            'DAT': 'D√©p√¥t √† Terme',
            'CNT': 'Contrat',
            'GDI': 'Gestion De Interface',
            'BDC': 'Bon De Caisse',
            'SGE': 'Code si√®ge',
            'DEV': 'Code devise',
        }
    
    def expand_query(self, query: str) -> List[str]:
        """Expand query with synonyms and banking terms"""
        expanded_queries = [query]
        
        query_lower = query.lower()
        
        # Add synonym expansions
        for term, synonyms in self.synonyms.items():
            if term in query_lower:
                for synonym in synonyms:
                    expanded = query_lower.replace(term, synonym)
                    if expanded != query_lower:
                        expanded_queries.append(expanded)
        
        # Expand abbreviations
        for abbr, full_form in self.abbreviations.items():
            if abbr in query:
                expanded = query.replace(abbr, full_form)
                expanded_queries.append(expanded)
            elif full_form.lower() in query_lower:
                expanded = query_lower.replace(full_form.lower(), abbr)
                expanded_queries.append(expanded)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_queries = []
        for q in expanded_queries:
            if q not in seen:
                seen.add(q)
                unique_queries.append(q)
        
        return unique_queries[:5]  # Limit to top 5 expansions

# ==============================================================================
# HYBRID RETRIEVAL SYSTEM
# ==============================================================================

class HybridRetriever:
    """Hybrid retrieval combining semantic search (ChromaDB) and keyword search (BM25)"""
    
    def __init__(self, chroma_collection, bm25_index: PersistentBM25, alpha: float = Config.HYBRID_ALPHA):
        self.chroma_collection = chroma_collection
        self.bm25_index = bm25_index
        self.alpha = alpha  # Weight between semantic (0) and keyword (1)
        self.query_expander = BankingQueryExpander()
    
    def retrieve(self, query: str, k: int = Config.INITIAL_RETRIEVAL_K) -> List[Dict]:
        """
        Hybrid retrieval with query expansion
        Returns top k documents based on weighted combination of semantic + keyword scores
        """
        # Expand query for better recall
        expanded_queries = self.query_expander.expand_query(query)
        
        all_results = {}
        
        for exp_query in expanded_queries:
            # 1. Semantic search via ChromaDB
            semantic_results = self.chroma_collection.query(
                query_texts=[exp_query],
                n_results=k,
                include=["documents", "metadatas", "distances"]
            )
            
            # Normalize semantic scores (distances -> similarities)
            semantic_scores = {}
            if semantic_results['ids'] and semantic_results['ids'][0]:
                max_distance = max(semantic_results['distances'][0]) if semantic_results['distances'][0] else 1
                for idx, doc_id in enumerate(semantic_results['ids'][0]):
                    distance = semantic_results['distances'][0][idx]
                    similarity = 1 - (distance / max_distance) if max_distance > 0 else 1
                    semantic_scores[doc_id] = similarity
            
            # 2. Keyword search via BM25
            bm25_scores = self.bm25_index.get_scores(exp_query)
            
            # Normalize BM25 scores
            max_bm25 = max(bm25_scores.values()) if bm25_scores else 1
            normalized_bm25 = {k: v / max_bm25 for k, v in bm25_scores.items()} if max_bm25 > 0 else bm25_scores
            
            # 3. Combine scores with weighted sum
            all_doc_ids = set(semantic_scores.keys()) | set(normalized_bm25.keys())
            
            for doc_id in all_doc_ids:
                sem_score = semantic_scores.get(doc_id, 0)
                bm25_score = normalized_bm25.get(doc_id, 0)
                
                # Hybrid score
                hybrid_score = (1 - self.alpha) * sem_score + self.alpha * bm25_score
                
                # Accumulate scores across expanded queries
                if doc_id in all_results:
                    all_results[doc_id]['score'] = max(all_results[doc_id]['score'], hybrid_score)
                else:
                    # Get document metadata
                    try:
                        doc_data = self.chroma_collection.get(
                            ids=[doc_id],
                            include=["documents", "metadatas"]
                        )
                        if doc_data['ids']:
                            all_results[doc_id] = {
                                'id': doc_id,
                                'text': doc_data['documents'][0],
                                'metadata': doc_data['metadatas'][0],
                                'score': hybrid_score,
                                'semantic_score': sem_score,
                                'bm25_score': bm25_score
                            }
                    except:
                        pass
        
        # Sort by score and return top k
        ranked_results = sorted(all_results.values(), key=lambda x: x['score'], reverse=True)
        return ranked_results[:k]

# ==============================================================================
# BATCH DOCUMENT INGESTION
# ==============================================================================

class BatchDocumentIngestion:
    """Optimized batch processing for large document collections"""
    
    def __init__(self, chroma_collection, processor: BankingDocumentProcessor):
        self.collection = chroma_collection
        self.processor = processor
        self.bm25_documents = []
    
    def ingest_documents(self, uploaded_files: List, progress_callback=None) -> Dict:
        """
        Batch ingest documents with progress tracking
        Returns statistics about the ingestion process
        """
        stats = {
            'total_files': len(uploaded_files),
            'processed_files': 0,
            'total_chunks': 0,
            'failed_files': [],
            'processing_time': 0
        }
        
        start_time = datetime.now()
        
        # Get existing documents to avoid duplicates
        existing_sources = self._get_existing_sources()
        
        batch_chunks = []
        
        for file_idx, uploaded_file in enumerate(uploaded_files):
            try:
                filename = uploaded_file.name
                
                # Skip if already indexed
                if filename in existing_sources:
                    if progress_callback:
                        progress_callback(file_idx + 1, len(uploaded_files), f"Skipped (already indexed): {filename}")
                    continue
                
                if progress_callback:
                    progress_callback(file_idx + 1, len(uploaded_files), f"Processing: {filename}")
                
                # Extract and chunk document
                doc_data = self.processor.extract_text_from_file(uploaded_file)
                
                if doc_data is None:
                    stats['failed_files'].append(filename)
                    continue
                
                chunks = self.processor.smart_chunk_document(doc_data, filename)
                
                # Add to batch
                batch_chunks.extend(chunks)
                self.bm25_documents.extend(chunks)
                
                # Process batch if it reaches batch size
                if len(batch_chunks) >= Config.BATCH_SIZE:
                    self._process_batch(batch_chunks)
                    stats['total_chunks'] += len(batch_chunks)
                    batch_chunks = []
                
                stats['processed_files'] += 1
                
            except Exception as e:
                stats['failed_files'].append(f"{uploaded_file.name}: {str(e)}")
        
        # Process remaining chunks
        if batch_chunks:
            self._process_batch(batch_chunks)
            stats['total_chunks'] += len(batch_chunks)
        
        stats['processing_time'] = (datetime.now() - start_time).total_seconds()
        
        return stats
    
    def _process_batch(self, chunks: List[Dict]):
        """Process a batch of chunks into ChromaDB"""
        if not chunks:
            return
        
        ids = [chunk['id'] for chunk in chunks]
        documents = [chunk['text'] for chunk in chunks]
        metadatas = [chunk['metadata'] for chunk in chunks]
        
        self.collection.add(
            ids=ids,
            documents=documents,
            metadatas=metadatas
        )
    
    def _get_existing_sources(self) -> set:
        """Get list of already indexed source files"""
        try:
            all_data = self.collection.get(include=["metadatas"])
            if all_data and all_data['metadatas']:
                return set(meta.get('source_file', '') for meta in all_data['metadatas'])
        except:
            pass
        return set()
    
    def build_bm25_index(self) -> PersistentBM25:
        """Build and save BM25 index from ingested documents"""
        bm25 = PersistentBM25()
        bm25.fit(self.bm25_documents)
        bm25.save(Config.BM25_INDEX_PATH)
        return bm25

# ==============================================================================
# RAG GENERATION WITH IMPROVED CONTEXT
# ==============================================================================

class BankingRAGGenerator:
    """Enhanced RAG generation for banking documentation"""
    
    def __init__(self, azure_client):
        self.client = azure_client
    
    def generate_response(self, query: str, context_docs: List[Dict]) -> Dict:
        """Generate response with improved context assembly"""
        
        # Assemble context with source citations
        context_parts = []
        sources = []
        
        for idx, doc in enumerate(context_docs, 1):
            metadata = doc.get('metadata', {})
            source_file = metadata.get('source_file', 'Unknown')
            page = metadata.get('page', 'N/A')
            
            context_parts.append(
                f"[Document {idx} - {source_file}, Page {page}]\n{doc['text']}\n"
            )
            
            if source_file not in sources:
                sources.append(source_file)
        
        context = "\n".join(context_parts)
        
        # Enhanced prompt for banking documentation
        prompt = f"""Vous √™tes un assistant RAG sp√©cialis√© dans les syst√®mes bancaires, l'architecture de donn√©es, et les produits financiers.

Votre r√¥le est de r√©pondre et d'expliquer clairement les informations issues des documents, en les rendant compr√©hensibles pour tout type d'utilisateur, du plus g√©n√©ral au plus technique.

[Contexte]
{context}

[Requ√™te]
{query}

[INSTRUCTIONS G√âN√âRALES]

1. Utilisez exclusivement les informations pr√©sentes dans le contexte ci-dessus.

2. Fournissez une r√©ponse claire, structur√©e et explicite, adapt√©e √† la compr√©hension de tout utilisateur.
   - Si le sujet est technique, expliquez les notions en termes simples.
   - Si la question est g√©n√©rale, donnez une r√©ponse compl√®te mais concise.
   - Si la question est complexe, d√©taillez le raisonnement et le fonctionnement.

3. Chaque fois que vous mentionnez une donn√©e, un m√©canisme ou un fait:
   - Citez imm√©diatement la source sous le format: [Source: nom_du_fichier.pdf, Page <num√©ro>]

4. Si l'information n'existe pas dans les documents, indiquez-le clairement.

5. Structure attendue:
   a. R√©ponse expliqu√©e: D√©taillez le contenu et son interpr√©tation.
   b. Synth√®se (si utile): R√©sumez la logique ou le fonctionnement global.
   c. Sources: Liste compl√®te des documents utilis√©s.

6. Objectif:
   Rendre la r√©ponse √† la fois informative, explicative et v√©rifiable,
   qu'il s'agisse d'un utilisateur curieux ou d'un expert technique.

[Exemple de format]
Les proc√©dures de v√©rification d'identit√©... [Source: procedure_kyc.pdf, Page 12]. 
Selon le chapitre 3... [Source: reglement_financier.pdf, Page 45].

Sources utilis√©es:
- procedure_kyc.pdf (Pages 12, 15)
- reglement_financier.pdf (Page 45)
"""
        
        try:
            completion = self.client.chat.completions.create(
                model=Config.AZURE_MODEL_DEPLOYMENT,
                messages=[
                    {"role": "system", "content": "You are an expert assistant for banking system documentation based on RAG (Retrieval-Augmented Generation)."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            answer = completion.choices[0].message.content
            
            return {
                'answer': answer,
                'sources': sources,
                'context_docs': context_docs,
                'num_tokens': completion.usage.total_tokens if hasattr(completion, 'usage') else None
            }
            
        except Exception as e:
            return {
                'answer': f"Error generating response: {str(e)}",
                'sources': [],
                'context_docs': [],
                'num_tokens': None
            }

# ==============================================================================
# STREAMLIT APPLICATION
# ==============================================================================

def initialize_session_state():
    """Initialize Streamlit session state"""
    if 'chroma_client' not in st.session_state:
        st.session_state.chroma_client = chromadb.PersistentClient(path=Config.CHROMA_PERSIST_PATH)
    
    if 'collection' not in st.session_state:
        # Load embedding model
        embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL_PATH)
        
        # Create embedding function
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=Config.EMBEDDING_MODEL_PATH
        )
        
        st.session_state.collection = st.session_state.chroma_client.get_or_create_collection(
            name=Config.COLLECTION_NAME,
            embedding_function=embedding_function,
            metadata={"hnsw:space": "cosine"}
        )
    
    if 'bm25_index' not in st.session_state:
        # Load or create BM25 index
        if os.path.exists(Config.BM25_INDEX_PATH):
            st.session_state.bm25_index = PersistentBM25.load(Config.BM25_INDEX_PATH)
        else:
            st.session_state.bm25_index = PersistentBM25()
    
    if 'chat_sessions' not in st.session_state:
        st.session_state.chat_sessions = {}
    
    if 'current_chat' not in st.session_state:
        st.session_state.current_chat = str(datetime.now())
        st.session_state.chat_sessions[st.session_state.current_chat] = {
            "title": "New Chat",
            "messages": []
        }
    
    if 'processor' not in st.session_state:
        st.session_state.processor = BankingDocumentProcessor()
    
    if 'azure_client' not in st.session_state:
        # Initialize Azure OpenAI client
        http_client = httpx.Client(verify=False)
        st.session_state.azure_client = AzureOpenAI(
            api_version=Config.AZURE_API_VERSION,
            azure_endpoint=Config.AZURE_ENDPOINT,
            api_key=Config.AZURE_API_KEY,
            http_client=http_client
        )
    
    if 'rag_generator' not in st.session_state:
        st.session_state.rag_generator = BankingRAGGenerator(st.session_state.azure_client)

def main():
    st.set_page_config(page_title="BNP Banking RAG Chatbot", layout="wide")
    
    initialize_session_state()
    
    st.title("üè¶ BNP Banking Documentation Chatbot")
    st.caption("Optimized RAG system with hybrid search and batch processing")
    
    # Sidebar for document management
    with st.sidebar:
        st.header("üìÅ Document Management")
        
        # Get collection stats
        try:
            collection_count = st.session_state.collection.count()
            st.metric("Indexed Documents", collection_count)
        except:
            collection_count = 0
            st.metric("Indexed Documents", "0")
        
        # Batch upload
        st.subheader("Upload Documents")
        uploaded_files = st.file_uploader(
            "Upload multiple documents",
            type=["pdf", "txt", "docx", "csv", "xlsx"],
            accept_multiple_files=True
        )
        
        if uploaded_files and st.button("üì§ Process All Files"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(current, total, message):
                progress = current / total
                progress_bar.progress(progress)
                status_text.text(message)
            
            with st.spinner("Processing documents..."):
                batch_ingestion = BatchDocumentIngestion(
                    st.session_state.collection,
                    st.session_state.processor
                )
                
                stats = batch_ingestion.ingest_documents(
                    uploaded_files,
                    progress_callback=update_progress
                )
                
                # Rebuild BM25 index
                status_text.text("Building BM25 index...")
                st.session_state.bm25_index = batch_ingestion.build_bm25_index()
                
                # Display results
                st.success(f"""
                ‚úÖ Processing complete!
                - Files processed: {stats['processed_files']}/{stats['total_files']}
                - Total chunks: {stats['total_chunks']}
                - Time: {stats['processing_time']:.2f}s
                """)
                
                if stats['failed_files']:
                    st.error(f"Failed files: {', '.join(stats['failed_files'])}")
        
        st.divider()
        
        # Chat management
        st.subheader("üí¨ Chat Sessions")
        if st.button("‚ûï New Chat"):
            new_chat_id = str(datetime.now())
            st.session_state.current_chat = new_chat_id
            st.session_state.chat_sessions[new_chat_id] = {
                "title": "New Chat",
                "messages": []
            }
            st.rerun()
        
        # List existing chats
        for chat_id, chat_data in list(st.session_state.chat_sessions.items()):
            if st.button(f"üìù {chat_data['title'][:30]}", key=f"chat_{chat_id}"):
                st.session_state.current_chat = chat_id
                st.rerun()
    
    # Main chat interface
    current_chat = st.session_state.chat_sessions.get(st.session_state.current_chat, {
        "title": "New Chat",
        "messages": []
    })
    
    # Display chat messages
    for message in current_chat['messages']:
        with st.chat_message(message['role']):
            st.markdown(message['content'])
            
            # Show sources if available
            if message['role'] == 'assistant' and 'sources' in message:
                with st.expander("üìö Sources"):
                    for source in message['sources']:
                        st.text(f"‚Ä¢ {source}")
    
    # Chat input
    if prompt := st.chat_input("Ask about banking documentation..."):
        # Add user message
        current_chat['messages'].append({
            "role": "user",
            "content": prompt,
            "timestamp": str(datetime.now())
        })
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("üîç Searching documentation..."):
                # Create hybrid retriever
                retriever = HybridRetriever(
                    st.session_state.collection,
                    st.session_state.bm25_index
                )
                
                # Retrieve relevant documents
                retrieved_docs = retriever.retrieve(prompt, k=Config.FINAL_RESULTS_K)
                
                # Generate response
                response = st.session_state.rag_generator.generate_response(
                    prompt,
                    retrieved_docs
                )
                
                # Display answer
                st.markdown(response['answer'])
                
                # Show sources
                if response['sources']:
                    with st.expander("üìö Sources"):
                        for source in response['sources']:
                            st.text(f"‚Ä¢ {source}")
                
                # Add to chat history
                current_chat['messages'].append({
                    "role": "assistant",
                    "content": response['answer'],
                    "sources": response['sources'],
                    "timestamp": str(datetime.now())
                })
                
                # Update chat title if first message
                if current_chat['title'] == "New Chat" and len(current_chat['messages']) > 0:
                    current_chat['title'] = prompt[:50] + "..."

if __name__ == "__main__":
    main()







Nouvelle slide ‚Äì Public cible
√Ä qui s‚Äôadresse la solution RAG BNP ED ?
RAG BNP ED a √©t√© d√©velopp√©e pour accompagner diff√©rents profils impliqu√©s dans l‚Äôexploitation et la transformation du syst√®me d‚Äôinformation :
üë• √âquipes m√©tiers
Acc√®s rapide et fiable √† la documentation ATLAS
Recherche simplifi√©e sans connaissance technique
R√©ponses claires, sourc√©es et contextualis√©es
üèóÔ∏è √âquipes IT / DSI
Compr√©hension de l‚Äôarchitecture de la documentation ATLAS
Acc√®s structur√© aux r√©f√©rentiels et aux d√©pendances documentaires
Support √† l‚Äôanalyse et √† la maintenance du SI
üîÑ √âquipes Transformation du Syst√®me d‚ÄôInformation
Accompagnement des projets de migration et d‚Äô√©volution du SI
Vision transverse de la documentation existante
R√©duction du temps de recherche et d‚Äôanalyse documentaire
üí° Variante plus synth√©tique (si tu veux une slide tr√®s l√©g√®re)
RAG BNP ED s‚Äôadresse aux √©quipes m√©tiers, √† la DSI et aux acteurs de la transformation du syst√®me d‚Äôinformation, en facilitant l‚Äôacc√®s, la compr√©hension et l‚Äôexploitation de la documentation ATLAS.














RAG BNP ED est un chatbot intelligent bas√© sur la m√©thode RAG (Retrieval Augmented Generation) et sur une base de connaissances centralis√©e contenant la documentation ATLAS.
Il permet une recherche conversationnelle rapide et fiable, avec des r√©ponses syst√©matiquement sourc√©es √† partir des documents officiels.
La solution √©volue automatiquement avec les nouvelles documentations, garantissant des r√©ponses toujours √† jour.






RAG BNP ED est un chatbot intelligent bas√© sur une base de connaissances centralis√©e (documentation ATLAS).
Il permet une recherche conversationnelle rapide et fiable, avec des r√©ponses syst√©matiquement sourc√©es.
La solution √©volue automatiquement avec les nouvelles documentations, garantissant des r√©ponses toujours √† jour.








La documentation ATLAS est volumineuse et critique pour les √©quipes m√©tiers
L‚Äôacc√®s √† l‚Äôinformation repose aujourd‚Äôhui sur une recherche manuelle chronophage
Objectif : faciliter l‚Äôacc√®s rapide, fiable et sourc√© √† l‚Äôinformation interne via un outil intelligent




Bonjour,

Pourriez-vous s‚Äôil vous pla√Æt proc√©der √† la r√©initialisation de mon mot de passe ? Je vous remercie par avance.

Cordialement,
Nihad Senhadji


Bonjour √† tous,

Suite √† notre pr√©c√©dent √©change, je vous informe que nous avons pu identifier et renseigner les dates d‚Äôimpay√©s en nous basant sur l‚Äôextraction transmise par Chaima.

Toutefois, 11 cas restent pour lesquels les dates d‚Äôimpay√©s n‚Äôont pas pu √™tre renseign√©es, malgr√© l‚Äôutilisation de cette extraction. Ces cas demeurent donc identifi√©s comme anomalies.

Je reste √† votre disposition pour toute pr√©cision compl√©mentaire ou pour √©changer sur ces cas si besoin.

Cordialement,
Nihad Senhadji
GA-15








Bonjour √† tous,

Veuillez trouver ci-joint la nouvelle version du fichier √âtat Central des Engagements arr√™t√© au 31/12/2025.

Lors du traitement, nous avons constat√© que certaines dates d‚Äôimpay√©s n‚Äôont pas √©t√© retrouv√©es dans les donn√©es d‚Äôextraction disponibles.
Pour cette raison, une nouvelle feuille a √©t√© ajout√©e au fichier, intitul√©e :

¬´ √âtat des engagements ‚Äì Anomalies dates d‚Äôimpay√©s ¬ª

Cette feuille regroupe l‚Äôensemble des engagements pour lesquels les dates d‚Äôimpay√©s n‚Äôont pas pu √™tre identifi√©es √† partir de l‚Äôextraction.

Le fichier contient d√©sormais les feuilles suivantes :

1. √âtat Central des Engagements
2. √âtat des engagements ‚Äì Anomalies dates d‚Äôimpay√©s
3. √âtat d√©taill√© par chapitre
4. Rapprochement des cr√©ances ‚Äì Bilan
5. Rapprochement des cr√©ances hors bilan
6. Rapprochement CR / AR / CB
7. Reporting social BG
8. √âtat d√©taill√© par ratios prudentiels
9. Risques ‚Äì Rapprochement RP
10. Risques ‚Äì Grands Risques
11. Risques ‚Äì Grands Risques par entit√©
12. Risques ‚Äì Annexe 9000
13. Rapprochement reporting provisions
14. Rapprochement affectation des garanties
15. √âtat des garanties

Je reste √† votre disposition pour toute information compl√©mentaire ou clarification si besoin.

Cordialement,
Nihad Senhadji
GA-15







Bien s√ªr ! Voici une version r√©√©crite, fluide, professionnelle et claire de ton email :


---

Objet : M√©thodologie ‚Äì D√©tection de communaut√©s de fraude (AI Sentinilia)

Bonjour Ryma,

Dans le cadre du projet AI Sentinilia d√©di√© √† la d√©tection de communaut√©s de fraude, la m√©thodologie utilis√©e repose sur les crit√®res suivants :

Les liens entre clients sont identifi√©s √† partir de co√Øncidences lorsqu‚Äôils effectuent des retraits :

√† la m√™me date ;

au m√™me GAB (ATM) ;

pour un montant sup√©rieur ou √©gal √† 500 euros ;

avec une fr√©quence d‚Äôau moins 5 retraits entre deux clients ou plus.



Lorsque ces conditions sont remplies, les clients sont consid√©r√©s comme li√©s, car leurs comportements de retrait pr√©sentent des similitudes r√©p√©t√©es et inhabituelles.

Afin de d√©tecter les communaut√©s de fraude, nous utilisons un algorithme appel√© Louvain, qui permet de regrouper automatiquement les clients ayant des comportements similaires. Chaque communaut√© ainsi identifi√©e repr√©sente un groupe potentiel de fraude, bas√© sur des comportements de retraits coordonn√©s.

Cette solution n‚Äôa pas √©t√© modifi√©e depuis sa mise en place, et le suivi √©tant trimestriel, nous enverrons les prochains r√©sultats en f√©vrier 2026.

Je reste √† votre disposition pour toute information compl√©mentaire ou clarification si n√©cessaire.

Cordialement,
Nihad Senhadji


---

Si tu veux, je peux aussi te faire une version encore plus courte et simple, parfaite pour des non-techniques, qui tient en 5-6 phrases.
Veux‚Äëtu que je fasse √ßa‚ÄØ?











Comme le suivi est trimestriel, nous enverrons les prochains r√©sultats en f√©vrier 2025.


Cet algorithme n‚Äôa pas √©t√© modifi√© depuis sa mise en place


Afin de d√©tecter les communaut√©s de fraude, nous utilisons un algorithme appel√© Louvain, qui permet de regrouper automatiquement les clients ayant des comportements similaires.


Bonjour,

Dans le cadre du projet AI Sentinilia d√©di√© √† la d√©tection de communaut√©s de fraude, la m√©thodologie utilis√©e repose sur les √©tapes suivantes :

- Les liens entre clients sont d‚Äôabord identifi√©s √† partir des crit√®res suivants :

   - la localisation du GAB (ATM) o√π les retraits ont √©t√© effectu√©s ;
   - la fr√©quence des retraits ;
   - des co√Øncidences entre plusieurs clients lorsque les retraits ont eu lieu :
      - √† la m√™me date,
      - au m√™me GAB,
      - avec un montant sup√©rieur ou √©gal √† 500 euros,
      - et ce au moins 5 fois entre deux clients ou plus.

- Lorsque ces conditions sont r√©unies, les clients sont consid√©r√©s comme li√©s, car leurs comportements de retrait pr√©sentent des similitudes r√©p√©t√©es et inhabituelles.

- La m√©thode de Louvain est ensuite utilis√©e pour regrouper automatiquement les clients li√©s :

   - elle permet de constituer des communaut√©s de clients fortement connect√©s ;
   - chaque communaut√© repr√©sente un groupe potentiel de fraude, bas√© sur des comportements de retraits coordonn√©s.

Je reste √† votre disposition pour toute information compl√©mentaire ou clarification si n√©cessaire.

Cordialement,

Nihad Senhadji



















Bonjour,

Je vous informe que j‚Äôai annul√© la version du fichier d√©j√† envoy√©e pr√©c√©demment.
Nous sommes actuellement en train de finaliser la nouvelle version, et je vous l‚Äôenverrai d√®s que ce sera termin√©.

Merci pour votre compr√©hension.

Cordialement,
Nihad Senhadji

Bonjour,

Veuillez trouver ci-joint le fichier demand√©, comme convenu.

Nous vous enverrons la version finale, contenant l‚Äôensemble des feuilles, dans la soir√©e.

Cordialement,
Nihad Senhadji


Les colonnes du fichier fournissent toutes les informations n√©cessaires ‚Äî identit√©, dates, lieux et r√©f√©rences clients ‚Äî afin de faciliter et structurer l‚Äôanalyse des √©quipes sur ces cas sensibles.


La deuxi√®me feuille, avec un score de similarit√© inf√©rieur √† 95 %, correspond aux cas o√π le mod√®le d‚ÄôIA a permis de r√©duire une grande partie de la recherche manuelle.
Dans cette feuille, tous les noms qui ne figurent pas dans notre base clients Atlas sont automatiquement √©cart√©s, ce qui √©limine les faux positifs et all√®ge consid√©rablement le travail des √©quipes op√©rationnelles.
üîπ La troisi√®me feuille, avec un score de similarit√© sup√©rieur √† 95 %, regroupe les noms n√©cessitant une analyse approfondie par l‚Äô√©quipe op√©rationnelle, car ils pr√©sentent une forte probabilit√© de correspondance.










üé¨ Script vid√©o ‚Äì Pr√©sentation de Decad AI Solution

> Bonjour,

Dans cette vid√©o, je vais vous pr√©senter Decad AI Solution, accessible via un lien de web application Domino.

On commence par l‚Äôinsertion du fichier DECAD, envoy√© par la Banque d‚ÄôAlg√©rie, directement dans l‚Äôapplication.

Ensuite, un simple clic permet de lancer le traitement. La solution s‚Äôappuie sur un mod√®le d‚Äôintelligence artificielle ex√©cut√© en arri√®re-plan, avec un temps d‚Äôex√©cution d‚Äôenviron 10 minutes.

Une fois le traitement termin√©, on clique sur ‚ÄúR√©sultats‚Äù afin de t√©l√©charger le fichier Excel de sortie, comme vous pouvez le voir √† l‚Äô√©cran.

Le fichier Excel g√©n√©r√© contient trois feuilles distinctes :

üîπ La premi√®re feuille, appel√©e Pi√®ce d‚Äôaudit, regroupe les r√©sultats avec des scores de similarit√© inf√©rieurs et sup√©rieurs √† 95 %.

üîπ La deuxi√®me feuille contient les r√©sultats avec un score inf√©rieur √† 95 %, correspondant aux cas non identifi√©s par le mod√®le d‚ÄôIA.

üîπ La troisi√®me feuille regroupe les r√©sultats avec un score sup√©rieur √† 95 %, qui repr√©sentent les cas prioritaires √† analyser par l‚Äô√©quipe op√©rationnelle.

Il est important de noter que les trois feuilles contiennent exactement les m√™mes colonnes.

Voici la liste des colonnes suivantes :

COLONNE 1 et 2 DECAD

Nom interdits chequiers

Raison sociale base atlas

Nom abr√©g√© tiers

Date naissance interdits chequiers ET DE NOTRE BASE ATLAS

Date de cr√©ation (entreprise)

Date naissance Match

Date de cr√©ation Match

Lieu naissance interdits chequier ET DE LA BASE atlas

Lieu naissance

Lieu naissance Match

Date d'effet et de lev√©e

Compte client

Code cat√©gorie client

Id tiers

Score de similarit√©


Gr√¢ce √† cette structuration, Decad AI Solution permet de r√©duire le temps de traitement, de prioriser les analyses, et de s√©curiser le processus de contr√¥le gr√¢ce √† l‚Äôintelligence artificielle.

Merci pour votre attention.




---

If you want, I can also make a version more ‚Äúoral / dynamique‚Äù, so it sounds natural when you speak it in the video, like a real demo narration.

Do you want me to do that?



















Bonjour,

Merci pour votre message et pour la transmission de l‚ÄôExpression de Besoin (EDB) relative √† l‚Äôautomatisation du processus de traitement des agios r√©serv√©s et de leurs reprises li√©es aux clients douteux ‚Äì p√©rim√®tre EPS.

Suite √† l‚Äôanalyse de cette EDB et conform√©ment aux √©l√©ments partag√©s, je vous prie de trouver en pi√®ce jointe les r√©sultats de l‚ÄôAPA relatifs au mois de d√©cembre 2025.

Ces r√©sultats correspondent uniquement au traitement du mois de d√©cembre 2025 et constituent la sortie actuelle de l‚ÄôAPA √† ce stade.
Le fichier Excel de r√©sultats contient √† la fois des clients sains et des clients douteux.

Ils sont le fruit de plusieurs ateliers de travail men√©s avec Mouna, au cours desquels nous avons r√©alis√© de nombreuses v√©rifications manuelles, ainsi que des rapprochements d√©taill√©s entre les r√©sultats APA et les r√©sultats manuels du mois d‚Äôoctobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les r√®gles de gestion appliqu√©es dans le cadre de cette APA, en coh√©rence avec les besoins fonctionnels et techniques d√©crits dans l‚ÄôEDB et avec la recommandation de la Banque d‚ÄôAlg√©rie visant √† remplacer le traitement manuel actuellement r√©alis√© par l‚Äô√©quipe DRAC.

Il s‚Äôagit actuellement d‚Äôune phase de test. Il est propos√© de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilit√© du dispositif et de permettre la validation d√©finitive des r√©sultats par vos soins.

Par ailleurs, l‚ÄôAPA sera livr√©e la semaine prochaine afin d‚Äô√™tre d√©ploy√©e et ex√©cut√©e sur les postes des utilisateurs concern√©s.
√Ä cet effet, nous aurons besoin :

- de l‚Äôinstallation de l‚ÄôAPA sur les postes utilisateurs,
- ainsi que de la mise √† disposition d‚Äôun lien d‚Äôacc√®s vers l‚Äôemplacement de d√©p√¥t des fichiers de sortie.

Je reste bien entendu √† votre disposition, ainsi qu‚ÄôAmina, pour tout compl√©ment d‚Äôinformation ou si vous souhaitez planifier un point d‚Äô√©change.

Cordialement,
Nihad Senhadji

---

If you want, I can now:

- make a very formal / regulatory version (signature‚Äìengagement context), or
- shorten it for a manager-level reply.












Le fichier Excel de r√©sultats contient √† la fois des clients sains et des clients douteux


Bonjour,

Merci pour votre message et pour les √©l√©ments transmis lors de notre √©change de jeudi.

Suite √† l‚Äôanalyse de l‚Äôexpression de besoin relative √† l‚Äôautomatisation du traitement des agios r√©serv√©s leasing, je vous prie de trouver en pi√®ce jointe les r√©sultats de l‚ÄôAPA relatifs au mois de d√©cembre 2025.

Ces r√©sultats correspondent uniquement au traitement du mois de d√©cembre 2025 et constituent la sortie actuelle de l‚ÄôAPA √† ce stade.

Ils sont le fruit de plusieurs ateliers de travail men√©s avec Mouna, au cours desquels nous avons r√©alis√© de nombreuses v√©rifications manuelles, ainsi que des rapprochements d√©taill√©s entre les r√©sultats APA et les r√©sultats manuels du mois d‚Äôoctobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les r√®gles de gestion appliqu√©es dans le cadre de cette APA.

Il s‚Äôagit actuellement d‚Äôune phase de test. Il est propos√© de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilit√© du dispositif et de permettre la validation d√©finitive des r√©sultats par vos soins.

Par ailleurs, l‚ÄôAPA sera livr√©e la semaine prochaine afin d‚Äô√™tre d√©ploy√©e et ex√©cut√©e sur les postes des utilisateurs concern√©s.
√Ä cet effet, nous aurons besoin :

- de l‚Äôinstallation de l‚ÄôAPA sur les postes utilisateurs,
- ainsi que de la mise √† disposition d‚Äôun lien d‚Äôacc√®s vers l‚Äôemplacement de d√©p√¥t des fichiers de sortie.

Je reste bien entendu √† votre disposition pour tout compl√©ment d‚Äôinformation, retour ou ajustement n√©cessaire.

Cordialement,
Nihad Senhadji

---

If you want, I can also adapt the wording to:

- explicitly request






Bonjour,

Merci pour votre message et pour les √©l√©ments transmis lors de notre √©change de jeudi.

Suite √† l‚Äôanalyse de l‚Äôexpression de besoin relative √† l‚Äôautomatisation du traitement des agios r√©serv√©s leasing, je vous prie de trouver en pi√®ce jointe les r√©sultats de l‚ÄôAPA.

Ces r√©sultats correspondent uniquement au traitement du mois en cours et constituent la sortie actuelle de l‚ÄôAPA √† ce stade.

Ils sont le fruit de plusieurs ateliers de travail men√©s avec Mouna, au cours desquels nous avons r√©alis√© de nombreuses v√©rifications manuelles, ainsi que des rapprochements d√©taill√©s entre les r√©sultats APA et les r√©sultats manuels du mois d‚Äôoctobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les r√®gles de gestion appliqu√©es dans le cadre de cette APA.

Il s‚Äôagit actuellement d‚Äôune phase de test. Il est propos√© de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilit√© du dispositif et de permettre la validation d√©finitive des r√©sultats par vos soins.

Par ailleurs, l‚ÄôAPA sera livr√©e la semaine prochaine afin d‚Äô√™tre d√©ploy√©e et ex√©cut√©e sur les postes des utilisateurs concern√©s.
√Ä cet effet, nous aurons besoin :

- de l‚Äôinstallation de l‚ÄôAPA sur les postes utilisateurs,
- ainsi que de la mise √† disposition d‚Äôun lien d‚Äôacc√®s vers l‚Äôemplacement de d√©p√¥t des fichiers de sortie.

Je reste bien entendu √† votre disposition pour tout compl√©ment d‚Äôinformation, retour ou ajustement n√©cessaire.

Cordialement,
Nihad Senhadji

---

If you want, I can also:

- slightly strengthen the request for validation, or
- adapt the wording to a more regulatory / audit-compliant style for DRAC.






Bonjour [Nom du manager],

Je souhaite vous partager un point de situation concernant l‚Äôavancement des projets APA.

1. Avancement g√©n√©ral (APA ESP & APA Arr√™t√©)
Suite aux diff√©rents workshops organis√©s avec la DRAC, l‚Äôensemble des r√®gles de gestion a √©t√© confirm√© et valid√© pour les deux projets.
Un rapprochement a √©galement √©t√© r√©alis√© sur la base de leurs fichiers de novembre 2025.

2. Projet APA ESP
Un seul point reste identifi√© : un probl√®me de code fusion concernant un compte unique.
La DRAC m‚Äôa confirm√© que ce point est d√©j√† port√© √† votre connaissance.

3. Projet APA Arr√™t√©
Un seul point est actuellement en suspens :

- Un √©cart li√© au solde du compte √† vue.
√Ä ce stade, la DRAC n‚Äôest pas en mesure d‚Äôidentifier la m√©thode ou la source utilis√©e pour le calcul de ce solde et doit effectuer des v√©rifications de son c√¥t√© avant de nous apporter un retour d√©finitif.

4. Projet APA Leasing
Le projet APA Leasing n‚Äôa pas encore √©t√© d√©marr√© √† ce stade.

Vous trouverez ci-dessous le lien regroupant les √©l√©ments pour les projets APA ESP et APA Arr√™t√© :
[Lien √† ins√©rer]

Je reste √† votre disposition pour toute pr√©cision compl√©mentaire ou pour un point de suivi.

Cordialement,
Nihad Senhadji






your thanks:

"Je tiens √† vous remercier pour votre accompagnement et vos conseils tout au long de l‚Äôann√©e, et je souhaite continuer √† contribuer activement √† la r√©ussite des objectifs de la Banque, tout en participant aux projets IA et m√©tiers."

It‚Äôs professional, appreciative, and forward-looking.

I can also make a slightly shorter, sharper version if you want it for an email reply.

You can write it like this:

"Je souhaite continuer √† contribuer activement √† la r√©ussite des objectifs de la Banque, tout en participant aux projets IA et m√©tiers."

It‚Äôs concise, professional, and inclusive of both AI and other banking projects.

If you want, I can also suggest 2‚Äì3 alternative formulations that sound slightly more executive.


Understood. We need a leadership-oriented tone without implying you want to replace him‚Äîshowing responsibility, initiative, and team support while still highlighting AI. Here‚Äôs a refined version:


---

Bonjour [Nom du manager],

Merci pour votre retour et pour l‚Äô√©valuation de ma contribution en 2025. J‚Äôai bien pris en compte les axes d‚Äôam√©lioration identifi√©s, notamment sur le cadrage des besoins m√©tiers, la structuration des travaux et la clart√© de la communication.

Je souhaite exprimer mon objectif professionnel, qui est d‚Äô√©voluer vers un r√¥le sur des projets en Intelligence Artificielle, avec des responsabilit√©s clairement d√©finies et des livrables √† impact op√©rationnel.

Je reste par ailleurs pleinement engag√©e √† soutenir l‚Äôensemble des projets bancaires et √† accompagner l‚Äô√©quipe dans l‚Äôorganisation et la coordination des travaux, afin de garantir la qualit√© et l‚Äôefficacit√© des livrables, tout en contribuant activement √† la r√©ussite des projets IA et m√©tiers.

Cordialement,
Nihad Senhadji


---

This version:

Shows initiative and readiness to support the team

Positions you for AI projects

Avoids sounding like you want to replace him


If you want, I can make an even snappier, executive-style version that fits in one paragraph. Do you want me to do that?
