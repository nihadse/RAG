import os
import chromadb
from chromadb.utils import embedding_functions
from langchain.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm

# Initialize Chroma client with persistent storage
chroma_client = chromadb.PersistentClient(path="chroma_db")

# Path to your local SentenceTransformer model directory (e.g., "./models/all-MiniLM-L6-v2")
model_path = "./models/all-MiniLM-L6-v2"  # Replace with your model directory path

# Initialize embedding function with local model
sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name=model_path  # Use local model directory
)

# Create or load a collection in Chroma
collection = chroma_client.get_or_create_collection(
    name="pdf_documents",
    embedding_function=sentence_transformer_ef
)

# Function to load and split PDFs from a directory
def load_and_split_pdfs(pdf_directory):
    print(f"Loading PDFs from {pdf_directory}...")
    loader = PyPDFDirectoryLoader(pdf_directory)
    documents = loader.load()
    
    # Split documents into chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100
    )
    chunks = text_splitter.split_documents(documents)
    return chunks

# Function to store chunks in Chroma
def store_chunks_in_chroma(chunks):
    print("Storing chunks in ChromaDB...")
    texts = [chunk.page_content for chunk in chunks]
    metadatas = [{"source": chunk.metadata["source"]} for chunk in chunks]
    
    # Generate embeddings and store in Chroma
    embeddings = sentence_transformer_ef(texts)  # Uses local model
    
    collection.add(
        documents=texts,
        embeddings=embeddings,
        metadatas=metadatas,
        ids=[f"chunk_{i}" for i in range(len(texts))]
    )

# Function to retrieve relevant chunks
def retrieve_relevant_chunks(query, top_k=3):
    results = collection.query(
        query_texts=[query],
        n_results=top_k
    )
    return results["documents"][0]

# Main chatbot function
def rag_chatbot(pdf_directory):
    # Load and split PDFs
    chunks = load_and_split_pdfs(pdf_directory)
    
    # Store chunks in Chroma
    store_chunks_in_chroma(chunks)
    print("PDFs processed and stored. Ask questions now!")
    
    while True:
        query = input("You: ")
        if query.lower() in ["exit", "quit"]:
            print("Goodbye!")
            break
        
        # Retrieve relevant chunks
        context = "\n".join(retrieve_relevant_chunks(query))
        
        # Generate response (replace with your LLM, e.g., GPT-4)
        print(f"Bot: [Response based on context: {context[:100]}...]")

# Run the code
if __name__ == "__main__":
    pdf_directory = input("Enter PDF directory path: ")
    if not os.path.isdir(pdf_directory):
        print("Invalid directory!")
    else:
        rag_chatbot(pdf_directory)
























import os
import chromadb
from chromadb.utils import embedding_functions
import openai
from langchain.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm

# Set up OpenAI API key
openai.api_key = "your_openai_api_key"

# Initialize Chroma client with persistent storage
chroma_client = chromadb.PersistentClient(path="chroma_db")  # Data will be stored in the "chroma_db" folder

# Use OpenAI embeddings
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key=openai.api_key,
    model_name="text-embedding-ada-002"  # OpenAI's default embedding model
)

# Create or load a collection in Chroma
collection = chroma_client.get_or_create_collection(
    name="pdf_documents",
    embedding_function=openai_ef
)

# Function to load and split PDFs from a directory
def load_and_split_pdfs(pdf_directory):
    print(f"Loading PDFs from {pdf_directory}...")
    
    # Use PyPDFDirectoryLoader to load all PDFs in the directory
    loader = PyPDFDirectoryLoader(pdf_directory)
    documents = loader.load()
    
    # Split documents into smaller chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # Adjust based on your needs
        chunk_overlap=100  # Overlap to preserve context
    )
    chunks = text_splitter.split_documents(documents)
    
    return chunks

# Function to store chunks in Chroma
def store_chunks_in_chroma(chunks):
    print("Storing chunks in ChromaDB...")
    for i, chunk in enumerate(tqdm(chunks, desc="Storing chunks")):
        # Add each chunk to ChromaDB with a unique ID
        collection.add(
            documents=[chunk.page_content],
            ids=[f"chunk_{i}"]
        )

# Function to retrieve relevant chunks
def retrieve_relevant_chunks(query, top_k=3):
    results = collection.query(
        query_texts=[query],
        n_results=top_k
    )
    return results["documents"][0]

# Function to generate response using GPT-4
def generate_response(query, context):
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response['choices'][0]['message']['content']

# Main chatbot function
def rag_chatbot(pdf_directory):
    # Load and split PDFs
    chunks = load_and_split_pdfs(pdf_directory)
    
    # Store chunks in Chroma
    store_chunks_in_chroma(chunks)
    print("PDFs have been processed and stored in Chroma. You can now ask questions.")
    
    while True:
        query = input("You: ")
        if query.lower() in ["exit", "quit"]:
            print("Goodbye!")
            break
        
        # Retrieve relevant chunks
        relevant_chunks = retrieve_relevant_chunks(query)
        context = "\n".join(relevant_chunks)
        
        # Generate response using GPT-4
        response = generate_response(query, context)
        print(f"Bot: {response}")

# Example usage
if __name__ == "__main__":
    # Ask the user for the PDF directory path
    pdf_directory = input("Enter the path to the directory containing PDFs: ")
    
    # Validate the directory path
    if not os.path.isdir(pdf_directory):
        print("Invalid directory path. Please try again.")
    else:
        # Start the chatbot
        rag_chatbot(pdf_directory)


import os
import pdfplumber  # Faster than PyPDF2
import chromadb
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
import openai
from multiprocessing import Pool, cpu_count
from tqdm import tqdm

# Set up OpenAI API key
openai.api_key = "your_openai_api_key"

# Initialize Chroma client with persistent storage
chroma_client = chromadb.PersistentClient(path="chroma_db")  # Data will be stored in the "chroma_db" folder

# Create or load a collection in Chroma
collection = chroma_client.get_or_create_collection(
    name="pdf_documents",
    embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="all-MiniLM-L6-v2"
    )
)

# Initialize embedding model (e.g., SentenceTransformer)
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Function to extract text from a single page
def extract_text_from_page(page):
    return page.extract_text() or ""

# Function to process a PDF in parallel
def process_pdf(pdf_path, chunk_size=500):
    print(f"Processing {pdf_path}...")
    chunks = []
    
    with pdfplumber.open(pdf_path) as pdf:
        # Use multiprocessing to extract text from pages in parallel
        with Pool(cpu_count()) as pool:
            pages = pdf.pages
            page_texts = list(tqdm(pool.imap(extract_text_from_page, pages), total=len(pages), desc="Extracting text"))
        
        # Combine text and chunk it
        full_text = "".join(page_texts)
        chunks = [full_text[i:i + chunk_size] for i in range(0, len(full_text), chunk_size)]
    
    return chunks

# Function to store chunks in Chroma in batches
def store_chunks_in_chroma(chunks, pdf_name, batch_size=100):
    print("Generating embeddings and storing chunks...")
    for i in tqdm(range(0, len(chunks), batch_size), desc="Storing chunks"):
        batch_chunks = chunks[i:i + batch_size]
        embeddings = embedding_model.encode(batch_chunks)
        
        # Add chunks to ChromaDB with unique IDs
        collection.add(
            documents=batch_chunks,
            embeddings=embeddings.tolist(),
            ids=[f"{pdf_name}_chunk_{i + j}" for j in range(len(batch_chunks))]
        )

# Function to retrieve relevant chunks
def retrieve_relevant_chunks(query, top_k=3):
    query_embedding = embedding_model.encode([query])[0].tolist()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    return results["documents"][0]

# Function to generate response using GPT-4
def generate_response(query, context):
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response['choices'][0]['message']['content']

# Main chatbot function
def rag_chatbot(pdf_path):
    # Check if the PDF has already been processed
    pdf_name = os.path.basename(pdf_path)
    existing_ids = collection.get(where={"source": pdf_name})["ids"]
    
    if existing_ids:
        print(f"PDF '{pdf_name}' has already been processed. Skipping extraction.")
    else:
        # Process the PDF
        chunks = process_pdf(pdf_path)
        
        # Store chunks in Chroma with metadata
        store_chunks_in_chroma(chunks, pdf_name)
        print(f"PDF '{pdf_name}' has been processed and stored in Chroma.")
    
    print("You can now ask questions.")
    
    while True:
        query = input("You: ")
        if query.lower() in ["exit", "quit"]:
            print("Goodbye!")
            break
        
        # Retrieve relevant chunks
        relevant_chunks = retrieve_relevant_chunks(query)
        context = "\n".join(relevant_chunks)
        
        # Generate response using GPT-4
        response = generate_response(query, context)
        print(f"Bot: {response}")

# Example usage
if __name__ == "__main__":
    # Ask the user for the PDF file path
    pdf_path = input("Enter the path to the PDF file: ")
    
    # Validate the file path
    if not os.path.isfile(pdf_path):
        print("Invalid file path. Please try again.")
    else:
        # Start the chatbot
        rag_chatbot(pdf_path)













pip install PyPDF2 chromadb sentence-transformers openai


import os
import PyPDF2
import chromadb
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
import openai

# Set up OpenAI API key
openai.api_key = "your_openai_api_key"

# Initialize Chroma client
chroma_client = chromadb.Client()

# Create or load a collection in Chroma
collection = chroma_client.create_collection(name="pdf_documents")

# Initialize embedding model (e.g., SentenceTransformer)
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Function to load and chunk PDF
def load_and_chunk_pdf(pdf_path, chunk_size=500):
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        
        # Split text into chunks
        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
        return chunks

# Function to store chunks in Chroma
def store_chunks_in_chroma(chunks):
    embeddings = embedding_model.encode(chunks)
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        collection.add(
            documents=[chunk],
            embeddings=[embedding.tolist()],
            ids=[str(i)]
        )

# Function to retrieve relevant chunks
def retrieve_relevant_chunks(query, top_k=3):
    query_embedding = embedding_model.encode([query])[0].tolist()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    return results["documents"][0]

# Function to generate response using GPT-4
def generate_response(query, context):
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response['choices'][0]['message']['content']

# Main chatbot function
def rag_chatbot(pdf_path):
    # Load and chunk the PDF
    chunks = load_and_chunk_pdf(pdf_path)
    
    # Store chunks in Chroma
    store_chunks_in_chroma(chunks)
    
    print("PDF has been processed and stored in Chroma. You can now ask questions.")
    
    while True:
        query = input("You: ")
        if query.lower() in ["exit", "quit"]:
            print("Goodbye!")
            break
        
        # Retrieve relevant chunks
        relevant_chunks = retrieve_relevant_chunks(query)
        context = "\n".join(relevant_chunks)
        
        # Generate response using GPT-4
        response = generate_response(query, context)
        print(f"Bot: {response}")

# Example usage
pdf_path = "your_pdf_file.pdf"
rag_chatbot(pdf_path)





import os
import PyPDF2
import chromadb
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
import openai

# Set up OpenAI API key
openai.api_key = "your_openai_api_key"

# Initialize Chroma client
chroma_client = chromadb.Client()

# Create or load a collection in Chroma
collection = chroma_client.create_collection(name="pdf_documents")

# Initialize embedding model (e.g., SentenceTransformer)
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Function to load and chunk PDF
def load_and_chunk_pdf(pdf_path, chunk_size=500):
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        
        # Split text into chunks
        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
        return chunks

# Function to store chunks in Chroma
def store_chunks_in_chroma(chunks):
    embeddings = embedding_model.encode(chunks)
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        collection.add(
            documents=[chunk],
            embeddings=[embedding.tolist()],
            ids=[str(i)]
        )

# Function to retrieve relevant chunks
def retrieve_relevant_chunks(query, top_k=3):
    query_embedding = embedding_model.encode([query])[0].tolist()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    return results["documents"][0]

# Function to generate response using GPT-4
def generate_response(query, context):
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response['choices'][0]['message']['content']

# Main chatbot function
def rag_chatbot(pdf_path):
    # Load and chunk the PDF
    chunks = load_and_chunk_pdf(pdf_path)
    
    # Store chunks in Chroma
    store_chunks_in_chroma(chunks)
    
    print("PDF has been processed and stored in Chroma. You can now ask questions.")
    
    while True:
        query = input("You: ")
        if query.lower() in ["exit", "quit"]:
            print("Goodbye!")
            break
        
        # Retrieve relevant chunks
        relevant_chunks = retrieve_relevant_chunks(query)
        context = "\n".join(relevant_chunks)
        
        # Generate response using GPT-4
        response = generate_response(query, context)
        print(f"Bot: {response}")

# Example usage
pdf_path = "your_pdf_file.pdf"
rag_chatbot(pdf_path)





pip install transformers



from transformers import BertTokenizer

def chunk_text_with_bert(text, max_tokens=512):
    """
    Chunk text using BERT's tokenizer.
    """
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    tokens = tokenizer.encode(text)
    chunks = []

    for i in range(0, len(tokens), max_tokens):
        chunk_tokens = tokens[i:i + max_tokens]
        chunk_text = tokenizer.decode(chunk_tokens)
        chunks.append(chunk_text)

    return chunks






from transformers import BertTokenizer

def chunk_text_with_bert(text, max_tokens=512):
    """
    Chunk text using BERT's tokenizer.
    """
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    tokens = tokenizer.encode(text)
    chunks = []

    for i in range(0, len(tokens), max_tokens):
        chunk_tokens = tokens[i:i + max_tokens]
        chunk_text = tokenizer.decode(chunk_tokens)
        chunks.append(chunk_text)

    return chunks









from transformers import GPT2Tokenizer 


# Step 2: Chunk the extracted text using transformers

def chunk_text_with_transformers(text, max_tokens=500):
    """
    Chunk text into smaller parts using Hugging Face's tokenizers.
    :param text: The input text to chunk.
    :param max_tokens: Maximum number of tokens per chunk.
    :return: List of text chunks.
    """
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")  # Load GPT2 tokenizer
    tokens = tokenizer.encode(text)  # Tokenize the text
    chunks = []

    for i in range(0, len(tokens), max_tokens):
        chunk_tokens = tokens[i:i + max_tokens]
        chunk_text = tokenizer.decode(chunk_tokens)
        chunks.append(chunk_text)
    
    return chunk








pip install fitz python-docx nltk tiktoken sentence-transformers chromadb openai






# Import necessary libraries
import fitz  # PyMuPDF for PDF processing
from docx import Document  # For Word file processing
from nltk.tokenize import sent_tokenize  # For text chunking
import tiktoken  # For token-based chunking
from sentence_transformers import SentenceTransformer  # For text embeddings
import chromadb  # Vector database
import openai  # For GPT-based generation

# Step 1: Extract text, tables, and images from files

# Extract text and images from a PDF file
def extract_from_pdf(pdf_path):
    """
    Extract text and images from a PDF file.
    :param pdf_path: Path to the PDF file.
    :return: Extracted text and images as a list of bytes.
    """
    doc = fitz.open(pdf_path)
    text = ""
    images = []
    
    for page in doc:
        # Extract text from the page
        text += page.get_text()
        # Extract images from the page
        for img in page.get_images(full=True):
            xref = img[0]
            pix = fitz.Pixmap(doc, xref)
            if pix.n > 4:  # Convert CMYK to RGB
                pix = fitz.Pixmap(fitz.csRGB, pix)
            # Save image as PNG bytes
            images.append(pix.tobytes("png"))
    return text, images

# Extract text from a Word document
def extract_from_word(doc_path):
    """
    Extract text from a Word document.
    :param doc_path: Path to the Word document.
    :return: Extracted text as a string.
    """
    doc = Document(doc_path)
    # Concatenate all paragraph texts
    text = "\n".join([p.text for p in doc.paragraphs])
    return text

# Step 2: Chunk the extracted text

def chunk_text(text, max_tokens=500):
    """
    Chunk the extracted text into smaller parts for embedding.
    :param text: The input text to chunk.
    :param max_tokens: Maximum number of tokens per chunk.
    :return: List of text chunks.
    """
    tokenizer = tiktoken.get_encoding("cl100k_base")  # Tokenizer for counting tokens
    sentences = sent_tokenize(text)  # Split text into sentences
    chunks = []
    current_chunk = []
    current_length = 0

    for sentence in sentences:
        tokenized = tokenizer.encode(sentence)
        # If adding this sentence exceeds max tokens, finalize current chunk
        if current_length + len(tokenized) > max_tokens:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            current_length = 0
        current_chunk.append(sentence)
        current_length += len(tokenized)
    
    # Add the last chunk
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    return chunks

# Step 3: Store chunks in ChromaDB

# Initialize ChromaDB
client = chromadb.Client()  # Initialize the ChromaDB client
collection = client.get_or_create_collection("document_chunks")  # Create or get the collection

# Embed text chunks and store them in ChromaDB
def store_chunks_in_chroma(chunks):
    """
    Store text chunks into ChromaDB after embedding them.
    :param chunks: List of text chunks.
    """
    model = SentenceTransformer('all-MiniLM-L6-v2')  # Load the sentence transformer model
    embeddings = model.encode(chunks)  # Generate embeddings for each chunk
    for i, chunk in enumerate(chunks):
        # Add the chunk to the collection
        collection.add(
            documents=[chunk],
            metadatas=[{"chunk_id": i}],  # Optional metadata
            embeddings=[embeddings[i]]
        )

# Step 4: Perform RAG-based retrieval and response generation

# Retrieve the most relevant chunks and generate a GPT-based response
def retrieve_and_generate_response(query):
    """
    Retrieve relevant chunks from ChromaDB and generate a GPT response.
    :param query: User query.
    :return: Generated response from GPT.
    """
    model = SentenceTransformer('all-MiniLM-L6-v2')  # Load the embedding model
    query_embedding = model.encode([query])  # Embed the query
    # Search for the most relevant chunks in ChromaDB
    results = collection.query(
        query_embeddings=query_embedding,
        n_results=5  # Number of results to retrieve
    )
    
    # Combine retrieved chunks into a single text
    retrieved_text = " ".join([doc for doc in results['documents'][0]])
    
    # Generate a response using GPT
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are an AI assistant."},
            {"role": "user", "content": f"Answer based on the following:\n\n{retrieved_text}\n\nQuestion: {query}"}
        ]
    )
    return response['choices'][0]['message']['content']

# Step 5: Putting it all together

def process_file_and_respond(file_path, file_type, query):
    """
    Process the file, store its contents in ChromaDB, and generate a response.
    :param file_path: Path to the input file (PDF/Word).
    :param file_type: Type of the file ('pdf' or 'word').
    :param query: User query.
    :return: GPT-generated response.
    """
    if file_type == 'pdf':
        text, images = extract_from_pdf(file_path)  # Extract text and images from PDF
    elif file_type == 'word':
        text = extract_from_word(file_path)  # Extract text from Word document
        images = []  # Word file images are not handled here

    # Chunk the extracted text
    chunks = chunk_text(text)
    # Store the chunks in ChromaDB
    store_chunks_in_chroma(chunks)
    # Generate a response for the query
    return retrieve_and_generate_response(query)

# Example usage:
if __name__ == "__main__":
    # File path and type
    file_path = "sample.pdf"  # Replace with the actual file path
    file_type = "pdf"  # 'pdf' or 'word'
    user_query = "What is the content of the document about?"
    
    # Process the file and respond to the query
    response = process_file_and_respond(file_path, file_type, user_query)
    print("Response:", response)









import openai
import requests

# Set up your OpenAI API key
openai.api_key = "your_openai_api_key"

# Function to extract and process PDF content
def extract_and_process_pdf(pdf_file_path):
    # Upload the PDF to the Domino workspace (if applicable)
    # Use an API call or platform-specific method to upload the PDF

    # Open the PDF and extract its content (you can also extract the text before sending to GPT-4)
    with open(pdf_file_path, 'rb') as file:
        pdf_content = file.read()  # Simulate reading the PDF file content (you would process it before)

    # Construct the prompt to send to ChatGPT 4.0
    prompt = """
    I have uploaded a PDF document. Please extract all the text from it and return the content in a clean and readable format.
    If there are any tables, images, or special sections, please indicate them clearly.
    """

    # Send the request to ChatGPT 4.0 (you could also send raw text if PDF parsing is handled earlier)
    response = openai.Completion.create(
        engine="gpt-4",  # or 'gpt-4-domino' if that is available
        prompt=prompt,
        max_tokens=1000
    )

    # Print the response (extracted text)
    return response.choices[0].text.strip()

# Example usage
pdf_path = "path_to_your_pdf_file.pdf"
extracted_text = extract_and_process_pdf(pdf_path)
print(extracted_text)




import fitz  # PyMuPDF
import re
import os

# Function to extract text and images from each page in the PDF
def extract_text_and_images(pdf_path):
    doc = fitz.open(pdf_path)
    
    all_text = ""
    images = []
    
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        
        # Extract text from the page
        page_text = page.get_text("text")
        all_text += page_text  # Add text from this page to all_text
        
        # Extract images from the page
        image_list = page.get_images(full=True)
        for img_index, img in enumerate(image_list):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]  # Binary image data
            images.append(image_data)
    
    return all_text, images

# Function to identify tables in the text (basic pattern detection)
def identify_tables(text):
    # Example: Look for common table patterns like rows and columns
    rows = text.split("\n")
    table_data = []
    for row in rows:
        # Split each row by whitespace or tabs (based on PDF formatting)
        columns = re.split(r'\s{2,}', row.strip())  # Assuming columns are separated by at least two spaces
        if len(columns) > 1:  # Heuristic: rows with multiple columns might be tables
            table_data.append(columns)
    
    return table_data

# Function to process the entire PDF
def process_pdf(pdf_path, output_dir="output"):
    # Ensure output directory exists
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # Extract text and images from the PDF
    text, images = extract_text_and_images(pdf_path)
    
    # Identify tables in the extracted text
    table_data = identify_tables(text)
    
    # Save the identified tables to a file (optional)
    table_file = os.path.join(output_dir, "tables.txt")
    with open(table_file, "w") as f:
        for table in table_data:
            f.write("\t".join(table) + "\n")
    
    print(f"Extracted {len(table_data)} tables from the PDF.")
    
    # Optionally save the images to separate files
    for i, image_data in enumerate(images):
        image_file = os.path.join(output_dir, f"image_{i + 1}.png")
        with open(image_file, "wb") as img_f:
            img_f.write(image_data)
    
    print(f"Extracted {len(images)} images from the PDF and saved them.")

# Example usage
pdf_path = "your_pdf_file.pdf"  # Replace with your PDF file path
process_pdf(pdf_path)
