from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
import torch

def split_documents_semantic(documents):
    """Semantic chunking with local model"""
    # Path to your downloaded model
    model_path = "/path/to/your/bge-m3"  # Replace with your actual path
    
    # Initialize embeddings from local files
    embeddings = HuggingFaceEmbeddings(
        model_name=model_path,
        model_kwargs={
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "trust_remote_code": True
        },
        encode_kwargs={
            "normalize_embeddings": True,
            "batch_size": 32  # Adjust based on VRAM
        }
    )

    # Configure semantic splitter
    text_splitter = SemanticChunker(
        embeddings=embeddings,
        add_start_index=True,
        max_chunk_size=1500,  # Target max token count
        min_chunk_size=300,    # Minimum meaningful chunk size
        breakpoint_percentile_threshold=92  # 92% similarity threshold
    )
    
    return text_splitter.split_documents(documents)


from langchain_experimental.text_splitter import SemanticChunker
def split_documents_semantic(documents):
    """Hybrid semantic+size-based splitting"""
    model = SentenceTransformer("BAAI/bge-m3", device="cuda" if torch.cuda.is_available() else "cpu")
    
    # 1. Semantic chunking


    semantic_splitter = SemanticChunker(
        embeddings=model,
        breakpoint_threshold=0.82,  # More splits
        add_start_index=True
    )
    semi_chunks = semantic_splitter.split_documents(documents)
    
    # 2. Size enforcement
    size_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1200,
        chunk_overlap=200,
        separators=["\n\n## ", "\n\n", ". ", "; "]
    )
    
    final_chunks = size_splitter.split_documents(semi_chunks)
    
    # 3. Validate chunks
    avg_size = sum(len(c.page_content) for c in final_chunks)/len(final_chunks)
    print(f"Average chunk size: {avg_size:.0f} chars")
    
    return final_chunks
