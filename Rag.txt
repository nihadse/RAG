keyword = "CLIENT_ACCOUNT"

data = chroma_collection.get(
    include=["documents", "metadatas"]
)

hits = 0

documents = data["documents"]
metadatas = data["metadatas"]

# Handle both flat list and list-of-lists
if documents and isinstance(documents[0], list):
    documents = documents[0]
    metadatas = metadatas[0]

for i, (doc, meta) in enumerate(zip(documents, metadatas)):
    if keyword.lower() in doc.lower():
        print(f"\n--- MATCHING CHUNK {i} ---")
        print("Metadata:", meta)
        print(doc)
        hits += 1

print("\nTotal matching chunks:", hits)











data = chroma_collection.get(
    where={"source": "core_banking_architecture.pdf"},
    include=["documents", "metadatas"]
)

for i, doc in enumerate(data["documents"]):
    print(f"\n--- CHUNK {i} ---\n")
    print(doc)





bm25_reranker = BM25Reranker(
    documents=documents,
    k=50   # how many BM25 docs you want
)


hybrid_retriever = HybridRetriever(
    collection=chroma_collection,
    bm25_reranker=bm25_reranker,
    k_dense=120,   # large recall
    k_final=20     # final docs sent to LLM
)

query = "Which database stores AML risk classification for retail clients?"

retrieved_docs = hybrid_retriever.retrieve(query)



from rank_bm25 import BM25Okapi
import re

class BM25Reranker:
    def __init__(self, documents, k=20):
        self.k = k
        self.documents = documents
        self.tokenized_corpus = [self._tokenize(d["text"]) for d in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)

    def _tokenize(self, text):
        return re.findall(r"[A-Za-z0-9_\.]+", text.lower())

    def rerank(self, query):
        query_tokens = self._tokenize(query)
        scores = self.bm25.get_scores(query_tokens)

        ranked = sorted(
            zip(scores, self.documents),
            key=lambda x: x[0],
            reverse=True
        )
        return [doc for _, doc in ranked[:self.k]]class HybridRetriever:
    def __init__(self, collection, bm25_reranker, k_dense=100, k_final=20):
        self.collection = collection
        self.bm25 = bm25_reranker
        self.k_dense = k_dense
        self.k_final = k_final

    def retrieve(self, query):
        dense = self.collection.query(
            query_texts=[query],
            n_results=self.k_dense,
            include=["documents", "metadatas"]
        )

        dense_docs = [
            {"text": d, "metadata": m}
            for d, m in zip(dense["documents"][0], dense["metadatas"][0])
        ]

        # BM25 rescue
        sparse_docs = self.bm25.rerank(query)

        # Union + dedup
        seen = set()
        merged = []
        for d in dense_docs + sparse_docs:
            h = hash(d["text"])
            if h not in seen:
                seen.add(h)
                merged.append(d)

        return merged[:self.k_final]from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_community.document_compressors.rankllm_rerank import RankLLMRerank
from typing import List

def hybrid_retrieve_langchain(
    collection,           # Your vector DB collection (Chroma/FAISS)
    llm_client,           # Azure/OpenAI client (LLM)
    query: str,
    k_dense: int = 50,    # Number of top docs from embeddings
    k_final: int = 20     # Number of final docs after reranking
) -> List[dict]:
    """
    Hybrid Retrieval using:
    Step 1: Dense retrieval from vector DB
    Step 2: RankLLM reranking (via LangChain)
    Returns top documents (text + metadata)
    """

    # --------------------------
    # Step 1: Dense retrieval
    # --------------------------
    dense_results = collection.query(
        query_texts=[query],
        n_results=k_dense,
        include=["documents", "metadatas"]
    )

    candidates = []
    for text, meta in zip(dense_results["documents"][0], dense_results["metadatas"][0]):
        candidates.append({"text": text, "metadata": meta})

    if not candidates:
        return []

    # --------------------------
    # Step 2: RankLLM reranker
    # --------------------------
    reranker = RankLLMRerank.from_llm(
        llm=llm_client,  # your Azure/OpenAI client
        top_n=k_final,
        verbose=True
    )

    # Optional: wrap with ContextualCompressionRetriever if you want
    # to compress context before passing to reranker (useful for long chunks)
    retriever = ContextualCompressionRetriever(
        base_retriever=candidates,  # here just the list of candidates
        compressor=reranker
    )

    # Run reranking
    reranked_docs = reranker.rerank(query=query, documents=candidates)

    return reranked_docs[:k_final]






query = "How to configure user permissions in the system"

top_docs = hybrid_retrieve_langchain(
    collection=chroma_collection,
    llm_client=azure_openai_client,
    query=query,
    k_dense=80,
    k_final=20
)

for doc in top_docs:
    print(doc["text"][:300], "...\n")







import streamlit as st
import chromadb
from chromadb.utils import embedding_functions
import fitz  # PyMuPDF
from docx import Document
import pandas as pd
from datetime import datetime
import json
import os
from typing import List, Dict, Tuple, Optional
import re
from collections import defaultdict
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
import hashlib

# Azure OpenAI imports
from openai import AzureOpenAI
import httpx

# ==============================================================================
# CONFIGURATION
# ==============================================================================

class Config:
    """Centralized configuration management"""
    
    # Azure OpenAI Configuration
    OIDC_CLIENT_ID = os.getenv("OIDC_CLIENT_ID", "your_client_id")
    OIDC_CLIENT_SECRET = os.getenv("OIDC_CLIENT_SECRET", "your_client_secret")
    OIDC_ENDPOINT = os.getenv("OIDC_ENDPOINT", "https://alfactory.api.staging.schonet/auth/oauth2/v2/token")
    OIDC_SCOPE = os.getenv("OIDC_SCOPE", "genai-model")
    
    AZURE_ENDPOINT = os.getenv("AZURE_ENDPOINT", "https://alfactory.api.staging.schonet/genai-model/v1")
    AZURE_API_VERSION = os.getenv("AZURE_API_VERSION", "2024-05-01-preview")
    AZURE_API_KEY = os.getenv("AZURE_API_KEY", "FAKE_KEY")
    AZURE_MODEL_DEPLOYMENT = os.getenv("AZURE_MODEL_DEPLOYMENT", "gpt-4o")
    
    # Embedding Configuration
    EMBEDDING_MODEL_PATH = "/domino/datasets/local/test-prd-base"
    EMBEDDING_DIMENSION = 1024
    
    # ChromaDB Configuration
    CHROMA_PERSIST_PATH = "/domino/datasets/local/chroma_persistent_db"
    COLLECTION_NAME = "banking_documents"
    
    # Chunking Configuration for Banking Documents
    CHUNK_SIZES = {
        'header': 300,      # For sections with headers
        'table': 500,       # For tables and structured data
        'paragraph': 400,   # For regular paragraphs
        'list': 350,        # For lists and enumerations
    }
    CHUNK_OVERLAP = 100
    
    # BM25 Configuration
    BM25_INDEX_PATH = "/domino/datasets/local/bm25_index.pkl"
    BM25_K1 = 1.5
    BM25_B = 0.75
    
    # Retrieval Configuration
    INITIAL_RETRIEVAL_K = 20  # Retrieve more for reranking
    FINAL_RESULTS_K = 5       # Final results after reranking
    HYBRID_ALPHA = 0.5        # Balance between semantic (0) and keyword (1)
    
    # Batch Processing Configuration
    BATCH_SIZE = 100
    MAX_WORKERS = 4

# ==============================================================================
# DOCUMENT PROCESSING & CHUNKING
# ==============================================================================

class BankingDocumentProcessor:
    """Advanced document processor optimized for banking/technical documents"""
    
    def __init__(self):
        self.chunk_patterns = {
            'section_header': re.compile(r'^(Sect\.|Section|Chapitre|Article|Â§)\s*[\dIVX]+[\.\s]', re.IGNORECASE),
            'subsection': re.compile(r'^[A-Z]{1,3}\.\d+(\.\d+)*\s+', re.MULTILINE),
            'table_marker': re.compile(r'(Description Rubrique|Num\s+donn\.|Val\s+init)', re.IGNORECASE),
            'code_block': re.compile(r'(Code|RÃ©fÃ©rence|Type de contrat):\s*\d+', re.IGNORECASE),
            'list_item': re.compile(r'^\s*[-â€¢*]\s+|\d+\.\s+', re.MULTILINE),
        }
    
    def extract_text_from_file(self, uploaded_file) -> Optional[Dict]:
        """Extract text with metadata from uploaded files"""
        filename = uploaded_file.name.lower()
        
        try:
            if filename.endswith('.pdf'):
                return self._extract_from_pdf(uploaded_file)
            elif filename.endswith('.txt'):
                text = uploaded_file.read().decode('utf-8')
                return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
            elif filename.endswith('.docx'):
                return self._extract_from_docx(uploaded_file)
            elif filename.endswith(('.csv', '.xlsx')):
                return self._extract_from_spreadsheet(uploaded_file, filename)
            else:
                return None
        except Exception as e:
            st.error(f"Error extracting from {uploaded_file.name}: {str(e)}")
            return None
    
    def _extract_from_pdf(self, uploaded_file) -> Dict:
        """Extract text from PDF with page-level granularity"""
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        pages = []
        
        for page_num, page in enumerate(doc, start=1):
            text = page.get_text()
            pages.append({
                'page_num': page_num,
                'text': text
            })
        
        full_text = "\n".join([p['text'] for p in pages])
        return {'text': full_text, 'pages': pages}
    
    def _extract_from_docx(self, uploaded_file) -> Dict:
        """Extract text from DOCX"""
        doc = Document(uploaded_file)
        text = "\n".join([p.text for p in doc.paragraphs])
        return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
    
    def _extract_from_spreadsheet(self, uploaded_file, filename: str) -> Dict:
        """Extract text from CSV/Excel"""
        if filename.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)
        
        text = df.to_string(index=False)
        return {'text': text, 'pages': [{'page_num': 1, 'text': text}]}
    
    def clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove excessive whitespace
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        
        # Normalize line breaks
        text = text.replace('\r\n', '\n')
        
        # Remove page headers/footers (common patterns)
        text = re.sub(r'Page \d+( of \d+)?', '', text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def smart_chunk_document(self, doc_data: Dict, source_file: str) -> List[Dict]:
        """
        Intelligent chunking that preserves document structure
        Optimized for banking/technical documents
        """
        chunks = []
        pages = doc_data.get('pages', [])
        
        for page_info in pages:
            page_num = page_info['page_num']
            text = self.clean_text(page_info['text'])
            
            # Detect document structure
            if self._is_table_content(text):
                page_chunks = self._chunk_table_content(text, source_file, page_num)
            elif self._has_clear_sections(text):
                page_chunks = self._chunk_by_sections(text, source_file, page_num)
            else:
                page_chunks = self._chunk_semantic(text, source_file, page_num)
            
            chunks.extend(page_chunks)
        
        return chunks
    
    def _is_table_content(self, text: str) -> bool:
        """Detect if content is primarily tabular"""
        return bool(self.chunk_patterns['table_marker'].search(text))
    
    def _has_clear_sections(self, text: str) -> bool:
        """Detect if content has clear section structure"""
        headers = self.chunk_patterns['section_header'].findall(text)
        return len(headers) >= 2
    
    def _chunk_table_content(self, text: str, source: str, page: int) -> List[Dict]:
        """Chunk table content preserving structure"""
        chunks = []
        lines = text.split('\n')
        current_chunk = []
        current_size = 0
        
        for line in lines:
            line_size = len(line)
            
            if current_size + line_size > Config.CHUNK_SIZES['table']:
                if current_chunk:
                    chunks.append(self._create_chunk(
                        '\n'.join(current_chunk), source, page, 'table'
                    ))
                    # Overlap: keep last few lines
                    overlap_lines = current_chunk[-3:] if len(current_chunk) > 3 else current_chunk
                    current_chunk = overlap_lines
                    current_size = sum(len(l) for l in current_chunk)
            
            current_chunk.append(line)
            current_size += line_size
        
        if current_chunk:
            chunks.append(self._create_chunk(
                '\n'.join(current_chunk), source, page, 'table'
            ))
        
        return chunks
    
    def _chunk_by_sections(self, text: str, source: str, page: int) -> List[Dict]:
        """Chunk by document sections"""
        chunks = []
        sections = re.split(r'(\n(?:Sect\.|Section|Chapitre|Article)\s+[\dIVX]+)', text, flags=re.IGNORECASE)
        
        current_section = ""
        
        for i, section in enumerate(sections):
            if i % 2 == 0:  # Content
                current_section += section
            else:  # Header
                if current_section.strip():
                    chunks.append(self._create_chunk(
                        current_section.strip(), source, page, 'section'
                    ))
                current_section = section
        
        if current_section.strip():
            chunks.append(self._create_chunk(
                current_section.strip(), source, page, 'section'
            ))
        
        return chunks
    
    def _chunk_semantic(self, text: str, source: str, page: int) -> List[Dict]:
        """Semantic chunking with overlap for regular content"""
        chunks = []
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            if current_size + sentence_size > Config.CHUNK_SIZES['paragraph']:
                if current_chunk:
                    chunks.append(self._create_chunk(
                        ' '.join(current_chunk), source, page, 'paragraph'
                    ))
                    # Overlap: keep last 2 sentences
                    overlap = current_chunk[-2:] if len(current_chunk) > 2 else current_chunk
                    current_chunk = overlap
                    current_size = sum(len(s) for s in current_chunk)
            
            current_chunk.append(sentence)
            current_size += sentence_size
        
        if current_chunk:
            chunks.append(self._create_chunk(
                ' '.join(current_chunk), source, page, 'paragraph'
            ))
        
        return chunks
    
    def _create_chunk(self, text: str, source: str, page: int, chunk_type: str) -> Dict:
        """Create a standardized chunk object"""
        chunk_id = hashlib.md5(f"{source}_{page}_{text[:50]}".encode()).hexdigest()
        
        return {
            'id': chunk_id,
            'text': text,
            'metadata': {
                'source_file': source,
                'page': page,
                'chunk_type': chunk_type,
                'char_count': len(text),
                'timestamp': str(datetime.now())
            }
        }

# ==============================================================================
# BM25 IMPLEMENTATION WITH PERSISTENT INDEX
# ==============================================================================

class PersistentBM25:
    """BM25 implementation with disk persistence for large-scale retrieval"""
    
    def __init__(self, k1: float = Config.BM25_K1, b: float = Config.BM25_B):
        self.k1 = k1
        self.b = b
        self.corpus_size = 0
        self.avgdl = 0
        self.doc_freqs = defaultdict(int)
        self.idf = {}
        self.doc_len = []
        self.doc_ids = []
        self.tokenized_corpus = []
        
    def tokenize(self, text: str) -> List[str]:
        """Tokenize with banking-specific preservation"""
        # Preserve codes and references
        text = re.sub(r'([A-Z]{2,}\d+)', r' \1 ', text)
        
        # Remove punctuation but keep hyphens in codes
        text = re.sub(r'[^\w\s-]', ' ', text.lower())
        
        # Tokenize
        tokens = text.split()
        
        # Filter stopwords (French banking context)
        stopwords = {'le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'ou', 'dans', 'pour', 'sur', 'avec'}
        tokens = [t for t in tokens if t and t not in stopwords]
        
        return tokens
    
    def fit(self, documents: List[Dict]):
        """Build BM25 index from documents"""
        self.corpus_size = len(documents)
        self.doc_ids = [doc['id'] for doc in documents]
        self.tokenized_corpus = []
        
        # Tokenize all documents
        for doc in documents:
            tokens = self.tokenize(doc['text'])
            self.tokenized_corpus.append(tokens)
            self.doc_len.append(len(tokens))
            
            # Count document frequencies
            unique_tokens = set(tokens)
            for token in unique_tokens:
                self.doc_freqs[token] += 1
        
        # Calculate average document length
        self.avgdl = sum(self.doc_len) / self.corpus_size if self.corpus_size > 0 else 0
        
        # Calculate IDF values
        for token, freq in self.doc_freqs.items():
            self.idf[token] = np.log((self.corpus_size - freq + 0.5) / (freq + 0.5) + 1)
    
    def get_scores(self, query: str) -> Dict[str, float]:
        """Calculate BM25 scores for a query"""
        query_tokens = self.tokenize(query)
        scores = {}
        
        for idx, (doc_id, doc_tokens, doc_length) in enumerate(
            zip(self.doc_ids, self.tokenized_corpus, self.doc_len)
        ):
            score = 0
            for token in query_tokens:
                if token not in self.idf:
                    continue
                
                # Calculate term frequency in document
                tf = doc_tokens.count(token)
                
                # BM25 formula
                numerator = tf * (self.k1 + 1)
                denominator = tf + self.k1 * (1 - self.b + self.b * (doc_length / self.avgdl))
                score += self.idf[token] * (numerator / denominator)
            
            scores[doc_id] = score
        
        return scores
    
    def save(self, filepath: str):
        """Save BM25 index to disk"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'k1': self.k1,
                'b': self.b,
                'corpus_size': self.corpus_size,
                'avgdl': self.avgdl,
                'doc_freqs': dict(self.doc_freqs),
                'idf': self.idf,
                'doc_len': self.doc_len,
                'doc_ids': self.doc_ids,
                'tokenized_corpus': self.tokenized_corpus
            }, f)
    
    @classmethod
    def load(cls, filepath: str) -> 'PersistentBM25':
        """Load BM25 index from disk"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        
        bm25 = cls(k1=data['k1'], b=data['b'])
        bm25.corpus_size = data['corpus_size']
        bm25.avgdl = data['avgdl']
        bm25.doc_freqs = defaultdict(int, data['doc_freqs'])
        bm25.idf = data['idf']
        bm25.doc_len = data['doc_len']
        bm25.doc_ids = data['doc_ids']
        bm25.tokenized_corpus = data['tokenized_corpus']
        
        return bm25

# ==============================================================================
# QUERY EXPANSION FOR BANKING TERMINOLOGY
# ==============================================================================

class BankingQueryExpander:
    """Query expansion specialized for banking/financial terminology"""
    
    def __init__(self):
        # Banking-specific synonym mappings
        self.synonyms = {
            'compte': ['account', 'compte support', 'compte client'],
            'contrat': ['contract', 'agreement', 'convention'],
            'dÃ©pÃ´t': ['deposit', 'versement', 'apport'],
            'garantie': ['guarantee', 'caution', 'warranty'],
            'Ã©chÃ©ance': ['maturity', 'deadline', 'due date', 'expiration'],
            'nantissement': ['pledge', 'collateral', 'guarantee'],
            'saisie': ['entry', 'input', 'capture'],
            'grille': ['grid', 'screen', 'form'],
            'transaction': ['operation', 'mouvement', 'transfer'],
            'client': ['customer', 'account holder', 'beneficiary'],
            'bÃ©nÃ©ficiaire': ['beneficiary', 'recipient', 'payee'],
            'taux': ['rate', 'interest rate', 'percentage'],
            'plafond': ['ceiling', 'limit', 'cap', 'maximum'],
            'retrait': ['withdrawal', 'extraction'],
            'virement': ['transfer', 'wire', 'payment'],
        }
        
        # Common banking abbreviations
        self.abbreviations = {
            'DAT': 'DÃ©pÃ´t Ã  Terme',
            'CNT': 'Contrat',
            'GDI': 'Gestion De Interface',
            'BDC': 'Bon De Caisse',
            'SGE': 'Code siÃ¨ge',
            'DEV': 'Code devise',
        }
    
    def expand_query(self, query: str) -> List[str]:
        """Expand query with synonyms and banking terms"""
        expanded_queries = [query]
        
        query_lower = query.lower()
        
        # Add synonym expansions
        for term, synonyms in self.synonyms.items():
            if term in query_lower:
                for synonym in synonyms:
                    expanded = query_lower.replace(term, synonym)
                    if expanded != query_lower:
                        expanded_queries.append(expanded)
        
        # Expand abbreviations
        for abbr, full_form in self.abbreviations.items():
            if abbr in query:
                expanded = query.replace(abbr, full_form)
                expanded_queries.append(expanded)
            elif full_form.lower() in query_lower:
                expanded = query_lower.replace(full_form.lower(), abbr)
                expanded_queries.append(expanded)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_queries = []
        for q in expanded_queries:
            if q not in seen:
                seen.add(q)
                unique_queries.append(q)
        
        return unique_queries[:5]  # Limit to top 5 expansions

# ==============================================================================
# HYBRID RETRIEVAL SYSTEM
# ==============================================================================

class HybridRetriever:
    """Hybrid retrieval combining semantic search (ChromaDB) and keyword search (BM25)"""
    
    def __init__(self, chroma_collection, bm25_index: PersistentBM25, alpha: float = Config.HYBRID_ALPHA):
        self.chroma_collection = chroma_collection
        self.bm25_index = bm25_index
        self.alpha = alpha  # Weight between semantic (0) and keyword (1)
        self.query_expander = BankingQueryExpander()
    
    def retrieve(self, query: str, k: int = Config.INITIAL_RETRIEVAL_K) -> List[Dict]:
        """
        Hybrid retrieval with query expansion
        Returns top k documents based on weighted combination of semantic + keyword scores
        """
        # Expand query for better recall
        expanded_queries = self.query_expander.expand_query(query)
        
        all_results = {}
        
        for exp_query in expanded_queries:
            # 1. Semantic search via ChromaDB
            semantic_results = self.chroma_collection.query(
                query_texts=[exp_query],
                n_results=k,
                include=["documents", "metadatas", "distances"]
            )
            
            # Normalize semantic scores (distances -> similarities)
            semantic_scores = {}
            if semantic_results['ids'] and semantic_results['ids'][0]:
                max_distance = max(semantic_results['distances'][0]) if semantic_results['distances'][0] else 1
                for idx, doc_id in enumerate(semantic_results['ids'][0]):
                    distance = semantic_results['distances'][0][idx]
                    similarity = 1 - (distance / max_distance) if max_distance > 0 else 1
                    semantic_scores[doc_id] = similarity
            
            # 2. Keyword search via BM25
            bm25_scores = self.bm25_index.get_scores(exp_query)
            
            # Normalize BM25 scores
            max_bm25 = max(bm25_scores.values()) if bm25_scores else 1
            normalized_bm25 = {k: v / max_bm25 for k, v in bm25_scores.items()} if max_bm25 > 0 else bm25_scores
            
            # 3. Combine scores with weighted sum
            all_doc_ids = set(semantic_scores.keys()) | set(normalized_bm25.keys())
            
            for doc_id in all_doc_ids:
                sem_score = semantic_scores.get(doc_id, 0)
                bm25_score = normalized_bm25.get(doc_id, 0)
                
                # Hybrid score
                hybrid_score = (1 - self.alpha) * sem_score + self.alpha * bm25_score
                
                # Accumulate scores across expanded queries
                if doc_id in all_results:
                    all_results[doc_id]['score'] = max(all_results[doc_id]['score'], hybrid_score)
                else:
                    # Get document metadata
                    try:
                        doc_data = self.chroma_collection.get(
                            ids=[doc_id],
                            include=["documents", "metadatas"]
                        )
                        if doc_data['ids']:
                            all_results[doc_id] = {
                                'id': doc_id,
                                'text': doc_data['documents'][0],
                                'metadata': doc_data['metadatas'][0],
                                'score': hybrid_score,
                                'semantic_score': sem_score,
                                'bm25_score': bm25_score
                            }
                    except:
                        pass
        
        # Sort by score and return top k
        ranked_results = sorted(all_results.values(), key=lambda x: x['score'], reverse=True)
        return ranked_results[:k]

# ==============================================================================
# BATCH DOCUMENT INGESTION
# ==============================================================================

class BatchDocumentIngestion:
    """Optimized batch processing for large document collections"""
    
    def __init__(self, chroma_collection, processor: BankingDocumentProcessor):
        self.collection = chroma_collection
        self.processor = processor
        self.bm25_documents = []
    
    def ingest_documents(self, uploaded_files: List, progress_callback=None) -> Dict:
        """
        Batch ingest documents with progress tracking
        Returns statistics about the ingestion process
        """
        stats = {
            'total_files': len(uploaded_files),
            'processed_files': 0,
            'total_chunks': 0,
            'failed_files': [],
            'processing_time': 0
        }
        
        start_time = datetime.now()
        
        # Get existing documents to avoid duplicates
        existing_sources = self._get_existing_sources()
        
        batch_chunks = []
        
        for file_idx, uploaded_file in enumerate(uploaded_files):
            try:
                filename = uploaded_file.name
                
                # Skip if already indexed
                if filename in existing_sources:
                    if progress_callback:
                        progress_callback(file_idx + 1, len(uploaded_files), f"Skipped (already indexed): {filename}")
                    continue
                
                if progress_callback:
                    progress_callback(file_idx + 1, len(uploaded_files), f"Processing: {filename}")
                
                # Extract and chunk document
                doc_data = self.processor.extract_text_from_file(uploaded_file)
                
                if doc_data is None:
                    stats['failed_files'].append(filename)
                    continue
                
                chunks = self.processor.smart_chunk_document(doc_data, filename)
                
                # Add to batch
                batch_chunks.extend(chunks)
                self.bm25_documents.extend(chunks)
                
                # Process batch if it reaches batch size
                if len(batch_chunks) >= Config.BATCH_SIZE:
                    self._process_batch(batch_chunks)
                    stats['total_chunks'] += len(batch_chunks)
                    batch_chunks = []
                
                stats['processed_files'] += 1
                
            except Exception as e:
                stats['failed_files'].append(f"{uploaded_file.name}: {str(e)}")
        
        # Process remaining chunks
        if batch_chunks:
            self._process_batch(batch_chunks)
            stats['total_chunks'] += len(batch_chunks)
        
        stats['processing_time'] = (datetime.now() - start_time).total_seconds()
        
        return stats
    
    def _process_batch(self, chunks: List[Dict]):
        """Process a batch of chunks into ChromaDB"""
        if not chunks:
            return
        
        ids = [chunk['id'] for chunk in chunks]
        documents = [chunk['text'] for chunk in chunks]
        metadatas = [chunk['metadata'] for chunk in chunks]
        
        self.collection.add(
            ids=ids,
            documents=documents,
            metadatas=metadatas
        )
    
    def _get_existing_sources(self) -> set:
        """Get list of already indexed source files"""
        try:
            all_data = self.collection.get(include=["metadatas"])
            if all_data and all_data['metadatas']:
                return set(meta.get('source_file', '') for meta in all_data['metadatas'])
        except:
            pass
        return set()
    
    def build_bm25_index(self) -> PersistentBM25:
        """Build and save BM25 index from ingested documents"""
        bm25 = PersistentBM25()
        bm25.fit(self.bm25_documents)
        bm25.save(Config.BM25_INDEX_PATH)
        return bm25

# ==============================================================================
# RAG GENERATION WITH IMPROVED CONTEXT
# ==============================================================================

class BankingRAGGenerator:
    """Enhanced RAG generation for banking documentation"""
    
    def __init__(self, azure_client):
        self.client = azure_client
    
    def generate_response(self, query: str, context_docs: List[Dict]) -> Dict:
        """Generate response with improved context assembly"""
        
        # Assemble context with source citations
        context_parts = []
        sources = []
        
        for idx, doc in enumerate(context_docs, 1):
            metadata = doc.get('metadata', {})
            source_file = metadata.get('source_file', 'Unknown')
            page = metadata.get('page', 'N/A')
            
            context_parts.append(
                f"[Document {idx} - {source_file}, Page {page}]\n{doc['text']}\n"
            )
            
            if source_file not in sources:
                sources.append(source_file)
        
        context = "\n".join(context_parts)
        
        # Enhanced prompt for banking documentation
        prompt = f"""Vous Ãªtes un assistant RAG spÃ©cialisÃ© dans les systÃ¨mes bancaires, l'architecture de donnÃ©es, et les produits financiers.

Votre rÃ´le est de rÃ©pondre et d'expliquer clairement les informations issues des documents, en les rendant comprÃ©hensibles pour tout type d'utilisateur, du plus gÃ©nÃ©ral au plus technique.

[Contexte]
{context}

[RequÃªte]
{query}

[INSTRUCTIONS GÃ‰NÃ‰RALES]

1. Utilisez exclusivement les informations prÃ©sentes dans le contexte ci-dessus.

2. Fournissez une rÃ©ponse claire, structurÃ©e et explicite, adaptÃ©e Ã  la comprÃ©hension de tout utilisateur.
   - Si le sujet est technique, expliquez les notions en termes simples.
   - Si la question est gÃ©nÃ©rale, donnez une rÃ©ponse complÃ¨te mais concise.
   - Si la question est complexe, dÃ©taillez le raisonnement et le fonctionnement.

3. Chaque fois que vous mentionnez une donnÃ©e, un mÃ©canisme ou un fait:
   - Citez immÃ©diatement la source sous le format: [Source: nom_du_fichier.pdf, Page <numÃ©ro>]

4. Si l'information n'existe pas dans les documents, indiquez-le clairement.

5. Structure attendue:
   a. RÃ©ponse expliquÃ©e: DÃ©taillez le contenu et son interprÃ©tation.
   b. SynthÃ¨se (si utile): RÃ©sumez la logique ou le fonctionnement global.
   c. Sources: Liste complÃ¨te des documents utilisÃ©s.

6. Objectif:
   Rendre la rÃ©ponse Ã  la fois informative, explicative et vÃ©rifiable,
   qu'il s'agisse d'un utilisateur curieux ou d'un expert technique.

[Exemple de format]
Les procÃ©dures de vÃ©rification d'identitÃ©... [Source: procedure_kyc.pdf, Page 12]. 
Selon le chapitre 3... [Source: reglement_financier.pdf, Page 45].

Sources utilisÃ©es:
- procedure_kyc.pdf (Pages 12, 15)
- reglement_financier.pdf (Page 45)
"""
        
        try:
            completion = self.client.chat.completions.create(
                model=Config.AZURE_MODEL_DEPLOYMENT,
                messages=[
                    {"role": "system", "content": "You are an expert assistant for banking system documentation based on RAG (Retrieval-Augmented Generation)."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            answer = completion.choices[0].message.content
            
            return {
                'answer': answer,
                'sources': sources,
                'context_docs': context_docs,
                'num_tokens': completion.usage.total_tokens if hasattr(completion, 'usage') else None
            }
            
        except Exception as e:
            return {
                'answer': f"Error generating response: {str(e)}",
                'sources': [],
                'context_docs': [],
                'num_tokens': None
            }

# ==============================================================================
# STREAMLIT APPLICATION
# ==============================================================================

def initialize_session_state():
    """Initialize Streamlit session state"""
    if 'chroma_client' not in st.session_state:
        st.session_state.chroma_client = chromadb.PersistentClient(path=Config.CHROMA_PERSIST_PATH)
    
    if 'collection' not in st.session_state:
        # Load embedding model
        embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL_PATH)
        
        # Create embedding function
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=Config.EMBEDDING_MODEL_PATH
        )
        
        st.session_state.collection = st.session_state.chroma_client.get_or_create_collection(
            name=Config.COLLECTION_NAME,
            embedding_function=embedding_function,
            metadata={"hnsw:space": "cosine"}
        )
    
    if 'bm25_index' not in st.session_state:
        # Load or create BM25 index
        if os.path.exists(Config.BM25_INDEX_PATH):
            st.session_state.bm25_index = PersistentBM25.load(Config.BM25_INDEX_PATH)
        else:
            st.session_state.bm25_index = PersistentBM25()
    
    if 'chat_sessions' not in st.session_state:
        st.session_state.chat_sessions = {}
    
    if 'current_chat' not in st.session_state:
        st.session_state.current_chat = str(datetime.now())
        st.session_state.chat_sessions[st.session_state.current_chat] = {
            "title": "New Chat",
            "messages": []
        }
    
    if 'processor' not in st.session_state:
        st.session_state.processor = BankingDocumentProcessor()
    
    if 'azure_client' not in st.session_state:
        # Initialize Azure OpenAI client
        http_client = httpx.Client(verify=False)
        st.session_state.azure_client = AzureOpenAI(
            api_version=Config.AZURE_API_VERSION,
            azure_endpoint=Config.AZURE_ENDPOINT,
            api_key=Config.AZURE_API_KEY,
            http_client=http_client
        )
    
    if 'rag_generator' not in st.session_state:
        st.session_state.rag_generator = BankingRAGGenerator(st.session_state.azure_client)

def main():
    st.set_page_config(page_title="BNP Banking RAG Chatbot", layout="wide")
    
    initialize_session_state()
    
    st.title("ğŸ¦ BNP Banking Documentation Chatbot")
    st.caption("Optimized RAG system with hybrid search and batch processing")
    
    # Sidebar for document management
    with st.sidebar:
        st.header("ğŸ“ Document Management")
        
        # Get collection stats
        try:
            collection_count = st.session_state.collection.count()
            st.metric("Indexed Documents", collection_count)
        except:
            collection_count = 0
            st.metric("Indexed Documents", "0")
        
        # Batch upload
        st.subheader("Upload Documents")
        uploaded_files = st.file_uploader(
            "Upload multiple documents",
            type=["pdf", "txt", "docx", "csv", "xlsx"],
            accept_multiple_files=True
        )
        
        if uploaded_files and st.button("ğŸ“¤ Process All Files"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(current, total, message):
                progress = current / total
                progress_bar.progress(progress)
                status_text.text(message)
            
            with st.spinner("Processing documents..."):
                batch_ingestion = BatchDocumentIngestion(
                    st.session_state.collection,
                    st.session_state.processor
                )
                
                stats = batch_ingestion.ingest_documents(
                    uploaded_files,
                    progress_callback=update_progress
                )
                
                # Rebuild BM25 index
                status_text.text("Building BM25 index...")
                st.session_state.bm25_index = batch_ingestion.build_bm25_index()
                
                # Display results
                st.success(f"""
                âœ… Processing complete!
                - Files processed: {stats['processed_files']}/{stats['total_files']}
                - Total chunks: {stats['total_chunks']}
                - Time: {stats['processing_time']:.2f}s
                """)
                
                if stats['failed_files']:
                    st.error(f"Failed files: {', '.join(stats['failed_files'])}")
        
        st.divider()
        
        # Chat management
        st.subheader("ğŸ’¬ Chat Sessions")
        if st.button("â• New Chat"):
            new_chat_id = str(datetime.now())
            st.session_state.current_chat = new_chat_id
            st.session_state.chat_sessions[new_chat_id] = {
                "title": "New Chat",
                "messages": []
            }
            st.rerun()
        
        # List existing chats
        for chat_id, chat_data in list(st.session_state.chat_sessions.items()):
            if st.button(f"ğŸ“ {chat_data['title'][:30]}", key=f"chat_{chat_id}"):
                st.session_state.current_chat = chat_id
                st.rerun()
    
    # Main chat interface
    current_chat = st.session_state.chat_sessions.get(st.session_state.current_chat, {
        "title": "New Chat",
        "messages": []
    })
    
    # Display chat messages
    for message in current_chat['messages']:
        with st.chat_message(message['role']):
            st.markdown(message['content'])
            
            # Show sources if available
            if message['role'] == 'assistant' and 'sources' in message:
                with st.expander("ğŸ“š Sources"):
                    for source in message['sources']:
                        st.text(f"â€¢ {source}")
    
    # Chat input
    if prompt := st.chat_input("Ask about banking documentation..."):
        # Add user message
        current_chat['messages'].append({
            "role": "user",
            "content": prompt,
            "timestamp": str(datetime.now())
        })
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("ğŸ” Searching documentation..."):
                # Create hybrid retriever
                retriever = HybridRetriever(
                    st.session_state.collection,
                    st.session_state.bm25_index
                )
                
                # Retrieve relevant documents
                retrieved_docs = retriever.retrieve(prompt, k=Config.FINAL_RESULTS_K)
                
                # Generate response
                response = st.session_state.rag_generator.generate_response(
                    prompt,
                    retrieved_docs
                )
                
                # Display answer
                st.markdown(response['answer'])
                
                # Show sources
                if response['sources']:
                    with st.expander("ğŸ“š Sources"):
                        for source in response['sources']:
                            st.text(f"â€¢ {source}")
                
                # Add to chat history
                current_chat['messages'].append({
                    "role": "assistant",
                    "content": response['answer'],
                    "sources": response['sources'],
                    "timestamp": str(datetime.now())
                })
                
                # Update chat title if first message
                if current_chat['title'] == "New Chat" and len(current_chat['messages']) > 0:
                    current_chat['title'] = prompt[:50] + "..."

if __name__ == "__main__":
    main()







Nouvelle slide â€“ Public cible
Ã€ qui sâ€™adresse la solution RAG BNP ED ?
RAG BNP ED a Ã©tÃ© dÃ©veloppÃ©e pour accompagner diffÃ©rents profils impliquÃ©s dans lâ€™exploitation et la transformation du systÃ¨me dâ€™information :
ğŸ‘¥ Ã‰quipes mÃ©tiers
AccÃ¨s rapide et fiable Ã  la documentation ATLAS
Recherche simplifiÃ©e sans connaissance technique
RÃ©ponses claires, sourcÃ©es et contextualisÃ©es
ğŸ—ï¸ Ã‰quipes IT / DSI
ComprÃ©hension de lâ€™architecture de la documentation ATLAS
AccÃ¨s structurÃ© aux rÃ©fÃ©rentiels et aux dÃ©pendances documentaires
Support Ã  lâ€™analyse et Ã  la maintenance du SI
ğŸ”„ Ã‰quipes Transformation du SystÃ¨me dâ€™Information
Accompagnement des projets de migration et dâ€™Ã©volution du SI
Vision transverse de la documentation existante
RÃ©duction du temps de recherche et dâ€™analyse documentaire
ğŸ’¡ Variante plus synthÃ©tique (si tu veux une slide trÃ¨s lÃ©gÃ¨re)
RAG BNP ED sâ€™adresse aux Ã©quipes mÃ©tiers, Ã  la DSI et aux acteurs de la transformation du systÃ¨me dâ€™information, en facilitant lâ€™accÃ¨s, la comprÃ©hension et lâ€™exploitation de la documentation ATLAS.














RAG BNP ED est un chatbot intelligent basÃ© sur la mÃ©thode RAG (Retrieval Augmented Generation) et sur une base de connaissances centralisÃ©e contenant la documentation ATLAS.
Il permet une recherche conversationnelle rapide et fiable, avec des rÃ©ponses systÃ©matiquement sourcÃ©es Ã  partir des documents officiels.
La solution Ã©volue automatiquement avec les nouvelles documentations, garantissant des rÃ©ponses toujours Ã  jour.






RAG BNP ED est un chatbot intelligent basÃ© sur une base de connaissances centralisÃ©e (documentation ATLAS).
Il permet une recherche conversationnelle rapide et fiable, avec des rÃ©ponses systÃ©matiquement sourcÃ©es.
La solution Ã©volue automatiquement avec les nouvelles documentations, garantissant des rÃ©ponses toujours Ã  jour.








La documentation ATLAS est volumineuse et critique pour les Ã©quipes mÃ©tiers
Lâ€™accÃ¨s Ã  lâ€™information repose aujourdâ€™hui sur une recherche manuelle chronophage
Objectif : faciliter lâ€™accÃ¨s rapide, fiable et sourcÃ© Ã  lâ€™information interne via un outil intelligent




Bonjour,

Pourriez-vous sâ€™il vous plaÃ®t procÃ©der Ã  la rÃ©initialisation de mon mot de passe ? Je vous remercie par avance.

Cordialement,
Nihad Senhadji


Bonjour Ã  tous,

Suite Ã  notre prÃ©cÃ©dent Ã©change, je vous informe que nous avons pu identifier et renseigner les dates dâ€™impayÃ©s en nous basant sur lâ€™extraction transmise par Chaima.

Toutefois, 11 cas restent pour lesquels les dates dâ€™impayÃ©s nâ€™ont pas pu Ãªtre renseignÃ©es, malgrÃ© lâ€™utilisation de cette extraction. Ces cas demeurent donc identifiÃ©s comme anomalies.

Je reste Ã  votre disposition pour toute prÃ©cision complÃ©mentaire ou pour Ã©changer sur ces cas si besoin.

Cordialement,
Nihad Senhadji
GA-15








Bonjour Ã  tous,

Veuillez trouver ci-joint la nouvelle version du fichier Ã‰tat Central des Engagements arrÃªtÃ© au 31/12/2025.

Lors du traitement, nous avons constatÃ© que certaines dates dâ€™impayÃ©s nâ€™ont pas Ã©tÃ© retrouvÃ©es dans les donnÃ©es dâ€™extraction disponibles.
Pour cette raison, une nouvelle feuille a Ã©tÃ© ajoutÃ©e au fichier, intitulÃ©e :

Â« Ã‰tat des engagements â€“ Anomalies dates dâ€™impayÃ©s Â»

Cette feuille regroupe lâ€™ensemble des engagements pour lesquels les dates dâ€™impayÃ©s nâ€™ont pas pu Ãªtre identifiÃ©es Ã  partir de lâ€™extraction.

Le fichier contient dÃ©sormais les feuilles suivantes :

1. Ã‰tat Central des Engagements
2. Ã‰tat des engagements â€“ Anomalies dates dâ€™impayÃ©s
3. Ã‰tat dÃ©taillÃ© par chapitre
4. Rapprochement des crÃ©ances â€“ Bilan
5. Rapprochement des crÃ©ances hors bilan
6. Rapprochement CR / AR / CB
7. Reporting social BG
8. Ã‰tat dÃ©taillÃ© par ratios prudentiels
9. Risques â€“ Rapprochement RP
10. Risques â€“ Grands Risques
11. Risques â€“ Grands Risques par entitÃ©
12. Risques â€“ Annexe 9000
13. Rapprochement reporting provisions
14. Rapprochement affectation des garanties
15. Ã‰tat des garanties

Je reste Ã  votre disposition pour toute information complÃ©mentaire ou clarification si besoin.

Cordialement,
Nihad Senhadji
GA-15







Bien sÃ»r ! Voici une version rÃ©Ã©crite, fluide, professionnelle et claire de ton email :


---

Objet : MÃ©thodologie â€“ DÃ©tection de communautÃ©s de fraude (AI Sentinilia)

Bonjour Ryma,

Dans le cadre du projet AI Sentinilia dÃ©diÃ© Ã  la dÃ©tection de communautÃ©s de fraude, la mÃ©thodologie utilisÃ©e repose sur les critÃ¨res suivants :

Les liens entre clients sont identifiÃ©s Ã  partir de coÃ¯ncidences lorsquâ€™ils effectuent des retraits :

Ã  la mÃªme date ;

au mÃªme GAB (ATM) ;

pour un montant supÃ©rieur ou Ã©gal Ã  500 euros ;

avec une frÃ©quence dâ€™au moins 5 retraits entre deux clients ou plus.



Lorsque ces conditions sont remplies, les clients sont considÃ©rÃ©s comme liÃ©s, car leurs comportements de retrait prÃ©sentent des similitudes rÃ©pÃ©tÃ©es et inhabituelles.

Afin de dÃ©tecter les communautÃ©s de fraude, nous utilisons un algorithme appelÃ© Louvain, qui permet de regrouper automatiquement les clients ayant des comportements similaires. Chaque communautÃ© ainsi identifiÃ©e reprÃ©sente un groupe potentiel de fraude, basÃ© sur des comportements de retraits coordonnÃ©s.

Cette solution nâ€™a pas Ã©tÃ© modifiÃ©e depuis sa mise en place, et le suivi Ã©tant trimestriel, nous enverrons les prochains rÃ©sultats en fÃ©vrier 2026.

Je reste Ã  votre disposition pour toute information complÃ©mentaire ou clarification si nÃ©cessaire.

Cordialement,
Nihad Senhadji


---

Si tu veux, je peux aussi te faire une version encore plus courte et simple, parfaite pour des non-techniques, qui tient en 5-6 phrases.
Veuxâ€‘tu que je fasse Ã§aâ€¯?











Comme le suivi est trimestriel, nous enverrons les prochains rÃ©sultats en fÃ©vrier 2025.


Cet algorithme nâ€™a pas Ã©tÃ© modifiÃ© depuis sa mise en place


Afin de dÃ©tecter les communautÃ©s de fraude, nous utilisons un algorithme appelÃ© Louvain, qui permet de regrouper automatiquement les clients ayant des comportements similaires.


Bonjour,

Dans le cadre du projet AI Sentinilia dÃ©diÃ© Ã  la dÃ©tection de communautÃ©s de fraude, la mÃ©thodologie utilisÃ©e repose sur les Ã©tapes suivantes :

- Les liens entre clients sont dâ€™abord identifiÃ©s Ã  partir des critÃ¨res suivants :

   - la localisation du GAB (ATM) oÃ¹ les retraits ont Ã©tÃ© effectuÃ©s ;
   - la frÃ©quence des retraits ;
   - des coÃ¯ncidences entre plusieurs clients lorsque les retraits ont eu lieu :
      - Ã  la mÃªme date,
      - au mÃªme GAB,
      - avec un montant supÃ©rieur ou Ã©gal Ã  500 euros,
      - et ce au moins 5 fois entre deux clients ou plus.

- Lorsque ces conditions sont rÃ©unies, les clients sont considÃ©rÃ©s comme liÃ©s, car leurs comportements de retrait prÃ©sentent des similitudes rÃ©pÃ©tÃ©es et inhabituelles.

- La mÃ©thode de Louvain est ensuite utilisÃ©e pour regrouper automatiquement les clients liÃ©s :

   - elle permet de constituer des communautÃ©s de clients fortement connectÃ©s ;
   - chaque communautÃ© reprÃ©sente un groupe potentiel de fraude, basÃ© sur des comportements de retraits coordonnÃ©s.

Je reste Ã  votre disposition pour toute information complÃ©mentaire ou clarification si nÃ©cessaire.

Cordialement,

Nihad Senhadji



















Bonjour,

Je vous informe que jâ€™ai annulÃ© la version du fichier dÃ©jÃ  envoyÃ©e prÃ©cÃ©demment.
Nous sommes actuellement en train de finaliser la nouvelle version, et je vous lâ€™enverrai dÃ¨s que ce sera terminÃ©.

Merci pour votre comprÃ©hension.

Cordialement,
Nihad Senhadji

Bonjour,

Veuillez trouver ci-joint le fichier demandÃ©, comme convenu.

Nous vous enverrons la version finale, contenant lâ€™ensemble des feuilles, dans la soirÃ©e.

Cordialement,
Nihad Senhadji


Les colonnes du fichier fournissent toutes les informations nÃ©cessaires â€” identitÃ©, dates, lieux et rÃ©fÃ©rences clients â€” afin de faciliter et structurer lâ€™analyse des Ã©quipes sur ces cas sensibles.


La deuxiÃ¨me feuille, avec un score de similaritÃ© infÃ©rieur Ã  95 %, correspond aux cas oÃ¹ le modÃ¨le dâ€™IA a permis de rÃ©duire une grande partie de la recherche manuelle.
Dans cette feuille, tous les noms qui ne figurent pas dans notre base clients Atlas sont automatiquement Ã©cartÃ©s, ce qui Ã©limine les faux positifs et allÃ¨ge considÃ©rablement le travail des Ã©quipes opÃ©rationnelles.
ğŸ”¹ La troisiÃ¨me feuille, avec un score de similaritÃ© supÃ©rieur Ã  95 %, regroupe les noms nÃ©cessitant une analyse approfondie par lâ€™Ã©quipe opÃ©rationnelle, car ils prÃ©sentent une forte probabilitÃ© de correspondance.










ğŸ¬ Script vidÃ©o â€“ PrÃ©sentation de Decad AI Solution

> Bonjour,

Dans cette vidÃ©o, je vais vous prÃ©senter Decad AI Solution, accessible via un lien de web application Domino.

On commence par lâ€™insertion du fichier DECAD, envoyÃ© par la Banque dâ€™AlgÃ©rie, directement dans lâ€™application.

Ensuite, un simple clic permet de lancer le traitement. La solution sâ€™appuie sur un modÃ¨le dâ€™intelligence artificielle exÃ©cutÃ© en arriÃ¨re-plan, avec un temps dâ€™exÃ©cution dâ€™environ 10 minutes.

Une fois le traitement terminÃ©, on clique sur â€œRÃ©sultatsâ€ afin de tÃ©lÃ©charger le fichier Excel de sortie, comme vous pouvez le voir Ã  lâ€™Ã©cran.

Le fichier Excel gÃ©nÃ©rÃ© contient trois feuilles distinctes :

ğŸ”¹ La premiÃ¨re feuille, appelÃ©e PiÃ¨ce dâ€™audit, regroupe les rÃ©sultats avec des scores de similaritÃ© infÃ©rieurs et supÃ©rieurs Ã  95 %.

ğŸ”¹ La deuxiÃ¨me feuille contient les rÃ©sultats avec un score infÃ©rieur Ã  95 %, correspondant aux cas non identifiÃ©s par le modÃ¨le dâ€™IA.

ğŸ”¹ La troisiÃ¨me feuille regroupe les rÃ©sultats avec un score supÃ©rieur Ã  95 %, qui reprÃ©sentent les cas prioritaires Ã  analyser par lâ€™Ã©quipe opÃ©rationnelle.

Il est important de noter que les trois feuilles contiennent exactement les mÃªmes colonnes.

Voici la liste des colonnes suivantes :

COLONNE 1 et 2 DECAD

Nom interdits chequiers

Raison sociale base atlas

Nom abrÃ©gÃ© tiers

Date naissance interdits chequiers ET DE NOTRE BASE ATLAS

Date de crÃ©ation (entreprise)

Date naissance Match

Date de crÃ©ation Match

Lieu naissance interdits chequier ET DE LA BASE atlas

Lieu naissance

Lieu naissance Match

Date d'effet et de levÃ©e

Compte client

Code catÃ©gorie client

Id tiers

Score de similaritÃ©


GrÃ¢ce Ã  cette structuration, Decad AI Solution permet de rÃ©duire le temps de traitement, de prioriser les analyses, et de sÃ©curiser le processus de contrÃ´le grÃ¢ce Ã  lâ€™intelligence artificielle.

Merci pour votre attention.




---

If you want, I can also make a version more â€œoral / dynamiqueâ€, so it sounds natural when you speak it in the video, like a real demo narration.

Do you want me to do that?



















Bonjour,

Merci pour votre message et pour la transmission de lâ€™Expression de Besoin (EDB) relative Ã  lâ€™automatisation du processus de traitement des agios rÃ©servÃ©s et de leurs reprises liÃ©es aux clients douteux â€“ pÃ©rimÃ¨tre EPS.

Suite Ã  lâ€™analyse de cette EDB et conformÃ©ment aux Ã©lÃ©ments partagÃ©s, je vous prie de trouver en piÃ¨ce jointe les rÃ©sultats de lâ€™APA relatifs au mois de dÃ©cembre 2025.

Ces rÃ©sultats correspondent uniquement au traitement du mois de dÃ©cembre 2025 et constituent la sortie actuelle de lâ€™APA Ã  ce stade.
Le fichier Excel de rÃ©sultats contient Ã  la fois des clients sains et des clients douteux.

Ils sont le fruit de plusieurs ateliers de travail menÃ©s avec Mouna, au cours desquels nous avons rÃ©alisÃ© de nombreuses vÃ©rifications manuelles, ainsi que des rapprochements dÃ©taillÃ©s entre les rÃ©sultats APA et les rÃ©sultats manuels du mois dâ€™octobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les rÃ¨gles de gestion appliquÃ©es dans le cadre de cette APA, en cohÃ©rence avec les besoins fonctionnels et techniques dÃ©crits dans lâ€™EDB et avec la recommandation de la Banque dâ€™AlgÃ©rie visant Ã  remplacer le traitement manuel actuellement rÃ©alisÃ© par lâ€™Ã©quipe DRAC.

Il sâ€™agit actuellement dâ€™une phase de test. Il est proposÃ© de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilitÃ© du dispositif et de permettre la validation dÃ©finitive des rÃ©sultats par vos soins.

Par ailleurs, lâ€™APA sera livrÃ©e la semaine prochaine afin dâ€™Ãªtre dÃ©ployÃ©e et exÃ©cutÃ©e sur les postes des utilisateurs concernÃ©s.
Ã€ cet effet, nous aurons besoin :

- de lâ€™installation de lâ€™APA sur les postes utilisateurs,
- ainsi que de la mise Ã  disposition dâ€™un lien dâ€™accÃ¨s vers lâ€™emplacement de dÃ©pÃ´t des fichiers de sortie.

Je reste bien entendu Ã  votre disposition, ainsi quâ€™Amina, pour tout complÃ©ment dâ€™information ou si vous souhaitez planifier un point dâ€™Ã©change.

Cordialement,
Nihad Senhadji

---

If you want, I can now:

- make a very formal / regulatory version (signatureâ€“engagement context), or
- shorten it for a manager-level reply.












Le fichier Excel de rÃ©sultats contient Ã  la fois des clients sains et des clients douteux


Bonjour,

Merci pour votre message et pour les Ã©lÃ©ments transmis lors de notre Ã©change de jeudi.

Suite Ã  lâ€™analyse de lâ€™expression de besoin relative Ã  lâ€™automatisation du traitement des agios rÃ©servÃ©s leasing, je vous prie de trouver en piÃ¨ce jointe les rÃ©sultats de lâ€™APA relatifs au mois de dÃ©cembre 2025.

Ces rÃ©sultats correspondent uniquement au traitement du mois de dÃ©cembre 2025 et constituent la sortie actuelle de lâ€™APA Ã  ce stade.

Ils sont le fruit de plusieurs ateliers de travail menÃ©s avec Mouna, au cours desquels nous avons rÃ©alisÃ© de nombreuses vÃ©rifications manuelles, ainsi que des rapprochements dÃ©taillÃ©s entre les rÃ©sultats APA et les rÃ©sultats manuels du mois dâ€™octobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les rÃ¨gles de gestion appliquÃ©es dans le cadre de cette APA.

Il sâ€™agit actuellement dâ€™une phase de test. Il est proposÃ© de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilitÃ© du dispositif et de permettre la validation dÃ©finitive des rÃ©sultats par vos soins.

Par ailleurs, lâ€™APA sera livrÃ©e la semaine prochaine afin dâ€™Ãªtre dÃ©ployÃ©e et exÃ©cutÃ©e sur les postes des utilisateurs concernÃ©s.
Ã€ cet effet, nous aurons besoin :

- de lâ€™installation de lâ€™APA sur les postes utilisateurs,
- ainsi que de la mise Ã  disposition dâ€™un lien dâ€™accÃ¨s vers lâ€™emplacement de dÃ©pÃ´t des fichiers de sortie.

Je reste bien entendu Ã  votre disposition pour tout complÃ©ment dâ€™information, retour ou ajustement nÃ©cessaire.

Cordialement,
Nihad Senhadji

---

If you want, I can also adapt the wording to:

- explicitly request






Bonjour,

Merci pour votre message et pour les Ã©lÃ©ments transmis lors de notre Ã©change de jeudi.

Suite Ã  lâ€™analyse de lâ€™expression de besoin relative Ã  lâ€™automatisation du traitement des agios rÃ©servÃ©s leasing, je vous prie de trouver en piÃ¨ce jointe les rÃ©sultats de lâ€™APA.

Ces rÃ©sultats correspondent uniquement au traitement du mois en cours et constituent la sortie actuelle de lâ€™APA Ã  ce stade.

Ils sont le fruit de plusieurs ateliers de travail menÃ©s avec Mouna, au cours desquels nous avons rÃ©alisÃ© de nombreuses vÃ©rifications manuelles, ainsi que des rapprochements dÃ©taillÃ©s entre les rÃ©sultats APA et les rÃ©sultats manuels du mois dâ€™octobre 2025, afin de valider les approches retenues.

Vous trouverez ci-dessous les rÃ¨gles de gestion appliquÃ©es dans le cadre de cette APA.

Il sâ€™agit actuellement dâ€™une phase de test. Il est proposÃ© de fonctionner selon ce mode sur les prochains mois, afin de confirmer la fiabilitÃ© du dispositif et de permettre la validation dÃ©finitive des rÃ©sultats par vos soins.

Par ailleurs, lâ€™APA sera livrÃ©e la semaine prochaine afin dâ€™Ãªtre dÃ©ployÃ©e et exÃ©cutÃ©e sur les postes des utilisateurs concernÃ©s.
Ã€ cet effet, nous aurons besoin :

- de lâ€™installation de lâ€™APA sur les postes utilisateurs,
- ainsi que de la mise Ã  disposition dâ€™un lien dâ€™accÃ¨s vers lâ€™emplacement de dÃ©pÃ´t des fichiers de sortie.

Je reste bien entendu Ã  votre disposition pour tout complÃ©ment dâ€™information, retour ou ajustement nÃ©cessaire.

Cordialement,
Nihad Senhadji

---

If you want, I can also:

- slightly strengthen the request for validation, or
- adapt the wording to a more regulatory / audit-compliant style for DRAC.






Bonjour [Nom du manager],

Je souhaite vous partager un point de situation concernant lâ€™avancement des projets APA.

1. Avancement gÃ©nÃ©ral (APA ESP & APA ArrÃªtÃ©)
Suite aux diffÃ©rents workshops organisÃ©s avec la DRAC, lâ€™ensemble des rÃ¨gles de gestion a Ã©tÃ© confirmÃ© et validÃ© pour les deux projets.
Un rapprochement a Ã©galement Ã©tÃ© rÃ©alisÃ© sur la base de leurs fichiers de novembre 2025.

2. Projet APA ESP
Un seul point reste identifiÃ© : un problÃ¨me de code fusion concernant un compte unique.
La DRAC mâ€™a confirmÃ© que ce point est dÃ©jÃ  portÃ© Ã  votre connaissance.

3. Projet APA ArrÃªtÃ©
Un seul point est actuellement en suspens :

- Un Ã©cart liÃ© au solde du compte Ã  vue.
Ã€ ce stade, la DRAC nâ€™est pas en mesure dâ€™identifier la mÃ©thode ou la source utilisÃ©e pour le calcul de ce solde et doit effectuer des vÃ©rifications de son cÃ´tÃ© avant de nous apporter un retour dÃ©finitif.

4. Projet APA Leasing
Le projet APA Leasing nâ€™a pas encore Ã©tÃ© dÃ©marrÃ© Ã  ce stade.

Vous trouverez ci-dessous le lien regroupant les Ã©lÃ©ments pour les projets APA ESP et APA ArrÃªtÃ© :
[Lien Ã  insÃ©rer]

Je reste Ã  votre disposition pour toute prÃ©cision complÃ©mentaire ou pour un point de suivi.

Cordialement,
Nihad Senhadji






your thanks:

"Je tiens Ã  vous remercier pour votre accompagnement et vos conseils tout au long de lâ€™annÃ©e, et je souhaite continuer Ã  contribuer activement Ã  la rÃ©ussite des objectifs de la Banque, tout en participant aux projets IA et mÃ©tiers."

Itâ€™s professional, appreciative, and forward-looking.

I can also make a slightly shorter, sharper version if you want it for an email reply.

You can write it like this:

"Je souhaite continuer Ã  contribuer activement Ã  la rÃ©ussite des objectifs de la Banque, tout en participant aux projets IA et mÃ©tiers."

Itâ€™s concise, professional, and inclusive of both AI and other banking projects.

If you want, I can also suggest 2â€“3 alternative formulations that sound slightly more executive.


Understood. We need a leadership-oriented tone without implying you want to replace himâ€”showing responsibility, initiative, and team support while still highlighting AI. Hereâ€™s a refined version:


---

Bonjour [Nom du manager],

Merci pour votre retour et pour lâ€™Ã©valuation de ma contribution en 2025. Jâ€™ai bien pris en compte les axes dâ€™amÃ©lioration identifiÃ©s, notamment sur le cadrage des besoins mÃ©tiers, la structuration des travaux et la clartÃ© de la communication.

Je souhaite exprimer mon objectif professionnel, qui est dâ€™Ã©voluer vers un rÃ´le sur des projets en Intelligence Artificielle, avec des responsabilitÃ©s clairement dÃ©finies et des livrables Ã  impact opÃ©rationnel.

Je reste par ailleurs pleinement engagÃ©e Ã  soutenir lâ€™ensemble des projets bancaires et Ã  accompagner lâ€™Ã©quipe dans lâ€™organisation et la coordination des travaux, afin de garantir la qualitÃ© et lâ€™efficacitÃ© des livrables, tout en contribuant activement Ã  la rÃ©ussite des projets IA et mÃ©tiers.

Cordialement,
Nihad Senhadji


---

This version:

Shows initiative and readiness to support the team

Positions you for AI projects

Avoids sounding like you want to replace him


If you want, I can make an even snappier, executive-style version that fits in one paragraph. Do you want me to do that?
