
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè¶ Bank Information System RAG Chatbot (Weaviate + Azure OpenAI, LangChain)\n",
    "\n",
    "This notebook demonstrates a full Retrieval Augmented Generation (RAG) workflow tailored for banking information system documentation, using **Weaviate** as the vector database and **Azure OpenAI** for LLM/embeddings, with French-friendly prompts."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# -- 1. Install required packages (run only once)\n",
    "!pip install python-dotenv langchain langchain-openai langchain-community weaviate-client[embedded] unstructured[all-docs] pypdf sentence-transformers rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment & Config\n",
    "Set up your secrets!\n",
    "- Create a `.env` file in your project with your Azure OpenAI and Weaviate credentials."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "REQUIRED_VARS = [\n",
    "    'AZURE_OPENAI_API_KEY',\n",
    "    'AZURE_OPENAI_ENDPOINT',\n",
    "    'AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME',\n",
    "    'AZURE_OPENAI_DEPLOYMENT_NAME'\n",
    "]\n",
    "\n",
    "for var in REQUIRED_VARS:\n",
    "    if not os.getenv(var):\n",
    "        raise EnvironmentError(f\"Missing env variable: {var}\")\n",
    "# Optionally set Weaviate config if connecting to a remote instance. For embedded test, skip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {}, 
   "source": [ "## 3. Document Ingestion (PDF/CSV/XLSX)\n", "Load your files from structured folders." ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader, CSVLoader, UnstructuredExcelLoader\n",
    "import glob\n",
    "\n",
    "DOCS_DIR = \"documents\"\n",
    "\n",
    "def load_documents():\n",
    "    documents = []\n",
    "    source_metadata = {}\n",
    "    \n",
    "    # PDFs from ./documents/pdfs\n",
    "    if os.path.exists(os.path.join(DOCS_DIR, \"pdfs\")):\n",
    "        for file in glob.glob(os.path.join(DOCS_DIR, \"pdfs\", \"*.pdf\")):\n",
    "            try:\n",
    "                loader = PyPDFDirectoryLoader(os.path.dirname(file))\n",
    "                docs = loader.load()\n",
    "                file_name = os.path.basename(file)\n",
    "                \n",
    "                # Add source metadata to each document\n",
    "                for doc in docs:\n",
    "                    doc.metadata[\"source_file\"] = file_name\n",
    "                    doc.metadata[\"source_type\"] = \"pdf\"\n",
    "                \n",
    "                documents.extend(docs)\n",
    "                print(f\"Loaded {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "    # CSVs from ./documents/csv\n",
    "    csv_dir = os.path.join(DOCS_DIR, \"csv\")\n",
    "    if os.path.exists(csv_dir):\n",
    "        for file in glob.glob(os.path.join(csv_dir, '*.csv')):\n",
    "            try:\n",
    "                docs = CSVLoader(file_path=file).load()\n",
    "                file_name = os.path.basename(file)\n",
    "                \n",
    "                for doc in docs:\n",
    "                    doc.metadata[\"source_file\"] = file_name\n",
    "                    doc.metadata[\"source_type\"] = \"csv\"\n",
    "                    \n",
    "                documents.extend(docs)\n",
    "                print(f\"Loaded {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "    # Excels from ./documents/excel\n",
    "    xls_dir = os.path.join(DOCS_DIR, \"excel\")\n",
    "    if os.path.exists(xls_dir):\n",
    "        for file in glob.glob(os.path.join(xls_dir, '*.xls*')):\n",
    "            try:\n",
    "                docs = UnstructuredExcelLoader(file_path=file).load()\n",
    "                file_name = os.path.basename(file)\n",
    "                \n",
    "                for doc in docs:\n",
    "                    doc.metadata[\"source_file\"] = file_name\n",
    "                    doc.metadata[\"source_type\"] = \"excel\"\n",
    "                    \n",
    "                documents.extend(docs)\n",
    "                print(f\"Loaded {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents in total\")\n",
    "    return documents\n",
    "\n",
    "docs = load_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [ "## 4. Semantic Text Splitting" ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Function for semantic text splitting\n",
    "def split_documents_semantic(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split documents into semantically coherent chunks\"\"\"\n",
    "    # First get basic chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"]\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Re-group semantically similar chunks\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        print(f\"Loaded semantic model: {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading semantic model: {e}\")\n",
    "        # Fall back to regular chunking\n",
    "        return chunks\n",
    "    \n",
    "    # Group similar chunks using embeddings similarity\n",
    "    grouped_chunks = []\n",
    "    current_group = []\n",
    "    last_embedding = None\n",
    "    similarity_threshold = 0.75\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Skip empty chunks\n",
    "        if not chunk.page_content.strip():\n",
    "            continue\n",
    "            \n",
    "        # Get embedding for this chunk\n",
    "        current_embedding = model.encode(chunk.page_content)\n",
    "        \n",
    "        # Start a new group if we don't have one yet\n",
    "        if not current_group:\n",
    "            current_group.append(chunk)\n",
    "            last_embedding = current_embedding\n",
    "            continue\n",
    "            \n",
    "        # Calculate similarity with the previous chunk\n",
    "        similarity = np.dot(current_embedding, last_embedding) / (np.linalg.norm(current_embedding) * np.linalg.norm(last_embedding))\n",
    "        \n",
    "        # If similar enough and combined length not too large, add to current group\n",
    "        combined_length = sum(len(c.page_content) for c in current_group) + len(chunk.page_content)\n",
    "        if similarity > similarity_threshold and combined_length < chunk_size * 1.5:\n",
    "            current_group.append(chunk)\n",
    "            # Update last embedding as average of group\n",
    "            group_embeddings = model.encode([c.page_content for c in current_group])\n",
    "            last_embedding = np.mean(group_embeddings, axis=0)\n",
    "        else:\n",
    "            # Combine current group into a single document and add to results\n",
    "            if current_group:\n",
    "                combined_text = \" \".join([c.page_content for c in current_group])\n",
    "                combined_metadata = current_group[0].metadata.copy()\n",
    "                combined_metadata[\"chunk_sources\"] = [c.metadata.get(\"source_file\", \"unknown\") for c in current_group]\n",
    "                grouped_chunks.append(Document(page_content=combined_text, metadata=combined_metadata))\n",
    "                \n",
    "            # Start a new group with this chunk\n",
    "            current_group = [chunk]\n",
    "            last_embedding = current_embedding\n",
    "    \n",
    "    # Add the last group if it exists\n",
    "    if current_group:\n",
    "        combined_text = \" \".join([c.page_content for c in current_group])\n",
    "        combined_metadata = current_group[0].metadata.copy()\n",
    "        combined_metadata[\"chunk_sources\"] = [c.metadata.get(\"source_file\", \"unknown\") for c in current_group]\n",
    "        grouped_chunks.append(Document(page_content=combined_text, metadata=combined_metadata))\n",
    "    \n",
    "    print(f\"Initial chunking: {len(chunks)} chunks\")\n",
    "    print(f\"After semantic grouping: {len(grouped_chunks)} chunks\")\n",
    "    return grouped_chunks\n",
    "\n",
    "# Apply the semantic splitter\n",
    "chunks = split_documents_semantic(docs)\n",
    "print(f\"Split into {len(chunks)} final chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [ "## 5. Weaviate Vector Store Setup + Embeddings (Azure OpenAI)" ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We use embedded Weaviate for testing. If you want remote, set the proper config env.\n",
    "import weaviate\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Weaviate as LCWeaviate\n",
    "\n",
    "WEAVIATE_CLASS = \"BankDocChunk\"\n",
    "BANK_DB_STATE_PATH = \"bank_rag_state.json\"\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    openai_api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "# Create embedded Weaviate client\n",
    "client = weaviate.Client(\"embedded\")\n",
    "\n",
    "# Clean up any previous test schema/classes\n",
    "if client.schema.exists(WEAVIATE_CLASS):\n",
    "    client.schema.delete_class(WEAVIATE_CLASS)\n",
    "\n",
    "# Add a class (collection) for chunks with metadata fields\n",
    "client.schema.create_class({\n",
    "    \"class\": WEAVIATE_CLASS,\n",
    "    \"vectorizer\": \"none\",\n",
    "    \"properties\": [\n",
    "        {\"name\": \"text\", \"dataType\": [\"text\"]},\n",
    "        {\"name\": \"source_file\", \"dataType\": [\"string\"]},\n",
    "        {\"name\": \"source_type\", \"dataType\": [\"string\"]},\n",
    "        {\"name\": \"page\", \"dataType\": [\"int\"]},\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(f\"Weaviate schema: {[x['class'] for x in client.schema.get()['classes']]}\")\n",
    "\n",
    "# Use LangChain's Weaviate wrapper for RAG\n",
    "vectorstore = LCWeaviate.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    client=client,\n",
    "    by_text=True,\n",
    "    text_key=\"text\",\n",
    "    class_name=WEAVIATE_CLASS,\n",
    ")\n",
    "\n",
    "print(f\"Inserted {vectorstore._client.query.aggregate(WEAVIATE_CLASS).with_meta_count().do()['data']['Aggregate'][WEAVIATE_CLASS][0]['meta']['count']} vector chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [ "## 6. Free Custom Re-ranker Using BM25 Instead of Cohere" ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import pickle\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "\n",
    "class BM25Reranker:\n",
    "    \"\"\"A document reranker using BM25 algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, k: int = 4):\n",
    "        \"\"\"Initialize with the number of documents to return\"\"\"\n",
    "        self.k = k\n",
    "        self.tokenized_corpus = None\n",
    "        self.bm25 = None\n",
    "        self.document_map = {}\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Fit the BM25 model on the corpus\"\"\"\n",
    "        tokenized_corpus = [doc.page_content.lower().split() for doc in documents]\n",
    "        self.document_map = {i: doc for i, doc in enumerate(documents)}\n",
    "        self.tokenized_corpus = tokenized_corpus\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        return self\n",
    "    \n",
    "    def rerank(self, query: str, documents):\n",
    "        \"\"\"Rerank documents based on BM25 scores\"\"\"\n",
    "        if not self.bm25:\n",
    "            self.fit(documents)\n",
    "        \n",
    "        # Tokenize query\n",
    "        tokenized_query = query.lower().split()\n",
    "        \n",
    "        # Create a map from content to original document to handle duplicates\n",
    "        doc_lookup = {}\n",
    "        for i, doc in enumerate(documents):\n",
    "            doc_lookup[doc.page_content] = i\n",
    "        \n",
    "        # Get BM25 scores\n",
    "        tokenized_docs = [doc.page_content.lower().split() for doc in documents]\n",
    "        temp_bm25 = BM25Okapi(tokenized_docs)\n",
    "        scores = temp_bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Sort documents by score\n",
    "        scored_docs = [(score, documents[i]) for i, score in enumerate(scores)]\n",
    "        sorted_docs = sorted(scored_docs, key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Return top k documents\n",
    "        return [doc for _, doc in sorted_docs[:self.k]]\n",
    "    \n",
    "class CustomRetrieverWithHistory:\n",
    "    \"\"\"A custom retriever that uses BM25 reranking and leverages query history\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, chat_history=None, k=10, rerank_k=4):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.chat_history = chat_history or []\n",
    "        self.reranker = BM25Reranker(k=rerank_k) \n",
    "        self.k = k\n",
    "    \n",
    "    def get_relevant_documents(self, query):\n",
    "        # First, get documents from vector store\n",
    "        docs = self.vectorstore.similarity_search(query, k=self.k)\n",
    "        \n",
    "        # If we have chat history, use it to enhance our retrieval\n",
    "        if self.chat_history and len(self.chat_history) > 0:\n",
    "            # Get the last few exchanges to provide context\n",
    "            recent_history = self.chat_history[-3:] if len(self.chat_history) > 3 else self.chat_history\n",
    "            history_context = \"\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in recent_history])\n",
    "            \n",
    "            # Combine with current query for broader retrieval\n",
    "            enhanced_query = f\"{query} {history_context}\"\n",
    "            additional_docs = self.vectorstore.similarity_search(enhanced_query, k=5)\n",
    "            \n",
    "            # Combine both sets of documents (removing duplicates)\n",
    "            unique_docs = {}\n",
    "            for doc in docs + additional_docs:\n",
    "                if doc.page_content not in unique_docs:\n",
    "                    unique_docs[doc.page_content] = doc\n",
    "            \n",
    "            docs = list(unique_docs.values())\n",
    "        \n",
    "        # Apply reranking to final set\n",
    "        return self.reranker.rerank(query, docs)\n",
    "\n",
    "    def update_history(self, query, answer):\n",
    "        \"\"\"Update chat history with new exchange\"\"\"\n",
    "        self.chat_history.append((query, answer))\n",
    "    \n",
    "    def save_history(self, file_path=BANK_DB_STATE_PATH):\n",
    "        \"\"\"Save chat history to a file\"\"\"\n",
    "        state = {\n",
    "            \"chat_history\": self.chat_history,\n",
    "            \"timestamp\": str(import datetime; datetime.datetime.now())\n",
    "        }\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(state, f)\n",
    "            \n",
    "    def load_history(self, file_path=BANK_DB_STATE_PATH):\n",
    "        \"\"\"Load chat history from a file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                state = json.load(f)\n",
    "                self.chat_history = state.get(\"chat_history\", [])\n",
    "                print(f\"Loaded {len(self.chat_history)} previous conversations\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"No previous history found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading history: {e}\")\n",
    "\n",
    "# Initialize our custom retriever\n",
    "retriever = CustomRetrieverWithHistory(vectorstore=vectorstore, k=10, rerank_k=4)\n",
    "\n",
    "# Load previous chat history if available\n",
    "retriever.load_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [ "## 7. Setup LLM, Prompts and Chain (French, RAG)" ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Setup LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"Vous √™tes un assistant expert en documentation de base de donn√©es bancaire.\n",
    "Votre objectif est de fournir des informations pr√©cises et sp√©cifiques sur l'emplacement des donn√©es dans le syst√®me de base de donn√©es de la banque.\n",
    "\n",
    "Informations contextuelles issues de la documentation de la base de donn√©es ci-dessous:\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "\n",
    "Compte tenu des informations contextuelles et de la question, fournissez une r√©ponse d√©taill√©e qui indique clairement:\n",
    "1. La ou les table(s) exacte(s) o√π se trouvent les donn√©es demand√©es\n",
    "2. Les noms sp√©cifiques des colonnes\n",
    "3. Les relations avec d'autres tables\n",
    "4. Les types de donn√©es et les contraintes si pertinent\n",
    "\n",
    "Important: √Ä la fin de votre r√©ponse, indiquez clairement les sources d'o√π proviennent ces informations en listant les noms des fichiers PDF sources.\n",
    "\n",
    "Soyez pr√©cis et technique. Si les informations ne peuvent pas √™tre trouv√©es dans le contexte, reconnaissez-le clairement.\n",
    "N'inventez pas d'informations sur les noms de tables ou les structures.\n",
    "\n",
    "Historique de la conversation: {chat_history}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=RAG_TEMPLATE,\n",
    "    input_variables=[\"context\", \"chat_history\", \"question\"]\n",
    ")\n",
    "\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [ "## 8. Query Reformulation, Explanation, Commentary Functions (French)" ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_alternative_queries(query: str) -> list:\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"G√©n√®re 3 fa√ßons alternatives de poser la question suivante √† propos d'une base de donn√©es bancaire.\\nRends les alternatives sp√©cifiques et concentr√©es sur la structure de la base de donn√©es et l'emplacement des donn√©es.\\nQuestion originale: {question}\\n\\nRetourne uniquement les questions, une par ligne.\"\"\",\n",
    "        input_variables=[\"question\"]\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(question=query)\n",
    "    alts = [q.strip() for q in result.split('\\n') if q.strip()]\n",
    "    return alts + [query]\n",
    "\n",
    "def generate_answer_explanation(query, answer, sources):\n",
    "    # Use only up to first 2 source texts for summary\n",
    "    sources_summary = \"\\n\".join([doc.page_content[:300] for doc in sources[:2]])\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"En tant qu'expert en base de donn√©es bancaire, explique comment tu as formul√© la r√©ponse suivante √† la question pos√©e.\\nFais r√©f√©rence aux donn√©es sp√©cifiques et √† la structure de la base de donn√©es bancaire.\\n\\nQuestion: {query}\\nR√©ponse: {answer}\\n\\nSources principales:\\n{sources}\\n\\nExplique ton raisonnement en fran√ßais en te concentrant sur:\\n1. Comment les sources ont influenc√© ta r√©ponse\\n2. Les √©l√©ments cl√©s de la structure de la base de donn√©es mentionn√©s\\n3. Les relations entre les tables identifi√©es\\n4. Toute implication ou consid√©ration technique importante\\n\\nExplication (en 3-5 phrases):\"\"\",\n",
    "        input_variables=[\"query\", \"answer\", \"sources\"]\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    explanation = chain.run(query=query, answer=answer, sources=sources_summary)\n",
    "    return explanation.strip()\n",
    "\n",
    "def generate_technical_commentary(query, answer):\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"En tant qu'expert en bases de donn√©es bancaires, fournis un commentaire technique bref\\nsur les aspects de la structure de la base de donn√©es mentionn√©s dans cette question et r√©ponse.\\n\\nQuestion: {query}\\nR√©ponse: {answer}\\n\\nFournis un commentaire technique (2-3 phrases) qui pourrait aider un d√©veloppeur ou un analyste de donn√©es\\n√† mieux comprendre les implications techniques de cette information:\"\"\",\n",
    "        input_variables=[\"query\", \"answer\"]\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    return chain.run(query=query, answer=answer).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation Functions & User Feedback"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_rag_response(query, answer, retrieved_docs, feedback=None):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of a RAG response based on various metrics\n",
    "    \"\"\"\n",
    "    evaluation = {}\n",
    "    \n",
    "    # 1. Document relevance score - Compare query with retrieved docs\n",
    "    query_tokens = set(query.lower().split())\n",
    "    relevant_docs = 0\n",
    "    \n",
    "    for doc in retrieved_docs:\n",
    "        doc_tokens = set(doc.page_content.lower().split())\n",
    "        overlap = len(query_tokens.intersection(doc_tokens)) / len(query_tokens) if query_tokens else 0\n",
    "        if overlap > 0.2:  # Simple relevance threshold\n",
    "            relevant_docs += 1\n",
    "    \n",
    "    doc_relevance = relevant_docs / len(retrieved_docs) if retrieved_docs else 0\n",
    "    evaluation[\"document_relevance\"] = round(doc_relevance * 10, 2)  # Score out of 10\n",
    "    \n",
    "    # 2. Source diversity - Are we pulling from different sources?\n",
    "    sources = set()\n",
    "    for doc in retrieved_docs:\n",
    "        if \"source_file\" in doc.metadata:\n",
    "            sources.add(doc.metadata[\"source_file\"])\n",
    "    \n",
    "    source_diversity = min(len(sources) / 3, 1.0)  # Normalize to max of 1\n",
    "    evaluation[\"source_diversity\"] = round(source_diversity * 10, 2)  # Score out of 10\n",
    "    \n",
    "    # 3. Answer relevance - Analyze how well the answer matches the query\n",
    "    # This requires LLM evaluation, creating a simplified heuristic\n",
    "    answer_tokens = set(answer.lower().split())\n",
    "    query_answer_overlap = len(query_tokens.intersection(answer_tokens)) / len(query_tokens) if query_tokens else 0\n",
    "    evaluation[\"answer_query_alignment\"] = round(query_answer_overlap * 10, 2)  # Score out of 10\n",
    "    \n",
    "    # 4. User feedback score if available\n",
    "    if feedback is not None:\n",
    "        evaluation[\"user_feedback\"] = feedback  # User score (1-5)\n",
    "    \n",
    "    # 5. Hallucination risk - Check if answer cites sources properly\n",
    "    contains_source_citation = any([\n",
    "        \"source\" in answer.lower(),\n",
    "        \"pdf\" in answer.lower(),\n",
    "        \"provien\" in answer.lower(),\n",
    "        \"extract\" in answer.lower()\n",
    "    ])\n",
    "    evaluation[\"hallucination_risk\"] = \"Low\" if contains_source_citation else \"Medium\"\n",
    "    \n",
    "    # Overall score (weighted average)\n",
    "    weights = {\"document_relevance\": 0.4, \"source_diversity\": 0.3, \"answer_query_alignment\": 0.3}\n",
    "    overall_score = sum(evaluation[k] * weights[k] for k in weights.keys())\n",
    "    evaluation[\"overall_score\"] = round(overall_score, 2)  # Score out of 10\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "class UserFeedbackTracker:\n",
    "    \"\"\"Track user feedback for responses\"\"\"\n",
    "    \n",
    "    def __init__(self, feedback_file=\"user_feedback.json\"):\n",
    "        self.feedback_file = feedback_file\n",
    "        self.feedback_history = self._load_feedback()\n",
    "    \n",
    "    def _load_feedback(self):\n",
    "        \"\"\"Load feedback history from file\"\"\"\n",
    "        try:\n",
    "            with open(self.feedback_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading feedback: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def save_feedback(self, query, answer, rating, comment=None):\n",
    "        \"\"\"Save user feedback\"\"\"\n",
    "        feedback_entry = {\n",
    "            \"timestamp\": str(import datetime; datetime.datetime.now()),\n",
    "            \"query\": query,\n",
    "            \"answer\": answer[:100] + \"...\",  # Store shortened answer\n",
    "            \"rating\": rating,\n",
    "            \"comment\": comment\n",
    "        }\n",
    "        \n",
    "        self.feedback_history.append(feedback_entry)\n",
    "        \n",
    "        with open(self.feedback_file, 'w') as f:\n",
    "            json.dump(self.feedback_history, f)\n",
    "        \n",
    "        print(f\"Feedback saved. Current rating: {rating}/5\")\n",
    "        return rating\n",
    "    \n",
    "    def get_average_rating(self):\n",
    "        \"\"\"Get average feedback rating\"\"\"\n",
    "        if not self.feedback_history:\n",
    "            return None\n",
    "        \n",
    "        ratings = [entry[\"rating\"] for entry in self.feedback_history if \"rating\" in entry]\n",
    "        return sum(ratings) / len(ratings) if ratings else None\n",
    "\n",
    "# Initialize feedback tracker\n",
    "feedback_tracker = UserFeedbackTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Process a Query Through the Full Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def extract_source_files(documents):\n",
    "    \"\"\"Extract source files from document metadata\"\"\"\n",
    "    sources = []\n",
    "    for doc in documents:\n",
    "        if \"source_file\" in doc.metadata:\n",
    "            sources.append(doc.metadata[\"source_file\"])\n",
    "        elif \"source\" in doc.metadata:\n",
    "            sources.append(doc.metadata[\"source\"])\n",
    "    return list(set(sources))\n",
    "\n",
    "def process_query(query: str):\n",
    "    # Generate alternative phrasings of the query\n",
    "    alt_queries = generate_alternative_queries(query)\n",
    "    all_results = []\n",
    "    \n",
    "    # Process each alternative query\n",
    "    for q in alt_queries:\n",
    "        res = rag_chain({\"question\": q, \"chat_history\": retriever.chat_history})\n",
    "        all_results.append(res)\n",
    "    \n",
    "    # Pick result with most source docs\n",
    "    best_result = max(all_results, key=lambda x: len(x['source_documents']))\n",
    "    answer, sources = best_result['answer'], best_result['source_documents']\n",
    "    \n",
    "    # Generate additional context\n",
    "    explanation = generate_answer_explanation(query, answer, sources)\n",
    "    commentary = generate_technical_commentary(query, answer)\n",
    "    evaluation = evaluate_rag_response(query, answer, sources)\n",
    "    \n",
    "    # Add source files information if not already in the answer\n",
    "    source_files = extract_source_files(sources)\n",
    "    if source_files and \"Sources:\" not in answer:\n",
    "        answer += f\"\\n\\nSources: {', '.join(source_files)}\"\n",
    "    \n",
    "    # Update chat history\n",
    "    retriever.update_history(query, answer)\n",
    "    retriever.save_history()\n",
    "    \n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'sources': sources,\n",
    "        'source_files': source_files,\n",
    "        'explanation': explanation,\n",
    "        'commentary': commentary,\n",
    "        'evaluation': evaluation\n",
    "    }\n",
    "\n",
    "def submit_feedback(query, answer, rating, comment=None):\n",
    "    \"\"\"Submit user feedback for a response\"\"\"\n",
    "    return feedback_tracker.save_feedback(query, answer, rating, comment)\n",
    "\n",
    "# Usage example:\n",
    "user_query = \"Dans quelle table puis-je trouver les informations sur les transactions clients ?\"\n",
    "result = process_query(user_query)\n",
    "\n",
    "print('R√©ponse:', result['answer'])\n",
    "print('\\nExplication:', result['explanation'])\n",
    "print('\\nCommentaire technique:', result['commentary'])\n",
    "print('\\n√âvaluation:', result['evaluation'])\n",
    "print('\\nSources des documents:', result['source_files'])\n",
    "\n",
    "# Example of submitting feedback\n",
    "# submit_feedback(user_query, result['answer'], 5, \"Excellente r√©ponse!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
